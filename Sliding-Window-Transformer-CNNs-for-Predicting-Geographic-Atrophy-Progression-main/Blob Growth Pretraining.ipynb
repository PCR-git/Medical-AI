{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d858047",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Blob growth simulation\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.ndimage import gaussian_filter, binary_dilation, uniform_filter, binary_erosion\n",
    "import matplotlib.pyplot as plt # ADDED for visualization\n",
    "from tqdm import tqdm # ADDED for progress tracking\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Set seed ---\n",
    "# SEED = 2024 # Used for test data\n",
    "SEED = 2025 # Used for training data\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "print(f\"Random seeds set to {SEED} for reproducibility.\")\n",
    "\n",
    "# --- FAF Configuration ---\n",
    "TARGET_SIZE = 256\n",
    "NUM_PEAKS = 50 \n",
    "GROWTH_RATE_MIN = 1 # Not used anymore, kept for reference\n",
    "GROWTH_RATE_MAX = 10 # Not used anymore, kept for reference\n",
    "NUM_SAMPLES = 1 # Target dataset size (Set to 1 for quick test)\n",
    "\n",
    "# --- Anisotropic Growth Configuration ---\n",
    "ANISO_GROWTH_STEPS = 5              # Number of smaller, randomized steps per frame\n",
    "ANISO_DIR_STRENGTH = 0.6            # Bias factor for directional growth (0.0 to 1.0, 1.0 is max anisotropy)\n",
    "ANISO_JAGGEDNESS_FREQ = 0.75        # Probability of applying the jagged erosion/dilation cycle (was 0.75)\n",
    "\n",
    "# --- Vein Configuration ---\n",
    "VEIN_VALUE_FLOAT = 0.01      # DECREASED for darker/more visible veins\n",
    "BACKGROUND_VALUE_FLOAT = 1.0\n",
    "VEIN_SMOOTHING_SIGMA = 6.0   # INCREASED further for thicker veins\n",
    "VEIN_WALK_LENGTH = 75        # INCREASED for longer veins\n",
    "VEIN_STEP_LENGTH = 5.0      \n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def generate_vein_mask(size=TARGET_SIZE):\n",
    "    \"\"\"Generates a static vein structure.\"\"\"\n",
    "    vein_scratchpad = np.zeros((size, size), dtype=np.float32)\n",
    "    num_start_locations = 12 # INCREASED for more common veins\n",
    "    zones = [\n",
    "        (1, 127, 1, 127), (128, 255, 1, 127), (1, 127, 128, 255),\n",
    "        (128, 255, 128, 255), (64, 192, 64, 192)\n",
    "    ]\n",
    "    for i in range(num_start_locations):\n",
    "        x_min, x_max, y_min, y_max = zones[i % len(zones)]\n",
    "        x, y = random.uniform(x_min, x_max), random.uniform(y_min, y_max)\n",
    "        angle = random.uniform(0, 2 * np.pi)\n",
    "        for _ in range(VEIN_WALK_LENGTH):\n",
    "            angle += random.uniform(-np.pi / 8, np.pi / 8) \n",
    "            x_new = x + VEIN_STEP_LENGTH * np.sin(angle)\n",
    "            y_new = y - VEIN_STEP_LENGTH * np.cos(angle)\n",
    "            ix, iy = int(np.clip(x, 0, size - 1)), int(np.clip(y, 0, size - 1))\n",
    "            vein_scratchpad[iy, ix] = 1.0\n",
    "            x, y = x_new, y_new\n",
    "            if not (0 <= x < size and 0 <= y < size):\n",
    "                break\n",
    "\n",
    "    vein_filtered = gaussian_filter(vein_scratchpad, sigma=VEIN_SMOOTHING_SIGMA)\n",
    "    if vein_filtered.max() > 0:\n",
    "        vein_filtered /= vein_filtered.max()\n",
    "    vein_mask = BACKGROUND_VALUE_FLOAT - (vein_filtered * (BACKGROUND_VALUE_FLOAT - VEIN_VALUE_FLOAT))\n",
    "    return vein_mask\n",
    "\n",
    "def generate_initial_solid_blob(size=TARGET_SIZE):\n",
    "    \"\"\"Generates the initial field and starting mask with high size variation.\"\"\"\n",
    "    base_field = np.zeros((size, size), dtype=np.float32)\n",
    "    for _ in range(NUM_PEAKS):\n",
    "        center_x = np.random.randint(size * 0.15, size * 0.85)\n",
    "        center_y = np.random.randint(size * 0.15, size * 0.85)\n",
    "        peak_height = np.random.uniform(0.3, 1.2) \n",
    "        peak_sigma = np.random.uniform(3, 25) \n",
    "        peak_mask = np.zeros((size, size)); peak_mask[center_y, center_x] = peak_height\n",
    "        peak_mound = gaussian_filter(peak_mask, sigma=peak_sigma)\n",
    "        base_field += peak_mound\n",
    "\n",
    "    if base_field.max() > 0:\n",
    "        base_field /= base_field.max()\n",
    "        \n",
    "    # MODIFIED: Increased max percentile from 30 to 50 for a higher chance of smaller lesions.\n",
    "    random_threshold = np.percentile(base_field[base_field > 0.01], np.random.uniform(5, 90))  \n",
    "    # MODIFIED: Increased max clip to allow higher thresholds resulting in smaller masks.\n",
    "    threshold = np.clip(random_threshold, 0.03, 0.45) \n",
    "\n",
    "    mask = (base_field > threshold).astype(np.float32)\n",
    "    return base_field, mask\n",
    "\n",
    "def generate_growing_masks(initial_mask, num_frames=4):\n",
    "    \"\"\"Generates a sequence of masks with anisotropic, non-uniform growth and jagged edges.\"\"\"\n",
    "    masks = [initial_mask]\n",
    "    current_mask = initial_mask.astype(bool)\n",
    "\n",
    "    # Directional kernels (Anisotropic Growth)\n",
    "    k_north = np.array([[0, 1, 0], [0, 1, 0], [0, 0, 0]])\n",
    "    k_south = np.array([[0, 0, 0], [0, 1, 0], [0, 1, 0]])\n",
    "    k_east = np.array([[0, 0, 0], [0, 1, 1], [0, 0, 0]])\n",
    "    k_west = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 0]])\n",
    "    k_jagged = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
    "    \n",
    "    directional_kernels = [k_north, k_south, k_east, k_west]\n",
    "\n",
    "    # Pre-select a *main* growth direction for this lesion (anisotropy)\n",
    "    main_direction_idx = random.randint(0, 3)\n",
    "\n",
    "    for _ in range(1, num_frames):\n",
    "        # 1. Non-uniform, Anisotropic Growth\n",
    "        for step in range(ANISO_GROWTH_STEPS):\n",
    "            # Biased random kernel selection\n",
    "            if random.random() < ANISO_DIR_STRENGTH:\n",
    "                # Use the pre-selected main direction for stronger growth\n",
    "                kernel = directional_kernels[main_direction_idx]\n",
    "            else:\n",
    "                # Use a random direction for non-uniformity\n",
    "                kernel = random.choice(directional_kernels)\n",
    "                \n",
    "            # Apply a small dilation step\n",
    "            current_mask = binary_dilation(current_mask, structure=kernel, iterations=1)\n",
    "\n",
    "        # 2. Jaggedness Cycle \n",
    "        if random.random() < ANISO_JAGGEDNESS_FREQ:\n",
    "            # Erosion to remove small isolated points and introduce jaggedness\n",
    "            next_mask_eroded = binary_erosion(current_mask, structure=k_jagged, iterations=1)\n",
    "            # Dilation to fill in some gaps and keep the growth\n",
    "            current_mask = binary_dilation(next_mask_eroded, structure=k_jagged, iterations=1)\n",
    "\n",
    "        masks.append(current_mask.astype(np.float32))\n",
    "\n",
    "    return masks\n",
    "\n",
    "def generate_edge_artifact(size=TARGET_SIZE, intensity=0.3):\n",
    "    \"\"\"\n",
    "    Generates a dark, sharp, lesion-like blob near one edge.\n",
    "    (UPDATED: Larger and Closer to the Edge)\n",
    "    \"\"\"\n",
    "    artifact_field = np.zeros((size, size), dtype=np.float32)\n",
    "    \n",
    "    side = np.random.randint(4)\n",
    "    \n",
    "    # CRITICAL CHANGE 1: Define a smaller peripheral zone (10% border)\n",
    "    border_zone = int(size * 0.10) \n",
    "    \n",
    "    # 1. Determine the center of the artifact in the border zone\n",
    "    # Logic remains similar, but uses the smaller border_zone\n",
    "    if side == 0: # Top\n",
    "        center_y = np.random.randint(0, border_zone)\n",
    "        center_x = np.random.randint(border_zone, size - border_zone)\n",
    "    elif side == 1: # Bottom\n",
    "        center_y = np.random.randint(size - border_zone, size)\n",
    "        center_x = np.random.randint(border_zone, size - border_zone)\n",
    "    elif side == 2: # Left\n",
    "        center_y = np.random.randint(border_zone, size - border_zone)\n",
    "        center_x = np.random.randint(0, border_zone)\n",
    "    else: # Right\n",
    "        center_y = np.random.randint(border_zone, size - border_zone)\n",
    "        center_x = np.random.randint(size - border_zone, size)\n",
    "\n",
    "    # 2. Generate a single smooth Gaussian blob\n",
    "    peak_mask = np.zeros((size, size))\n",
    "    peak_mask[center_y, center_x] = intensity \n",
    "    \n",
    "    # CRITICAL CHANGE 2: INCREASED MAX SIGMA for a larger artifact\n",
    "    peak_sigma = np.random.uniform(8, 18) \n",
    "    artifact_mound = gaussian_filter(peak_mask, sigma=peak_sigma)\n",
    "    \n",
    "    if artifact_mound.max() > 0:\n",
    "        artifact_mound /= artifact_mound.max()\n",
    "        \n",
    "    return artifact_mound * intensity\n",
    "\n",
    "def generate_static_faf_background(size=TARGET_SIZE, base_field=None):\n",
    "    \"\"\"Pre-calculates the static, textured background layer for speed and includes the optional edge artifact.\"\"\"\n",
    "    if base_field is None:\n",
    "        base_field = np.ones((size, size))    \n",
    "        \n",
    "    background_texture = np.zeros((size, size))\n",
    "    for _ in range(5):    \n",
    "        rand_sigma = np.random.uniform(20, 100)\n",
    "        rand_intensity = np.random.uniform(0.05, 0.15)\n",
    "        background_texture += gaussian_filter(np.random.rand(size, size) * rand_intensity, sigma=rand_sigma)\n",
    "    background_texture -= background_texture.mean()      \n",
    "    background_texture *= 8.0    \n",
    "    \n",
    "    MIN_TISSUE_FAF = 0.65\n",
    "    MAX_TISSUE_FAF = 0.75\n",
    "    FAF_base_smooth = MIN_TISSUE_FAF + (1 - base_field) * (MAX_TISSUE_FAF - MIN_TISSUE_FAF)      \n",
    "    FAF_tissue_area = FAF_base_smooth + background_texture\n",
    "    \n",
    "    # --- NEW INTEGRATION: Optional Edge Artifact (50% Chance) ---\n",
    "    if np.random.rand() < 0.5:\n",
    "        # Generate a dark artifact with random darkness\n",
    "        # INCREASED MAX DARKNESS from 0.35 to 0.50\n",
    "        darkness = np.random.uniform(0.15, 0.50)\n",
    "        edge_artifact = generate_edge_artifact(size=size, intensity=darkness)\n",
    "        \n",
    "        # Subtract the dark artifact from the background\n",
    "        FAF_tissue_area -= edge_artifact\n",
    "    # -------------------------------------------------------------\n",
    "    \n",
    "    # Ensure the background remains positive after subtraction\n",
    "    FAF_tissue_area = np.clip(FAF_tissue_area, 0.0, 1.0)\n",
    "    \n",
    "    return FAF_tissue_area.astype(np.float32)\n",
    "\n",
    "def add_fine_speckle_noise(image, noise_factor=0.02):\n",
    "    \"\"\"Adds very fine, granular speckle noise to the image.\"\"\"\n",
    "    noise = (np.random.rand(*image.shape) - 0.5) * noise_factor\n",
    "    return np.clip(image + noise, 0.0, 1.0)\n",
    "\n",
    "def generate_synthetic_faf(static_faf_background, mask, size=TARGET_SIZE, lesion_contrast_factor=0.7):\n",
    "    \"\"\"Generates FAF image for a single frame with dynamic, size-dependent smoothing.\"\"\"\n",
    "    FAF_final = static_faf_background.copy()\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    # *** CRITICAL FIX: DYNAMICALLY SCALE SMOOTHING SIGMA ***\n",
    "    mask_area = mask.sum()\n",
    "    if mask_area > 0:\n",
    "        # Scale sigma based on area, clamping to ensure it's not too small or too large\n",
    "        # We use cbrt(area) for a gentler scaling than sqrt(area)\n",
    "        # 1. Calculate base sigma (e.g., cbrt(area) / 2)\n",
    "        base_sigma = np.cbrt(mask_area) * 0.5 \n",
    "        \n",
    "        # 2. Clamp the sigma: Min 8.0 (for jaggedness), Max 15.0 (to prevent over-smoothing large lesions)\n",
    "        dynamic_sigma = np.clip(base_sigma, 8.0, 15.0) \n",
    "    else:\n",
    "        dynamic_sigma = 8.0 # Fallback for an empty mask\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    # Internal Lesion Splotches (omitted for brevity)\n",
    "    lesion_splotches = np.zeros_like(mask)\n",
    "    if mask.sum() > 0:\n",
    "        num_splotches = np.random.randint(3, 6)\n",
    "        for _ in range(num_splotches):\n",
    "             center_x = np.random.randint(0, size); center_y = np.random.randint(0, size)\n",
    "             splotch_sigma = np.random.uniform(8, 15); splotch_val = np.random.uniform(-0.2, 0.2)\n",
    "             splotch_mask = np.zeros((size, size)); splotch_mask[center_y, center_x] = splotch_val \n",
    "             lesion_splotches += gaussian_filter(splotch_mask, sigma=splotch_sigma)\n",
    "        lesion_splotches = np.clip(lesion_splotches, -0.1, 0.15)   \n",
    "        lesion_splotches *= mask   \n",
    "\n",
    "    # Final Assembly  \n",
    "    DARKNESS_FACTOR = 0.55       \n",
    "    # Apply the DYNAMICALLY CALCULATED sigma\n",
    "    SMOOTHED_MASK = np.clip(gaussian_filter(mask, sigma=dynamic_sigma), 0.0, 1.0) \n",
    "    FAF_final -= SMOOTHED_MASK * DARKNESS_FACTOR\n",
    "    \n",
    "    # Noise, Texture, and Contrast Stretch (omitted for brevity)\n",
    "    local_intensity_map = gaussian_filter(np.random.randn(size, size), sigma=50) * 0.1\n",
    "    FAF_final += local_intensity_map\n",
    "    TEXTURE_AMPLITUDE = 0.08\n",
    "    structured_noise_low = uniform_filter(np.random.rand(size, size), size=int(size/64)) * TEXTURE_AMPLITUDE * 0.5\n",
    "    structured_noise_high = gaussian_filter(np.random.randn(size, size), sigma=0.7) * TEXTURE_AMPLITUDE * 0.5\n",
    "    FAF_final += structured_noise_low + structured_noise_high\n",
    "    \n",
    "    background_region = FAF_final[mask == 0]    \n",
    "    if background_region.size > 0:\n",
    "        bg_min = np.percentile(background_region, 10); bg_max = np.percentile(background_region, 90)\n",
    "        if bg_max > bg_min:\n",
    "            stretched_background = (background_region - bg_min) / (bg_max - bg_min)\n",
    "            stretched_background = stretched_background * 0.2 + 0.55\n",
    "            FAF_final[mask == 0] = stretched_background\n",
    "    \n",
    "    CONTRAST_POWER_FACTOR = 1.0       \n",
    "    FAF_final = np.power(FAF_final, CONTRAST_POWER_FACTOR)\n",
    "    \n",
    "    FAF_normalized = np.clip(FAF_final, 0.0, 1.0)      \n",
    "\n",
    "    return FAF_normalized.astype(np.float32)\n",
    "\n",
    "def apply_soft_vignette(image, strength=0.08, sigma_ratio=0.3):\n",
    "    \"\"\"Applies a smooth, soft vignette effect to fade the edges.\"\"\"\n",
    "    size = image.shape[0]\n",
    "    center_y, center_x = size // 2, size // 2\n",
    "    y, x = np.ogrid[-center_y:size-center_y, -center_x:size-center_x]\n",
    "    \n",
    "    max_dist = np.sqrt((size/2)**2 + (size/2)**2)\n",
    "    distance_map = np.sqrt(x*x + y*y) / max_dist\n",
    "    \n",
    "    sigma = size * sigma_ratio\n",
    "    weight = np.exp(-(distance_map**2) / (2 * (sigma/size)**2))\n",
    "    \n",
    "    weight = (weight - weight.min()) / (weight.max() - weight.min()) * strength + (1.0 - strength/2)\n",
    "    \n",
    "    return np.clip(image * weight, 0.0, 1.0)\n",
    "\n",
    "def generate_single_simulation():\n",
    "    \"\"\"\n",
    "    Runs one simulation and returns the stacked data for 4 frames: (3, 4, 256, 256).\n",
    "    \"\"\"\n",
    "    # 1. Initialization\n",
    "    base_field, initial_mask = generate_initial_solid_blob(TARGET_SIZE)\n",
    "\n",
    "    # 2. Pre-calculate Static Components\n",
    "    static_faf_background = generate_static_faf_background(TARGET_SIZE, base_field)\n",
    "    vein_mask = generate_vein_mask(TARGET_SIZE)\n",
    "\n",
    "    # 3. Generate Masks and Residuals\n",
    "    all_masks = generate_growing_masks(initial_mask, num_frames=4)\n",
    "\n",
    "    residual_masks = [np.zeros_like(initial_mask)] \n",
    "    for i in range(1, 4):\n",
    "        # Calculate the difference and ensure it's a binary mask of new growth\n",
    "        residual = (all_masks[i] - all_masks[i-1]) > 0\n",
    "        residual_masks.append(residual.astype(np.float32))\n",
    "\n",
    "    # 4. Generate FAF Images\n",
    "    final_images = []\n",
    "    for i, mask in enumerate(all_masks):\n",
    "        # Calls the function with the lesion edge fix (sigma=10.0)\n",
    "        faf_image = generate_synthetic_faf(static_faf_background, mask)\n",
    "        \n",
    "        # Overlay Veins\n",
    "        final_overlay = np.minimum(faf_image, vein_mask)\n",
    "\n",
    "        # Add Fine Speckle Noise\n",
    "        final_artifact_image = add_fine_speckle_noise(final_overlay, noise_factor=0.02)\n",
    "\n",
    "        # Apply Vignette for global edge smoothness (to prevent artifacts at the image border)\n",
    "        final_vignetted_image = apply_soft_vignette(final_artifact_image) # <-- ADDED\n",
    "\n",
    "        final_images.append(final_vignetted_image) # <-- APPENDS VIGNETTED IMAGE\n",
    "    \n",
    "    # 5. Stack into (3, 4, 256, 256)\n",
    "    stacked_frames = np.stack([\n",
    "        np.array(final_images),     # Channel 0: FAF Images\n",
    "        np.array(all_masks),        # Channel 1: Lesion Masks\n",
    "        np.array(residual_masks)    # Channel 2: Residual Masks\n",
    "    ], axis=0) # Stacks along the channel dimension (axis 0)\n",
    "    \n",
    "    return stacked_frames\n",
    "\n",
    "# --- MAIN DATASET GENERATION FUNCTION ---\n",
    "\n",
    "def generate_synthetic_faf_dataset(num_samples=NUM_SAMPLES):\n",
    "    \"\"\"\n",
    "    Generates a dataset of synthetic FAF growth simulations.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): The number of independent simulations to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A dataset array of shape (num_samples, 3, 4, 256, 256).\n",
    "                    Channels: [FAF_Image, Lesion_Mask, Residual_Mask].\n",
    "    \"\"\"\n",
    "    dataset_list = []\n",
    "    \n",
    "    # USING TQDM FOR PROGRESS TRACKING\n",
    "    print(f\"Generating {num_samples} synthetic FAF simulations with ANISOTROPIC growth...\")\n",
    "\n",
    "    for i in tqdm(range(num_samples), desc=\"Building FAF Dataset\"):\n",
    "        try:\n",
    "            sample_data = generate_single_simulation()\n",
    "            dataset_list.append(sample_data)\n",
    "        except Exception as e:\n",
    "            # Handle rare cases where random generation might cause an error (e.g., empty mask percentile)\n",
    "            # Use tqdm.write to avoid interfering with the progress bar\n",
    "            tqdm.write(f\"Warning: Simulation {i} failed with error: {e}. Retrying.\")\n",
    "            num_samples += 1\n",
    "            continue\n",
    "\n",
    "    # Stack all simulations along the first dimension (axis 0) to get the final shape\n",
    "    final_dataset = np.stack(dataset_list, axis=0)\n",
    "    \n",
    "    tqdm.write(f\"\\nGeneration complete. Final dataset shape: {final_dataset.shape}\")\n",
    "    \n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Generate the dataset and print the final shape\n",
    "synthetic_dataset = generate_synthetic_faf_dataset(num_samples=NUM_SAMPLES)\n",
    "\n",
    "# --- Visualization of an Example ---\n",
    "if synthetic_dataset.shape[0] > 0:\n",
    "    sample_index = 0\n",
    "    example_sample = synthetic_dataset[sample_index] # Shape (3, 4, 256, 256)\n",
    "\n",
    "    print(f\"\\nDisplaying Example Sample {sample_index} from the dataset (FAF, Mask, Residual over 4 frames):\")\n",
    "\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12)) \n",
    "    fig.suptitle(f'Example Synthetic FAF Sample {sample_index} | ANISOTROPIC Growth', fontsize=16)\n",
    "\n",
    "    titles = [\"FAF Image (Channel 0)\", \"Lesion Mask (Channel 1)\", \"Residual Mask (Channel 2)\"]\n",
    "\n",
    "    for channel in range(3):\n",
    "        for frame in range(4):\n",
    "            img = example_sample[channel, frame]\n",
    "            vmin = 0.0\n",
    "            vmax = 1.0\n",
    "\n",
    "            # For masks, show the new growth in a distinct color for better visualization\n",
    "            if channel == 2:\n",
    "                # Residual mask - show new growth in bright color\n",
    "                axes[channel, frame].imshow(img, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            elif channel == 1:\n",
    "                 # Lesion mask - show in a binary color\n",
    "                axes[channel, frame].imshow(img, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "            else:\n",
    "                # FAF image\n",
    "                axes[channel, frame].imshow(img, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "\n",
    "            axes[channel, frame].set_title(f'{titles[channel]} - Frame {frame+1}')    \n",
    "            axes[channel, frame].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "else:\n",
    "    print(\"Dataset generation completed, but no samples were available to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d96781",
   "metadata": {},
   "source": [
    "## Generate Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04528a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Execution and Saving Logic ---\n",
    "\n",
    "# # 1. Generate the dataset\n",
    "# NUM_SAMPLES = 100\n",
    "\n",
    "# synthetic_dataset = generate_synthetic_faf_dataset(num_samples=NUM_SAMPLES)\n",
    "\n",
    "# # 2. Define the save path\n",
    "# save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Synthetic Data')\n",
    "# file_path = save_dir / 'synthetic_faf_dataset.npy'\n",
    "\n",
    "# # 3. Create the directory if it doesn't exist\n",
    "# save_dir.mkdir(parents=True, exist_ok=True)\n",
    "# print(f\"Saving dataset to: {file_path}\")\n",
    "\n",
    "# # 4. Save the dataset using NumPy\n",
    "# np.save(file_path, synthetic_dataset)\n",
    "# print(\"Dataset successfully saved!\")\n",
    "\n",
    "# # --- Visualization of an Example (Optional) ---\n",
    "# if synthetic_dataset.shape[0] > 0:\n",
    "#     sample_index = 0\n",
    "#     example_sample = synthetic_dataset[sample_index] # Shape (3, 4, 256, 256)\n",
    "\n",
    "#     print(f\"\\nDisplaying Example Sample {sample_index} from the dataset (FAF, Mask, Residual over 4 frames):\")\n",
    "\n",
    "#     fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "#     fig.suptitle(f'Example Synthetic FAF Sample {sample_index} | ANISOTROPIC Growth + Enhanced Noise', fontsize=16)\n",
    "\n",
    "#     titles = [\"FAF Image (Channel 0)\", \"Lesion Mask (Channel 1)\", \"Residual Mask (Channel 2)\"]\n",
    "\n",
    "#     for channel in range(3):\n",
    "#         for frame in range(4):\n",
    "#             img = example_sample[channel, frame]\n",
    "#             vmin = 0.0\n",
    "#             vmax = 1.0\n",
    "\n",
    "#             if channel == 2:\n",
    "#                 axes[channel, frame].imshow(img, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "#             elif channel == 1:\n",
    "#                 axes[channel, frame].imshow(img, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "#             else:\n",
    "#                 axes[channel, frame].imshow(img, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "\n",
    "#             axes[channel, frame].set_title(f'{titles[channel]} - Frame {frame+1}')\n",
    "#             axes[channel, frame].axis('off')\n",
    "\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     plt.show()\n",
    "# else:\n",
    "#     print(\"Dataset generation completed, but no samples were available to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Code to Display Multiple Samples ---\n",
    "\n",
    "# # Define how many samples you want to visualize\n",
    "# NUM_SAMPLES_TO_DISPLAY = NUM_SAMPLES\n",
    "\n",
    "# for sample_index in range(min(NUM_SAMPLES_TO_DISPLAY, synthetic_dataset.shape[0])):\n",
    "#     example_sample = synthetic_dataset[sample_index] # Shape (3, 4, 256, 256)\n",
    "\n",
    "#     print(f\"\\nDisplaying Example Sample {sample_index} from the dataset (FAF, Mask, Residual over 4 frames):\")\n",
    "\n",
    "#     fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "#     fig.suptitle(f'Example Synthetic FAF Sample {sample_index} | ANISOTROPIC Growth + Enhanced Noise', fontsize=16)\n",
    "\n",
    "#     titles = [\"FAF Image (Channel 0)\", \"Lesion Mask (Channel 1)\", \"Residual Mask (Channel 2)\"]\n",
    "\n",
    "#     for channel in range(3):\n",
    "#         for frame in range(4):\n",
    "#             img = example_sample[channel, frame]\n",
    "#             vmin = 0.0\n",
    "#             vmax = 1.0\n",
    "\n",
    "#             if channel == 2:\n",
    "#                 # Residual mask\n",
    "#                 axes[channel, frame].imshow(img, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "#             elif channel == 1:\n",
    "#                 # Lesion mask\n",
    "#                 axes[channel, frame].imshow(img, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "#             else:\n",
    "#                 # FAF image\n",
    "#                 axes[channel, frame].imshow(img, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "\n",
    "#             axes[channel, frame].set_title(f'{titles[channel]} - Frame {frame+1}')\n",
    "#             axes[channel, frame].axis('off')\n",
    "\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     plt.show()\n",
    "\n",
    "# if synthetic_dataset.shape[0] == 0:\n",
    "#     print(\"Dataset generation completed, but no samples were available to display.\")\n",
    "\n",
    "# print(f\"Finished displaying {min(NUM_SAMPLES_TO_DISPLAY, synthetic_dataset.shape[0])} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9bb94",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "\n",
    "%cd /Users/Pracioppo/Desktop/GA Forecasting\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import functools\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming these utilities are available as imported\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset \n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "from preprocessing_utils import f_rescale_dataset, f_Residuals, f_reshape_training_data, f_rotate_and_zoom, f_random_crop, f_rotate_and_zoom_all, f_crop_all, f_flip_all, f_augment_dataset2\n",
    "\n",
    "from data_utils import DataWrapper, visualize_sample, compare_split_masks\n",
    "\n",
    "from visualization import f_display_autoencoder, plot_log_loss, f_display_frames\n",
    "\n",
    "from models import init_weights, count_parameters\n",
    "from models import rotate_half, RotaryPositionalEmbedding, RoPEMultiheadAttention, RoPETransformerEncoderLayer, ResidualBlock, ChannelReducer, Unet_Enc, Unet_Dec, U_Net_AE\n",
    "\n",
    "from augmentation_utils import f_augment_spatial_and_intensity\n",
    "from training_utils import dsc, dice_loss, GDLoss\n",
    "from training_utils import freeze_batch_norm, f_single_epoch_AE, f_single_epoch_spatiotemporal, calculate_total_loss, f_single_epoch_spatiotemporal_accumulated\n",
    "from training_utils import save_model_weights, load_model\n",
    "from eval_utils import f_eval_pred_dice_test_set, f_eval_pred_dice_train_set, plot_train_test_dice_history, soft_dice_score, f_get_individual_dice, f_plot_individual_dice\n",
    "\n",
    "from models import DynNet, CausalConvAggregator, UPredNet, FusionBlockBottleneck, ChannelFusionBlock, LocalSpatioTemporalMixer, SpatioTemporalGatedMixer, AxialTemporalSWAInterleavedLayer, InterleavedAxialTemporalSWAIntegrator, SlidingWindowAttention, SWAU_Net, SWAU_CFB_Ablation, SWAU_DynNet_Ablation\n",
    "from models import ConvLSTMCell, ConvLSTMCore, ConvLSTMBaseline, ConvLSTM_Simple\n",
    "from models import InterleavedAxialTemporalRKAIntegrator, RKAFeatureAggregator, RKAU_Net\n",
    "from models import AxialMultiheadAttention, StandardAxialInterleavedLayer, StandardAxialIntegrator, AxialU_Net\n",
    "from models import RKA_MultiheadAttention_Fast, AxialTemporalRKAInterleavedLayer_Fast, InterleavedAxialTemporalRKAIntegrator_Fast, RKAFeatureAggregator_Fast, RKAU_Net_Fast\n",
    "from models import CNN_Unet_Enc, CNN_Unet_Dec, CNN_U_Net_AE, CNN_DynNet, SWAU_Net_CNN\n",
    "    \n",
    "# ---------------------------------------\n",
    "\n",
    "ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models')\n",
    "tensorboard_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models')\n",
    "                            \n",
    "start_epoch = 0\n",
    "\n",
    "resume_ckpt = None\n",
    "\n",
    "summary_writer = SummaryWriter(tensorboard_save_dir.absolute().as_posix())\n",
    "\n",
    "\n",
    "# --- SETUP ---\n",
    "# Define a simple placeholder for command-line arguments and configuration\n",
    "parser = argparse.ArgumentParser('AE Model Args')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# Defining essential arguments (set to match your intended lightweight AE setup)\n",
    "args.N = 4                       # CRITICAL FIX: Batch size is 4 for memory safety\n",
    "args.nhead = 4                   # CRITICAL FIX: Reduced heads from 8 to 4\n",
    "args.d_attn1 = 192               # FFN dimension for L3 (112 channels)\n",
    "args.d_attn2 = 384               # FFN dimension for L4 (224 channels)\n",
    "args.img_channels = 3            # Three grayscale images (FAF, masks, growth masks)\n",
    "args.img_sz = 256                # Image size 256x256\n",
    "args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "BASE_CHANNELS = 24               # Reduced from 28 to 24 for extra parameter savings\n",
    "\n",
    "# Training loop arguments\n",
    "args.num_epochs = 1             # UPDATED: Set to 1 epoch as requested\n",
    "args.show_example_epochs = 5\n",
    "args.batch_size = args.N        # Batch size for iteration is args.N\n",
    "args.num_t_steps = 4            # Time steps (used only for data simulation/flattening)\n",
    "\n",
    "# Initialize paths (for saving checkpoints)\n",
    "resume_AE_ckpt = Path('./ae_checkpoints')\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "args.device = torch.device('cuda:0')\n",
    "print(f\"Using {args.device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be11ec",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('/Users/Pracioppo/Desktop//GA Forecasting//Synthetic Data')\n",
    "print(\"Loading synthetic test data...\")\n",
    "\n",
    "# PATH TO THE SAVED SYNTHETIC TEST DATA\n",
    "SYNTHETIC_TEST_PATH = DATA_DIR / 'synthetic_faf_dataset.npy'\n",
    "\n",
    "# Load the single NumPy array\n",
    "synthetic_data_np = np.load(SYNTHETIC_TEST_PATH) \n",
    "\n",
    "# Convert to PyTorch tensor and ensure float32\n",
    "synthetic_data_tensor = torch.from_numpy(synthetic_data_np.astype(np.float32))\n",
    "\n",
    "# Shape of loaded data: (N_test_samples, 3, 4, 256, 256)\n",
    "# N_test_samples = 140 (as per previous K-Fold logic for fold 0)\n",
    "N_TEST_SAMPLES = synthetic_data_tensor.shape[0]\n",
    "C = synthetic_data_tensor.shape[1] # 3: [FAF, Mask, Residual]\n",
    "T = synthetic_data_tensor.shape[2] # 4: Frames\n",
    "\n",
    "# Split the single tensor into FAFs, Masks, and Residuals\n",
    "# We assume the saved order is: Channel 0: FAFs, Channel 1: Masks, Channel 2: Residuals\n",
    "FAFs_test = synthetic_data_tensor[:, 0:1, :, :, :] # [N, 1, T, H, W]\n",
    "masks_test = synthetic_data_tensor[:, 1:2, :, :, :] # [N, 1, T, H, W]\n",
    "residuals_test = synthetic_data_tensor[:, 2:3, :, :, :] # [N, 1, T, H, W]\n",
    "\n",
    "# Global normalization (if not already done during save)\n",
    "# Assuming FAFs and masks were normalized [0, 1] before saving, \n",
    "# but a re-check doesn't hurt.\n",
    "masks_test /= torch.max(masks_test) if torch.max(masks_test) > 0 else 1.0\n",
    "FAFs_test /= torch.max(FAFs_test) if torch.max(FAFs_test) > 0 else 1.0\n",
    "# Residuals are already relative, no max norm needed.\n",
    "\n",
    "\n",
    "# --- 3. Direct Test Dataset Creation ---\n",
    "\n",
    "# Instantiate the DataWrapper with the loaded test features\n",
    "test_dataset = DataWrapper(FAFs_test, masks_test, residuals_test)\n",
    "print(f\"Test Dataset size (samples): {len(test_dataset)}\")\n",
    "print(f\"Test FAFs tensor size: {FAFs_test.size()}\")\n",
    "\n",
    "# --- K-Fold Split Result (Removed K-Fold Logic) ---\n",
    "\n",
    "print(\"--- Data Loading Result (Synthetic Test Only) ---\")\n",
    "print(f\"Test Dataset Samples: {len(test_dataset)} (Expected 140)\")\n",
    "\n",
    "# Placeholder for train_dataset (since we don't load it yet)\n",
    "train_dataset = None \n",
    "\n",
    "\n",
    "# --- 4. DATA ASSEMBLY FOR MANUAL ITERATION (Train Data Removed) ---\n",
    "# Skip loading and assembling training data (full_clean_data_tensor_cpu)\n",
    "# since we are only dealing with the test set for now.\n",
    "\n",
    "\n",
    "# --- DataLoader Setup (Only for the Test/Validation Set) ---\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=args.N, # Use args.N (batch_size for GPU)\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTest DataLoader setup complete with batch size: {args.N}\")\n",
    "\n",
    "\n",
    "# --- UPDATED VISUALIZATION EXAMPLE (Side-by-Side) ---\n",
    "\n",
    "# Example function call\n",
    "# Note: Since 'train_dataset' is None, we need to adjust the visualization call\n",
    "visualize_sample(train_dataset=None, test_dataset=test_dataset, sample_idx=np.random.choice(100), dataset_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067ec89",
   "metadata": {},
   "source": [
    "2,000 videos times 4 frames/video = 8,000 unique images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485231e5",
   "metadata": {},
   "source": [
    "## Pretrain Baseline Conv LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35707a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update (Reconfirmed) ---\n",
    "# BASE_CHANNELS = 16 # 16 channels\n",
    "# args.img_channels = 3 # 3 channels (FAF, Mask, Residual)\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the ConvLSTMBaseline model...\")\n",
    "\n",
    "# # Instantiate the baseline model (assumes Unet_Enc, Unet_Dec, ConvLSTMCore are defined)\n",
    "# model_baseline = ConvLSTMBaseline(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(model_baseline.E1)\n",
    "# p_lstm_params = count_parameters(model_baseline.P_LSTM) # ConvLSTM Core\n",
    "# d1_params = count_parameters(model_baseline.D1)\n",
    "# total_params = e1_params + p_lstm_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor\", f\"{e1_params:,}\"],\n",
    "#     [\"ConvLSTMCore (P_LSTM)\", \"**Recurrent Temporal Core**\", f\"**{p_lstm_params:,}**\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**ConvLSTMBaseline Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### ConvLSTMBaseline Parameter Summary\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- INITIALIZATION AND HYPERPARAMETER SETUP ---\n",
    "\n",
    "# # --- DYNAMIC DATA GENERATION CONFIG ---\n",
    "# # Define the size of the synthetic training set to generate each time\n",
    "# NUM_TRAIN_SAMPLES = 100 # Example size, adjust as needed\n",
    "# DATA_GENERATION_EPOCH_CYCLE = 5 \n",
    "# # ------------------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS\n",
    "# args.num_epochs = 50\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = True # Use Soft Dice for stability\n",
    "# lr = 1E-3 # Initial LR\n",
    "\n",
    "# optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# scheduler_baseline = ReduceLROnPlateau(\n",
    "#     optimizer_baseline, \n",
    "#     mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss # This relies on your custom dice_loss function\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# # LLR_WEIGHT and BOTTLENECK_L2_WEIGHT are correctly used below\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# freeze_batch_norm(model_baseline)\n",
    "\n",
    "# # --- BASELINE HISTORY INITIALIZATION (REQUIRED FOR THIS SCOPE) ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median)\n",
    "# baseline_train_t1, baseline_train_t2, baseline_train_t3 = [], [], []\n",
    "# baseline_test_t1, baseline_test_t2, baseline_test_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# baseline_train_sd_t1, baseline_train_sd_t2, baseline_train_sd_t3 = [], [], []\n",
    "# baseline_test_sd_t1, baseline_test_sd_t2, baseline_test_sd_t3 = [], [], []\n",
    "\n",
    "# # Mask Scores (Mean/Median)\n",
    "# baseline_train_mask_t1, baseline_train_mask_t2, baseline_train_mask_t3 = [], [], []\n",
    "# baseline_test_mask_t1, baseline_test_mask_t2, baseline_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# baseline_train_mask_sd_t1, baseline_train_mask_sd_t2, baseline_train_mask_sd_t3 = [], [], []\n",
    "# baseline_test_mask_sd_t1, baseline_test_mask_sd_t2, baseline_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting ConvLSTM Baseline Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# # print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "# # current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES)\n",
    "# # current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device)\n",
    "# # print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "# current_train_data = None\n",
    "\n",
    "# # --- TRAINING LOOP (100 Epochs) ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- DYNAMIC DATA GENERATION ---\n",
    "#     if epoch % DATA_GENERATION_EPOCH_CYCLE == 0:\n",
    "#         print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "#         # NOTE: Replace 'generate_synthetic_faf_dataset' with your actual function.\n",
    "#         # It must return a PyTorch tensor of shape (N_samples, 3, 4, 256, 256).\n",
    "#         current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES)\n",
    "#         current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device) # Ensure it's on device\n",
    "#         print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "    \n",
    "#     # --- 1. Training Step (Using SWAU_Net's accumulated loss function) ---\n",
    "#     # The training function uses the newly generated 'current_train_data'\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, model_baseline, optimizer_baseline, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=1e-2, lambda_faf=0.5, lambda_mask=2.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.5, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 2. Evaluation Step (Median/SD) ---\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, model_baseline, args, soft_dice=soft_dice, use_median=True)\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, model_baseline, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 3. Accumulation ---\n",
    "#     # Residual Scores\n",
    "#     baseline_train_t1.append(res_train_scores[0]); baseline_train_t2.append(res_train_scores[1]); baseline_train_t3.append(res_train_scores[2])\n",
    "#     baseline_test_t1.append(res_test_scores[0]); baseline_test_t2.append(res_test_scores[1]); baseline_test_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     baseline_train_sd_t1.append(res_train_sds[0]); baseline_train_sd_t2.append(res_train_sds[1]); baseline_train_sd_t3.append(res_train_sds[2])\n",
    "#     baseline_test_sd_t1.append(res_test_sds[0]); baseline_test_sd_t2.append(res_test_sds[1]); baseline_test_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     baseline_train_mask_t1.append(msk_train_scores[0]); baseline_train_mask_t2.append(msk_train_scores[1]); baseline_train_mask_t3.append(msk_train_scores[2])\n",
    "#     baseline_test_mask_t1.append(msk_test_scores[0]); baseline_test_mask_t2.append(msk_test_scores[1]); baseline_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     baseline_train_mask_sd_t1.append(msk_train_sds[0]); baseline_train_mask_sd_t2.append(msk_train_sds[1]); baseline_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     baseline_test_mask_sd_t1.append(msk_test_sds[0]); baseline_test_mask_sd_t2.append(msk_test_sds[1]); baseline_test_mask_sd_t3.append(msk_test_sds[2]) # Corrected logic using test_sds\n",
    "\n",
    "#     # --- 4. Scheduler & Logging ---\n",
    "#     scheduler_baseline.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_baseline.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     f_display_frames(current_train_data, model_baseline, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         baseline_train_t1, baseline_train_t2, baseline_train_t3,\n",
    "#         baseline_test_t1, baseline_test_t2, baseline_test_t3,\n",
    "#         baseline_train_sd_t1, baseline_train_sd_t2, baseline_train_sd_t3,\n",
    "#         baseline_test_sd_t1, baseline_test_sd_t2, baseline_test_sd_t3,\n",
    "#         plot_title='ConvLSTM Baseline Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History\n",
    "#     plot_train_test_dice_history(\n",
    "#         baseline_train_mask_t1, baseline_train_mask_t2, baseline_train_mask_t3,\n",
    "#         baseline_test_mask_t1, baseline_test_mask_t2, baseline_test_mask_t3,\n",
    "#         baseline_train_mask_sd_t1, baseline_train_mask_sd_t2, baseline_train_mask_sd_t3,\n",
    "#         baseline_test_mask_sd_t1, baseline_test_mask_sd_t2, baseline_test_mask_sd_t3,\n",
    "#         plot_title='ConvLSTM Baseline Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15929525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Setup for Plotting ---\n",
    "\n",
    "# model = model_baseline\n",
    "\n",
    "# ## Plot with soft DICE (Residual and Mask)\n",
    "# soft_dice = True\n",
    "# metric_type_str = \"Soft Dice\"\n",
    "\n",
    "# (res_scores_test, msk_scores_test), _ = f_get_individual_dice(\n",
    "#     test_dataset, model, args, is_train_set=False, soft_dice=soft_dice\n",
    "# )\n",
    "\n",
    "# # 2. Plot Residuals (Soft) - (Plotting logic remains correct)\n",
    "# f_plot_individual_dice(res_scores_test, res_scores_test, metric_type_str, channel_name='Residual Mask')\n",
    "\n",
    "# # 3. Plot Masks (Soft)\n",
    "# f_plot_individual_dice(msk_scores_test, msk_scores_test, metric_type_str, channel_name='Full Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af0783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "# saved_path = save_model_weights(\n",
    "#     model=model_baseline, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     save_dir=ckpt_save_dir,\n",
    "#     model_name = \"ConvLSTM_baseline_pretrain\"\n",
    "    \n",
    "# )\n",
    "\n",
    "# # del model_baseline\n",
    "# # torch.cuda.empty_cache()\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcf0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # Load the model\n",
    "# MODEL_FILENAME = \"ConvLSTM_baseline_synthetic_pretrain_epoch50_20251104_163556.pth\" \n",
    "# MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# loaded_model, loaded_epoch = load_model(\n",
    "#     model=model_baseline, \n",
    "#     model_path=MODEL_PATH, \n",
    "#     device=args.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c475647",
   "metadata": {},
   "source": [
    "## Pretrain Simple ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Update (Reconfirmed) ---\n",
    "BASE_CHANNELS = 16 # 16 channels\n",
    "args.img_channels = 3 # 3 channels (FAF, Mask, Residual)\n",
    "\n",
    "# Function to count trainable parameters (Provided in setup)\n",
    "def count_parameters(model):\n",
    "    \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "print(\"\\nInstantiating the ConvLSTM_Simple model...\")\n",
    "\n",
    "# Instantiate the simple CNN-based model\n",
    "# NOTE: This model uses CNN_Unet_Enc and CNN_Unet_Dec\n",
    "model_simple = ConvLSTM_Simple(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# Calculate parameters for each main component\n",
    "e1_params = count_parameters(model_simple.E1)\n",
    "p_lstm_params = count_parameters(model_simple.P_LSTM) # ConvLSTM Core\n",
    "d1_params = count_parameters(model_simple.D1)\n",
    "total_params = e1_params + p_lstm_params + d1_params\n",
    "\n",
    "# --- Create Table Data ---\n",
    "param_data = [\n",
    "    [\"CNN_Unet_Enc (E1)\", \"Feature Extractor (Ablated CNN)\", f\"{e1_params:,}\"],\n",
    "    [\"ConvLSTMCore (P_LSTM)\", \"**Recurrent Temporal Core**\", f\"**{p_lstm_params:,}**\"],\n",
    "    [\"CNN_Unet_Dec (D1)\", \"Frame Reconstructor (Ablated CNN)\", f\"{d1_params:,}\"],\n",
    "    [\"\", \"\", \"\"], # Separator\n",
    "    [\"**TOTAL**\", \"**ConvLSTM_Simple Model**\", f\"**{total_params:,}**\"],\n",
    "]\n",
    "\n",
    "# --- Print Table ---\n",
    "print(\"\\n### ConvLSTM_Simple Parameter Summary\\n\")\n",
    "print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INITIALIZATION AND HYPERPARAMETER SETUP ---\n",
    "\n",
    "# --- DYNAMIC DATA GENERATION CONFIG ---\n",
    "# Define the size of the synthetic training set to generate each time\n",
    "NUM_TRAIN_SAMPLES = 100 # Example size, adjust as needed\n",
    "DATA_GENERATION_EPOCH_CYCLE = 5 \n",
    "# ------------------------------------------\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "args.num_epochs = 50\n",
    "ACCUMULATION_STEPS = 8 \n",
    "soft_dice = True # Use Soft Dice for stability\n",
    "lr = 1E-3 # Initial LR\n",
    "\n",
    "# Update: model_baseline replaced with model_simple\n",
    "optimizer_simple = torch.optim.Adam(model_simple.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# Update: scheduler_baseline replaced with scheduler_simple\n",
    "scheduler_simple = ReduceLROnPlateau(\n",
    "    optimizer_simple, \n",
    "    mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "loss_fn_dice = dice_loss # This relies on your custom dice_loss function\n",
    "loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# LLR_WEIGHT and BOTTLENECK_L2_WEIGHT are correctly used below\n",
    "BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# Freeze Batch Norm layers (essential for small batches)\n",
    "# Update: model_baseline replaced with model_simple\n",
    "freeze_batch_norm(model_simple)\n",
    "\n",
    "# --- BASELINE HISTORY INITIALIZATION (REQUIRED FOR THIS SCOPE) ---\n",
    "\n",
    "# Loss/Iteration Tracking\n",
    "all_iteration_losses = [] \n",
    "epoch_iteration_counts = []\n",
    "\n",
    "# Update: prefix changed from 'baseline' to 'simple'\n",
    "# Residual Scores (Mean/Median)\n",
    "simple_train_t1, simple_train_t2, simple_train_t3 = [], [], []\n",
    "simple_test_t1, simple_test_t2, simple_test_t3 = [], [], []\n",
    "# Residual SDs\n",
    "simple_train_sd_t1, simple_train_sd_t2, simple_train_sd_t3 = [], [], []\n",
    "simple_test_sd_t1, simple_test_sd_t2, simple_test_sd_t3 = [], [], []\n",
    "\n",
    "# Mask Scores (Mean/Median)\n",
    "simple_train_mask_t1, simple_train_mask_t2, simple_train_mask_t3 = [], [], []\n",
    "simple_test_mask_t1, simple_test_mask_t2, simple_test_mask_t3 = [], [], []\n",
    "# Mask SDs\n",
    "simple_train_mask_sd_t1, simple_train_mask_sd_t2, simple_train_mask_sd_t3 = [], [], []\n",
    "simple_test_mask_sd_t1, simple_test_mask_sd_t2, simple_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# Update: Log message changed\n",
    "print(f\"\\n Starting ConvLSTM Simple Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "current_train_data = None\n",
    "\n",
    "# --- TRAINING LOOP (args.num_epochs) ---\n",
    "for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "    # --- DYNAMIC DATA GENERATION ---\n",
    "    if epoch % DATA_GENERATION_EPOCH_CYCLE == 0:\n",
    "        print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "        # NOTE: Replace 'generate_synthetic_faf_dataset' with your actual function.\n",
    "        # It must return a PyTorch tensor of shape (N_samples, 3, 4, 256, 256).\n",
    "        current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES)\n",
    "        current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device) # Ensure it's on device\n",
    "        print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "    \n",
    "    # --- 1. Training Step (Using SWAU_Net's accumulated loss function) ---\n",
    "    # Update: model_baseline and optimizer_baseline replaced with model_simple and optimizer_simple\n",
    "    epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "        current_train_data, model_simple, optimizer_simple, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "        accumulation_steps=ACCUMULATION_STEPS, \n",
    "        lambda_gdl=1e-2, lambda_faf=0.5, lambda_mask=2.0, lambda_residual=5.0, \n",
    "        lambda_recon=0.5, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "    )\n",
    "\n",
    "    all_iteration_losses.extend(epoch_losses.tolist())\n",
    "    epoch_iteration_counts.append(len(epoch_losses))\n",
    "    mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "    # --- 2. Evaluation Step (Median/SD) ---\n",
    "    # Update: model_baseline replaced with model_simple\n",
    "    (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "        f_eval_pred_dice_test_set(test_loader, model_simple, args, soft_dice=soft_dice, use_median=True)\n",
    "    (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "        f_eval_pred_dice_train_set(current_train_data, model_simple, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "    # --- 3. Accumulation ---\n",
    "    # Update: Variable names changed from 'baseline' to 'simple'\n",
    "    # Residual Scores\n",
    "    simple_train_t1.append(res_train_scores[0]); simple_train_t2.append(res_train_scores[1]); simple_train_t3.append(res_train_scores[2])\n",
    "    simple_test_t1.append(res_test_scores[0]); simple_test_t2.append(res_test_scores[1]); simple_test_t3.append(res_test_scores[2])\n",
    "    # Residual SDs\n",
    "    simple_train_sd_t1.append(res_train_sds[0]); simple_train_sd_t2.append(res_train_sds[1]); simple_train_sd_t3.append(res_train_sds[2])\n",
    "    simple_test_sd_t1.append(res_test_sds[0]); simple_test_sd_t2.append(res_test_sds[1]); simple_test_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "    # Mask Scores\n",
    "    simple_train_mask_t1.append(msk_train_scores[0]); simple_train_mask_t2.append(msk_train_scores[1]); simple_train_mask_t3.append(msk_train_scores[2])\n",
    "    simple_test_mask_t1.append(msk_test_scores[0]); simple_test_mask_t2.append(msk_test_scores[1]); simple_test_mask_t3.append(msk_test_scores[2])\n",
    "    # Mask SDs\n",
    "    simple_train_mask_sd_t1.append(msk_train_sds[0]); simple_train_mask_sd_t2.append(msk_train_sds[1]); simple_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "    simple_test_mask_sd_t1.append(msk_test_sds[0]); simple_test_mask_sd_t2.append(msk_test_sds[1]); simple_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "    # --- 4. Scheduler & Logging ---\n",
    "    # Update: scheduler_baseline replaced with scheduler_simple\n",
    "    scheduler_simple.step(mean_epoch_loss)\n",
    "\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_simple.param_groups[0]['lr']:.2e}) ---\")\n",
    "    print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "    print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "    # --- Per-Epoch Visualizations ---\n",
    "    print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "    # A. Plot Loss History\n",
    "    plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "    # B. Plot Sample Prediction\n",
    "    # Update: model_baseline replaced with model_simple\n",
    "    f_display_frames(current_train_data, model_simple, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "    # C. Plot Residual History\n",
    "    # Update: Variable names and plot title changed\n",
    "    plot_train_test_dice_history(\n",
    "        simple_train_t1, simple_train_t2, simple_train_t3,\n",
    "        simple_test_t1, simple_test_t2, simple_test_t3,\n",
    "        simple_train_sd_t1, simple_train_sd_t2, simple_train_sd_t3,\n",
    "        simple_test_sd_t1, simple_test_sd_t2, simple_test_sd_t3,\n",
    "        plot_title='ConvLSTM Simple Residual Dice History (Median ± SD)'\n",
    "    )\n",
    "\n",
    "    # D. Plot Mask History\n",
    "    # Update: Variable names and plot title changed\n",
    "    plot_train_test_dice_history(\n",
    "        simple_train_mask_t1, simple_train_mask_t2, simple_train_mask_t3,\n",
    "        simple_test_mask_t1, simple_test_mask_t2, simple_test_mask_t3,\n",
    "        simple_train_mask_sd_t1, simple_train_mask_sd_t2, simple_train_mask_sd_t3,\n",
    "        simple_test_mask_sd_t1, simple_test_mask_sd_t2, simple_test_mask_sd_t3,\n",
    "        plot_title='ConvLSTM Simple Full Mask Dice History (Median ± SD)'\n",
    "    )\n",
    "\n",
    "# --- Final Message ---\n",
    "print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "FINAL_EPOCH = args.num_epochs\n",
    "saved_path = save_model_weights(\n",
    "    model=model_simple,\n",
    "    final_epoch=FINAL_EPOCH,\n",
    "    save_dir=ckpt_save_dir,\n",
    "    model_name = \"ConvLSTM_simple_pretrain\"\n",
    "    \n",
    ")\n",
    "\n",
    "# del model_simple\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d75b7",
   "metadata": {},
   "source": [
    "## Pretrain SWAU Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Reduced from 24 to 16\n",
    "# args.d_attn1 = 128 # Reduced from 192 to 128\n",
    "# args.d_attn2 = 256 # Reduced from 384 to 256\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the full SWAU_Net model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the full model and move it to the device\n",
    "# # The model is SWAU_Net, which owns E1, CFB_enc, CFB_dec, SWA, P, and D1.\n",
    "# swau_model = SWAU_Net(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(swau_model.E1)\n",
    "\n",
    "# # CORRECTED: Calculate parameters for both CFB modules separately\n",
    "# cfb_enc_params = count_parameters(swau_model.CFB_enc) \n",
    "# cfb_dec_params = count_parameters(swau_model.CFB_dec) \n",
    "# cfb_total_params = cfb_enc_params + cfb_dec_params\n",
    "\n",
    "# swa_params = count_parameters(swau_model.SWA) \n",
    "# p_params = count_parameters(swau_model.P)\n",
    "# d1_params = count_parameters(swau_model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + cfb_total_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (Time t)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB (Total, 2x Modules)\", \"**Pre/Post-Dynamics Mixer**\", f\"**{cfb_total_params:,}**\"], # NEW LINE: Aggregate CFB\n",
    "#     [\"SlidingWindowAttention (SWA)\", \"**Feature Aggregator/Integrator**\", f\"**{swa_params:,}**\"], \n",
    "#     [\"DynNet (P)\", \"Temporal Feature Predictor (M_t → Evolved_t)\", f\"{p_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor (Time t+1)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**Full SWAU_Net Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### SWAU_Net Component Parameter Summary\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DYNAMIC DATA GENERATION CONFIG ---\n",
    "# # Define the size of the synthetic training set to generate each time\n",
    "# NUM_TRAIN_SAMPLES = 100 # Example size, adjust as needed\n",
    "# DATA_GENERATION_EPOCH_CYCLE = 5 \n",
    "# # ------------------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS\n",
    "# args.num_epochs = 50\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = True # Use Soft Dice for stability\n",
    "# lr = 1E-3 # Initial LR\n",
    "\n",
    "# # --- MODEL, OPTIMIZER, SCHEDULER RENAMING ---\n",
    "# # Assuming the instantiated model is now referred to as 'swau_model'\n",
    "# # (The old 'model_baseline' reference will be replaced by 'swau_model' in function calls)\n",
    "# optimizer_swau = torch.optim.Adam(swau_model.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# scheduler_swau = ReduceLROnPlateau(\n",
    "#     optimizer_swau, \n",
    "#     mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss # This relies on your custom dice_loss function\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# freeze_batch_norm(swau_model) # Use swau_model\n",
    "\n",
    "# # --- HISTORY INITIALIZATION (REQUIRED FOR THIS SCOPE) ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median) - RENAMED to swau_...\n",
    "# swau_train_t1, swau_train_t2, swau_train_t3 = [], [], []\n",
    "# swau_test_t1, swau_test_t2, swau_test_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# swau_train_sd_t1, swau_train_sd_t2, swau_train_sd_t3 = [], [], []\n",
    "# swau_test_sd_t1, swau_test_sd_t2, swau_test_sd_t3 = [], [], []\n",
    "# # Mask Scores (Mean/Median)\n",
    "# swau_train_mask_t1, swau_train_mask_t2, swau_train_mask_t3 = [], [], []\n",
    "# swau_test_mask_t1, swau_test_mask_t2, swau_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# swau_train_mask_sd_t1, swau_train_mask_sd_t2, swau_train_mask_sd_t3 = [], [], []\n",
    "# swau_test_mask_sd_t1, swau_test_mask_sd_t2, swau_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting SWAU Model Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# current_train_data = None\n",
    "\n",
    "# # --- TRAINING LOOP (50 Epochs) ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- DYNAMIC DATA GENERATION ---\n",
    "#     if epoch % DATA_GENERATION_EPOCH_CYCLE == 0:\n",
    "#         print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "#         current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES)\n",
    "#         current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device)\n",
    "#         print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "    \n",
    "#     # --- 1. Training Step (Using SWAU_Net's accumulated loss function) ---\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, swau_model, optimizer_swau, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=1e-2, lambda_faf=0.5, lambda_mask=2.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.5, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 2. Evaluation Step (Median/SD) ---\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, swau_model, args, soft_dice=soft_dice, use_median=True)\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, swau_model, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 3. Accumulation (REVISED for consistency) ---\n",
    "#     # Residual Scores\n",
    "#     swau_train_t1.append(res_train_scores[0]); swau_train_t2.append(res_train_scores[1]); swau_train_t3.append(res_train_scores[2])\n",
    "#     swau_test_t1.append(res_test_scores[0]); swau_test_t2.append(res_test_scores[1]); swau_test_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     swau_train_sd_t1.append(res_train_sds[0]); swau_train_sd_t2.append(res_train_sds[1]); swau_train_sd_t3.append(res_train_sds[2])\n",
    "#     swau_test_sd_t1.append(res_test_sds[0]); swau_test_sd_t2.append(res_test_sds[1]); swau_test_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     swau_train_mask_t1.append(msk_train_scores[0]); swau_train_mask_t2.append(msk_train_scores[1]); swau_train_mask_t3.append(msk_train_scores[2])\n",
    "#     swau_test_mask_t1.append(msk_test_scores[0]); swau_test_mask_t2.append(msk_test_scores[1]); swau_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     swau_train_mask_sd_t1.append(msk_train_sds[0]); swau_train_mask_sd_t2.append(msk_train_sds[1]); swau_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     swau_test_mask_sd_t1.append(msk_test_sds[0]); swau_test_mask_sd_t2.append(msk_test_sds[1]); swau_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "#     # --- 4. Scheduler & Logging ---\n",
    "#     scheduler_swau.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_swau.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     f_display_frames(current_train_data, swau_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         swau_train_t1, swau_train_t2, swau_train_t3,\n",
    "#         swau_test_t1, swau_test_t2, swau_test_t3,\n",
    "#         swau_train_sd_t1, swau_train_sd_t2, swau_train_sd_t3,\n",
    "#         swau_test_sd_t1, swau_test_sd_t2, swau_test_sd_t3,\n",
    "#         plot_title='SWAU Model Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History\n",
    "#     plot_train_test_dice_history(\n",
    "#         swau_train_mask_t1, swau_train_mask_t2, swau_train_mask_t3,\n",
    "#         swau_test_mask_t1, swau_test_mask_t2, swau_test_mask_t3,\n",
    "#         swau_train_mask_sd_t1, swau_train_mask_sd_t2, swau_train_mask_sd_t3,\n",
    "#         swau_test_mask_sd_t1, swau_test_mask_sd_t2, swau_test_mask_sd_t3,\n",
    "#         plot_title='SWAU Model Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "# saved_path = save_model_weights(\n",
    "#     model=swau_model, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     save_dir=ckpt_save_dir,\n",
    "#     model_name = \"SWAU_synthetic_pretrain\"\n",
    "# )\n",
    "\n",
    "# # # del swau_modelckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # # Load the model\n",
    "# # MODEL_FILENAME = \"SWAU_synthetic_pretrain_epoch50_20251104_185416.pth\" \n",
    "# # MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# # loaded_model, loaded_epoch = load_model(\n",
    "# #     model=swau_model, \n",
    "# #     model_path=MODEL_PATH, \n",
    "# #     device=args.device\n",
    "# # )\n",
    "# # # torch.cuda.empty_cache()\n",
    "# # # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ad36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # Load the model\n",
    "# MODEL_FILENAME = \"SWAU_synthetic_pretrain_epoch50_20251104_185416.pth\" \n",
    "# MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# loaded_model, loaded_epoch = load_model(\n",
    "#     model=swau_model, \n",
    "#     model_path=MODEL_PATH, \n",
    "#     device=args.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493de90",
   "metadata": {},
   "source": [
    "## Pretrain CFB Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e361b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Reduced from 24 to 16\n",
    "# args.d_attn1 = 128 # Reduced from 192 to 128\n",
    "# args.d_attn2 = 256 # Reduced from 384 to 256\n",
    "\n",
    "# args.num_attn_layers = 2\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the full SWAU_Net model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the full model and move it to the device\n",
    "# # The model is SWAU_Net, which owns E1, CFB_enc, CFB_dec, SWA, P, and D1.\n",
    "# swau_model = SWAU_CFB_Ablation(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(swau_model.E1)\n",
    "\n",
    "# swa_params = count_parameters(swau_model.SWA) \n",
    "# p_params = count_parameters(swau_model.P)\n",
    "# d1_params = count_parameters(swau_model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (Time t)\", f\"{e1_params:,}\"],\n",
    "#     [\"SlidingWindowAttention (SWA)\", \"**Feature Aggregator/Integrator**\", f\"**{swa_params:,}**\"], \n",
    "#     [\"DynNet (P)\", \"Temporal Feature Predictor (M_t → Evolved_t)\", f\"{p_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor (Time t+1)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**Full SWAU_Net Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### SWAU_Net Component Parameter Summary\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa504cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DYNAMIC DATA GENERATION CONFIG ---\n",
    "# # Define the size of the synthetic training set to generate each time\n",
    "# NUM_TRAIN_SAMPLES = 100 # Example size, adjust as needed\n",
    "# DATA_GENERATION_EPOCH_CYCLE = 5 \n",
    "# # ------------------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS\n",
    "# args.num_epochs = 50\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = True # Use Soft Dice for stability\n",
    "# lr = 1E-3 # Initial LR\n",
    "\n",
    "# # --- MODEL, OPTIMIZER, SCHEDULER RENAMING ---\n",
    "# # Assuming the instantiated model is now referred to as 'swau_model'\n",
    "# # (The old 'model_baseline' reference will be replaced by 'swau_model' in function calls)\n",
    "# optimizer_swau = torch.optim.Adam(swau_model.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# scheduler_swau = ReduceLROnPlateau(\n",
    "#     optimizer_swau, \n",
    "#     mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss # This relies on your custom dice_loss function\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# freeze_batch_norm(swau_model) # Use swau_model\n",
    "\n",
    "# # --- HISTORY INITIALIZATION (REQUIRED FOR THIS SCOPE) ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median) - RENAMED to swau_...\n",
    "# swau_train_t1, swau_train_t2, swau_train_t3 = [], [], []\n",
    "# swau_test_t1, swau_test_t2, swau_test_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# swau_train_sd_t1, swau_train_sd_t2, swau_train_sd_t3 = [], [], []\n",
    "# swau_test_sd_t1, swau_test_sd_t2, swau_test_sd_t3 = [], [], []\n",
    "# # Mask Scores (Mean/Median)\n",
    "# swau_train_mask_t1, swau_train_mask_t2, swau_train_mask_t3 = [], [], []\n",
    "# swau_test_mask_t1, swau_test_mask_t2, swau_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# swau_train_mask_sd_t1, swau_train_mask_sd_t2, swau_train_mask_sd_t3 = [], [], []\n",
    "# swau_test_mask_sd_t1, swau_test_mask_sd_t2, swau_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting SWAU Model Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# current_train_data = None\n",
    "\n",
    "# # --- TRAINING LOOP (50 Epochs) ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- DYNAMIC DATA GENERATION ---\n",
    "#     if epoch % DATA_GENERATION_EPOCH_CYCLE == 0:\n",
    "#         print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "#         current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES)\n",
    "#         current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device)\n",
    "#         print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "    \n",
    "#     # --- 1. Training Step (Using SWAU_Net's accumulated loss function) ---\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, swau_model, optimizer_swau, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=1e-2, lambda_faf=0.5, lambda_mask=2.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.5, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 2. Evaluation Step (Median/SD) ---\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, swau_model, args, soft_dice=soft_dice, use_median=True)\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, swau_model, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 3. Accumulation (REVISED for consistency) ---\n",
    "#     # Residual Scores\n",
    "#     swau_train_t1.append(res_train_scores[0]); swau_train_t2.append(res_train_scores[1]); swau_train_t3.append(res_train_scores[2])\n",
    "#     swau_test_t1.append(res_test_scores[0]); swau_test_t2.append(res_test_scores[1]); swau_test_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     swau_train_sd_t1.append(res_train_sds[0]); swau_train_sd_t2.append(res_train_sds[1]); swau_train_sd_t3.append(res_train_sds[2])\n",
    "#     swau_test_sd_t1.append(res_test_sds[0]); swau_test_sd_t2.append(res_test_sds[1]); swau_test_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     swau_train_mask_t1.append(msk_train_scores[0]); swau_train_mask_t2.append(msk_train_scores[1]); swau_train_mask_t3.append(msk_train_scores[2])\n",
    "#     swau_test_mask_t1.append(msk_test_scores[0]); swau_test_mask_t2.append(msk_test_scores[1]); swau_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     swau_train_mask_sd_t1.append(msk_train_sds[0]); swau_train_mask_sd_t2.append(msk_train_sds[1]); swau_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     swau_test_mask_sd_t1.append(msk_test_sds[0]); swau_test_mask_sd_t2.append(msk_test_sds[1]); swau_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "#     # --- 4. Scheduler & Logging ---\n",
    "#     scheduler_swau.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_swau.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     f_display_frames(current_train_data, swau_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         swau_train_t1, swau_train_t2, swau_train_t3,\n",
    "#         swau_test_t1, swau_test_t2, swau_test_t3,\n",
    "#         swau_train_sd_t1, swau_train_sd_t2, swau_train_sd_t3,\n",
    "#         swau_test_sd_t1, swau_test_sd_t2, swau_test_sd_t3,\n",
    "#         plot_title='SWAU Model Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History\n",
    "#     plot_train_test_dice_history(\n",
    "#         swau_train_mask_t1, swau_train_mask_t2, swau_train_mask_t3,\n",
    "#         swau_test_mask_t1, swau_test_mask_t2, swau_test_mask_t3,\n",
    "#         swau_train_mask_sd_t1, swau_train_mask_sd_t2, swau_train_mask_sd_t3,\n",
    "#         swau_test_mask_sd_t1, swau_test_mask_sd_t2, swau_test_mask_sd_t3,\n",
    "#         plot_title='SWAU Model Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "# saved_path = save_model_weights(\n",
    "#     model=swau_model, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     save_dir=ckpt_save_dir,\n",
    "#     model_name = \"CFB_Ablation_synthetic_pretrain\"\n",
    "# )\n",
    "\n",
    "# # del swau_modelckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # Load the model\n",
    "# MODEL_FILENAME = \"SWAU_synthetic_pretrain_epoch50_20251104_185416.pth\" \n",
    "# MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# loaded_model, loaded_epoch = load_model(\n",
    "#     model=swau_model, \n",
    "#     model_path=MODEL_PATH, \n",
    "#     device=args.device\n",
    "# )\n",
    "# # torch.cuda.empty_cache()\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8101b93",
   "metadata": {},
   "source": [
    "## Pretrain DynNet Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Base channel width\n",
    "# args.d_attn1 = 128 # Feed-forward dim for L3\n",
    "# args.d_attn2 = 256 # Feed-forward dim for L4/L5\n",
    "\n",
    "# args.num_attn_layers = 2 # Number of SWA layers\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the SWAU_DynNet_Ablation model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the ablation model and move it to the device\n",
    "# # NOTE: The CFB components are now included inside E1 and D1 or explicitly defined.\n",
    "# # Using the SWAU_Net_NoDynNet_Decoupled class.\n",
    "# swau_dynnet_ablation_model = SWAU_DynNet_Ablation(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(swau_dynnet_ablation_model.E1)\n",
    "# cfb_enc_params = count_parameters(swau_dynnet_ablation_model.CFB_enc)\n",
    "# cfb_dec_params = count_parameters(swau_dynnet_ablation_model.CFB_dec)\n",
    "# swa_params = count_parameters(swau_dynnet_ablation_model.SWA)\n",
    "\n",
    "# # DynNet is removed, so its parameter count is 0.\n",
    "# p_params = 0 \n",
    "\n",
    "# d1_params = count_parameters(swau_dynnet_ablation_model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count (including CFB blocks)\n",
    "# total_params = e1_params + cfb_enc_params + cfb_dec_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (Time t)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB_enc\", \"Pre-Aggregation Channel Refinement\", f\"{cfb_enc_params:,}\"],\n",
    "#     [\"SlidingWindowAttention (SWA)\", \"**Feature Aggregator/Estimator**\", f\"**{swa_params:,}**\"],\n",
    "#     [\"DynNet (P)\", \"Temporal Evolution Module\", f\"**{p_params:,} (Removed)**\"], # P_params = 0\n",
    "#     [\"CFB_dec\", \"Post-Aggregation Channel Refinement\", f\"{cfb_dec_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor (Time t+1)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**SWAU_DynNet_Ablation Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### SWAU_DynNet_Ablation Component Parameter Summary\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a99e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DYNAMIC DATA GENERATION CONFIG ---\n",
    "# # Define the size of the synthetic training set to generate each time\n",
    "# NUM_TRAIN_SAMPLES = 100 # Example size, adjust as needed\n",
    "# DATA_GENERATION_EPOCH_CYCLE = 5 \n",
    "# # ------------------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS\n",
    "# args.num_epochs = 50\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = True # Use Soft Dice for stability\n",
    "# lr = 1E-3 # Initial LR\n",
    "\n",
    "# # --- MODEL INSTANTIATION REFERENCE ---\n",
    "# # NOTE: The model swau_dynnet_ablation_model is already defined in the preceding block.\n",
    "# # We will treat this instance as the working 'swau_model' for the loop, \n",
    "# # although we must use the original instance variable name (swau_dynnet_ablation_model) \n",
    "# # for the optimizer and scheduler definitions to ensure continuity.\n",
    "\n",
    "# # Renaming the existing instance for clarity within the loop context:\n",
    "# swau_model = swau_dynnet_ablation_model\n",
    "\n",
    "# # --- MODEL, OPTIMIZER, SCHEDULER ---\n",
    "# optimizer_swau = torch.optim.Adam(swau_model.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# # NOTE: Assume ReduceLROnPlateau is defined elsewhere\n",
    "# scheduler_swau = ReduceLROnPlateau(\n",
    "#     optimizer_swau, \n",
    "#     mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "# # NOTE: Assume dice_loss and GDLoss are defined elsewhere.\n",
    "# loss_fn_dice = dice_loss \n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# # NOTE: Assume freeze_batch_norm is defined elsewhere.\n",
    "# freeze_batch_norm(swau_model) \n",
    "\n",
    "# # --- HISTORY INITIALIZATION (REQUIRED FOR THIS SCOPE) ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median) - RENAMED to swau_...\n",
    "# swau_train_t1, swau_train_t2, swau_train_t3 = [], [], []\n",
    "# swau_test_t1, swau_test_t2, swau_test_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# swau_train_sd_t1, swau_train_sd_t2, swau_train_sd_t3 = [], [], []\n",
    "# swau_test_sd_t1, swau_test_sd_t2, swau_test_sd_t3 = [], [], []\n",
    "# # Mask Scores (Mean/Median)\n",
    "# swau_train_mask_t1, swau_train_mask_t2, swau_train_mask_t3 = [], [], []\n",
    "# swau_test_mask_t1, swau_test_mask_t2, swau_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# swau_train_mask_sd_t1, swau_train_mask_sd_t2, swau_train_mask_sd_t3 = [], [], []\n",
    "# swau_test_mask_sd_t1, swau_test_mask_sd_t2, swau_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting SWAU Model Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# current_train_data = None\n",
    "\n",
    "# # --- TRAINING LOOP (50 Epochs) ---\n",
    "# # NOTE: Assume tqdm, generate_synthetic_faf_dataset, f_single_epoch_spatiotemporal_accumulated, \n",
    "# # f_eval_pred_dice_test_set, f_eval_pred_dice_train_set, plot_log_loss, and f_display_frames are defined elsewhere.\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- DYNAMIC DATA GENERATION ---\n",
    "#     if epoch % DATA_GENERATION_EPOCH_CYCLE == 0:\n",
    "#         print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "#         # NOTE: Assume generate_synthetic_faf_dataset is available\n",
    "#         current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES)\n",
    "#         current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device)\n",
    "#         print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "    \n",
    "#     # --- 1. Training Step (Using SWAU_Net's accumulated loss function) ---\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, swau_model, optimizer_swau, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=1e-2, lambda_faf=0.5, lambda_mask=2.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.5, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 2. Evaluation Step (Median/SD) ---\n",
    "#     # NOTE: Assume f_eval_pred_dice_* functions and test_loader are available\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, swau_model, args, soft_dice=soft_dice, use_median=True)\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, swau_model, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 3. Accumulation (REVISED for consistency) ---\n",
    "#     # Residual Scores\n",
    "#     swau_train_t1.append(res_train_scores[0]); swau_train_t2.append(res_train_scores[1]); swau_train_t3.append(res_train_scores[2])\n",
    "#     swau_test_t1.append(res_test_scores[0]); swau_test_t2.append(res_test_scores[1]); swau_test_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     swau_train_sd_t1.append(res_train_sds[0]); swau_train_sd_t2.append(res_train_sds[1]); swau_train_sd_t3.append(res_train_sds[2])\n",
    "#     swau_test_sd_t1.append(res_test_sds[0]); swau_test_sd_t2.append(res_test_sds[1]); swau_test_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     swau_train_mask_t1.append(msk_train_scores[0]); swau_train_mask_t2.append(msk_train_scores[1]); swau_train_mask_t3.append(msk_train_scores[2])\n",
    "#     swau_test_mask_t1.append(msk_test_scores[0]); swau_test_mask_t2.append(msk_test_scores[1]); swau_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     swau_train_mask_sd_t1.append(msk_train_sds[0]); swau_train_mask_sd_t2.append(msk_train_sds[1]); swau_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     swau_test_mask_sd_t1.append(msk_test_sds[0]); swau_test_mask_sd_t2.append(msk_test_sds[1]); swau_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "#     # --- 4. Scheduler & Logging ---\n",
    "#     # NOTE: Assume scheduler_swau and plotting functions are available\n",
    "#     scheduler_swau.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_swau.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     f_display_frames(current_train_data, swau_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         swau_train_t1, swau_train_t2, swau_train_t3,\n",
    "#         swau_test_t1, swau_test_t2, swau_test_t3,\n",
    "#         swau_train_sd_t1, swau_train_sd_t2, swau_train_sd_t3,\n",
    "#         swau_test_sd_t1, swau_test_sd_t2, swau_test_sd_t3,\n",
    "#         plot_title='SWAU Model Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History\n",
    "#     plot_train_test_dice_history(\n",
    "#         swau_train_mask_t1, swau_train_mask_t2, swau_train_mask_t3,\n",
    "#         swau_test_mask_t1, swau_test_mask_t2, swau_test_mask_t3,\n",
    "#         swau_train_mask_sd_t1, swau_train_mask_sd_t2, swau_train_mask_sd_t3,\n",
    "#         swau_test_mask_sd_t1, swau_test_mask_sd_t2, swau_test_mask_sd_t3,\n",
    "#         plot_title='SWAU Model Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "# saved_path = save_model_weights(\n",
    "#     model=swau_model, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     save_dir=ckpt_save_dir,\n",
    "#     model_name = \"DynNet_Ablation_synthetic_pretrain\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc9b816",
   "metadata": {},
   "source": [
    "## Pretrain RKAU Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c89838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Pretrain RKAU-Net\n",
    "\n",
    "# # --- Configuration Update ---\n",
    "# BASE_CHANNELS = 16 # Retaining C=16\n",
    "# args.d_attn1 = 128 # Retaining d_attn1=128\n",
    "# args.d_attn2 = 256 # Retaining d_attn2=256\n",
    "# args.img_channels = 3\n",
    "# args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# args.num_attn_layers = 2\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the FULL RKAU_Net model (E1 -> CFB -> RKA -> DynNet -> CFB -> D1)...\")\n",
    "\n",
    "# # Instantiate the full model and move it to the device\n",
    "# rkau_model = RKAU_Net(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # --- Calculate parameters for each main component ---\n",
    "\n",
    "# e1_params = count_parameters(rkau_model.E1)\n",
    "# d1_params = count_parameters(rkau_model.D1)\n",
    "\n",
    "# # RKA is the Feature Aggregation module\n",
    "# rka_agg_params = count_parameters(rkau_model.RKA_Aggregator) \n",
    "\n",
    "# # P is the DynNet State Evolutionary Predictor\n",
    "# p_params = count_parameters(rkau_model.P) \n",
    "\n",
    "# # CFBs (Enc and Dec)\n",
    "# cfb_enc_params = count_parameters(rkau_model.CFB_enc)\n",
    "# cfb_dec_params = count_parameters(rkau_model.CFB_dec)\n",
    "# cfb_total_params = cfb_enc_params + cfb_dec_params\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + cfb_enc_params + rka_agg_params + p_params + cfb_dec_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (Spatial)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB_enc\", \"Pre-Dynamics Mixer (Refinement)\", f\"{cfb_enc_params:,}\"],\n",
    "#     [\"RKA Aggregator\", \"Temporal Aggregation Core (RKA)\", f\"**{rka_agg_params:,}**\"],\n",
    "#     [\"DynNet (P)\", \"State Evolutionary Predictor\", f\"**{p_params:,}**\"],\n",
    "#     [\"CFB_dec\", \"Post-Dynamics Mixer (Refinement)\", f\"{cfb_dec_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor (Spatial)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**Full RKAU_Net Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### RKAU_Net Component Parameter Summary (Full Architecture)\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b0226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DYNAMIC DATA GENERATION CONFIG ---\n",
    "# # Define the size of the synthetic training set to generate each time\n",
    "# NUM_TRAIN_SAMPLES = 100 # Example size, adjust as needed\n",
    "# DATA_GENERATION_EPOCH_CYCLE = 5 \n",
    "# # ------------------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS\n",
    "# args.num_epochs = 50\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = True # Use Soft Dice for stability\n",
    "# lr = 1E-3 # Initial LR\n",
    "\n",
    "# # --- MODEL, OPTIMIZER, SCHEDULER RENAMING ---\n",
    "# # NOTE: Assuming 'rkau_model' is the instantiated RKAU_Net model object\n",
    "# optimizer_rkau = torch.optim.Adam(rkau_model.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# scheduler_rkau = ReduceLROnPlateau(\n",
    "#     optimizer_rkau, \n",
    "#     mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss \n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# freeze_batch_norm(rkau_model) # Use rkau_model\n",
    "\n",
    "# # --- HISTORY INITIALIZATION (RENAMED to rkau_...) ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median)\n",
    "# rkau_train_t1, rkau_train_t2, rkau_train_t3 = [], [], []\n",
    "# rkau_test_t1, rkau_test_t2, rkau_test_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# rkau_train_sd_t1, rkau_train_sd_t2, rkau_train_sd_t3 = [], [], []\n",
    "# rkau_test_sd_t1, rkau_test_sd_t2, rkau_test_sd_t3 = [], [], []\n",
    "# # Mask Scores (Mean/Median)\n",
    "# rkau_train_mask_t1, rkau_train_mask_t2, rkau_train_mask_t3 = [], [], []\n",
    "# rkau_test_mask_t1, rkau_test_mask_t2, rkau_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# rkau_train_mask_sd_t1, rkau_train_mask_sd_t2, rkau_train_mask_sd_t3 = [], [], []\n",
    "# rkau_test_mask_sd_t1, rkau_test_mask_sd_t2, rkau_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting RKAU Model Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# current_train_data = None\n",
    "\n",
    "# # --- TRAINING LOOP (50 Epochs) ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- DYNAMIC DATA GENERATION ---\n",
    "#     if epoch % DATA_GENERATION_EPOCH_CYCLE == 0:\n",
    "#         print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "#         # NOTE: Assuming generate_synthetic_faf_dataset function is available\n",
    "#         current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES) \n",
    "#         current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device)\n",
    "#         print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "    \n",
    "#     # --- 1. Training Step (Using RKAU_Net's accumulated loss function) ---\n",
    "#     # NOTE: Function name f_single_epoch_spatiotemporal_accumulated retained, but called with rkau_model/optimizer_rkau\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, rkau_model, optimizer_rkau, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=1e-2, lambda_faf=0.5, lambda_mask=2.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.5, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 2. Evaluation Step (Median/SD) ---\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, rkau_model, args, soft_dice=soft_dice, use_median=True)\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, rkau_model, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 3. Accumulation (REVISED for consistency) ---\n",
    "#     # Residual Scores\n",
    "#     rkau_train_t1.append(res_train_scores[0]); rkau_train_t2.append(res_train_scores[1]); rkau_train_t3.append(res_train_scores[2])\n",
    "#     rkau_test_t1.append(res_test_scores[0]); rkau_test_t2.append(res_test_scores[1]); rkau_test_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     rkau_train_sd_t1.append(res_train_sds[0]); rkau_train_sd_t2.append(res_train_sds[1]); rkau_train_sd_t3.append(res_train_sds[2])\n",
    "#     rkau_test_sd_t1.append(res_test_sds[0]); rkau_test_sd_t2.append(res_test_sds[1]); rkau_test_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     rkau_train_mask_t1.append(msk_train_scores[0]); rkau_train_mask_t2.append(msk_train_scores[1]); rkau_train_mask_t3.append(msk_train_scores[2])\n",
    "#     rkau_test_mask_t1.append(msk_test_scores[0]); rkau_test_mask_t2.append(msk_test_scores[1]); rkau_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     rkau_train_mask_sd_t1.append(msk_train_sds[0]); rkau_train_mask_sd_t2.append(msk_train_sds[1]); rkau_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     rkau_test_mask_sd_t1.append(msk_test_sds[0]); rkau_test_mask_sd_t2.append(msk_test_sds[1]); rkau_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "#     # --- 4. Scheduler & Logging ---\n",
    "#     scheduler_rkau.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_rkau.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     f_display_frames(current_train_data, rkau_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         rkau_train_t1, rkau_train_t2, rkau_train_t3,\n",
    "#         rkau_test_t1, rkau_test_t2, rkau_test_t3,\n",
    "#         rkau_train_sd_t1, rkau_train_sd_t2, rkau_train_sd_t3,\n",
    "#         rkau_test_sd_t1, rkau_test_sd_t2, rkau_test_sd_t3,\n",
    "#         plot_title='RKAU Model Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History\n",
    "#     plot_train_test_dice_history(\n",
    "#         rkau_train_mask_t1, rkau_train_mask_t2, rkau_train_mask_t3,\n",
    "#         rkau_test_mask_t1, rkau_test_mask_t2, rkau_test_mask_t3,\n",
    "#         rkau_train_mask_sd_t1, rkau_train_mask_sd_t2, rkau_train_mask_sd_t3,\n",
    "#         rkau_test_mask_sd_t1, rkau_test_mask_sd_t2, rkau_test_mask_sd_t3,\n",
    "#         plot_title='RKAU Model Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca44c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "# saved_path = save_model_weights(\n",
    "#     model=rkau_model, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     save_dir=ckpt_save_dir,\n",
    "#     model_name = \"RKAU_synthetic_pretrain\"\n",
    "# )\n",
    "\n",
    "# # ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # # Load the model\n",
    "# # MODEL_FILENAME = \"RKAU_synthetic_pretrain_epoch50_20251104_185416.pth\" \n",
    "# # MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# # loaded_model, loaded_epoch = load_model(\n",
    "# #     model=swau_model, \n",
    "# #     model_path=MODEL_PATH, \n",
    "# #     device=args.device\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20dd44",
   "metadata": {},
   "source": [
    "## Pretrain RKAU Fast Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fafb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Pretrain RKAU Fast Net\n",
    "\n",
    "# # --- Configuration Update ---\n",
    "# BASE_CHANNELS = 16 # Retaining C=16\n",
    "# args.d_attn1 = 128 # Retaining d_attn1=128\n",
    "# args.d_attn2 = 256 # Retaining d_attn2=256\n",
    "# args.img_channels = 3\n",
    "# args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# args.num_attn_layers = 2\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the FULL RKAU_Net model (E1 -> CFB -> RKA -> DynNet -> CFB -> D1)...\")\n",
    "\n",
    "# # Instantiate the full model and move it to the device\n",
    "# rkau_fast_model = RKAU_Net_Fast(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # --- Calculate parameters for each main component ---\n",
    "\n",
    "# e1_params = count_parameters(rkau_fast_model.E1)\n",
    "# d1_params = count_parameters(rkau_fast_model.D1)\n",
    "\n",
    "# # RKA is the Feature Aggregation module\n",
    "# rka_agg_params = count_parameters(rkau_fast_model.RKA_Aggregator) \n",
    "\n",
    "# # P is the DynNet State Evolutionary Predictor\n",
    "# p_params = count_parameters(rkau_fast_model.P) \n",
    "\n",
    "# # CFBs (Enc and Dec)\n",
    "# cfb_enc_params = count_parameters(rkau_fast_model.CFB_enc)\n",
    "# cfb_dec_params = count_parameters(rkau_fast_model.CFB_dec)\n",
    "# cfb_total_params = cfb_enc_params + cfb_dec_params\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + cfb_enc_params + rka_agg_params + p_params + cfb_dec_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (Spatial)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB_enc\", \"Pre-Dynamics Mixer (Refinement)\", f\"{cfb_enc_params:,}\"],\n",
    "#     [\"RKA Aggregator\", \"Temporal Aggregation Core (RKA)\", f\"**{rka_agg_params:,}**\"],\n",
    "#     [\"DynNet (P)\", \"State Evolutionary Predictor\", f\"**{p_params:,}**\"],\n",
    "#     [\"CFB_dec\", \"Post-Dynamics Mixer (Refinement)\", f\"{cfb_dec_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor (Spatial)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**Full RKAU_Net Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### RKAU_Net Component Parameter Summary (Full Architecture)\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DYNAMIC DATA GENERATION CONFIG ---\n",
    "# # Define the size of the synthetic training set to generate each time\n",
    "# NUM_TRAIN_SAMPLES = 100 # Example size, adjust as needed\n",
    "# DATA_GENERATION_EPOCH_CYCLE = 5 \n",
    "# # ------------------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS\n",
    "# args.num_epochs = 50\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = True # Use Soft Dice for stability\n",
    "# lr = 1E-3 # Initial LR\n",
    "\n",
    "# # --- MODEL, OPTIMIZER, SCHEDULER RENAMING ---\n",
    "# optimizer_rkau = torch.optim.Adam(rkau_fast_model.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# scheduler_rkau = ReduceLROnPlateau(\n",
    "#     optimizer_rkau, \n",
    "#     mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss \n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# freeze_batch_norm(rkau_fast_model)\n",
    "\n",
    "# # --- HISTORY INITIALIZATION (RENAMED to rkau_...) ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median)\n",
    "# rkau_train_t1, rkau_train_t2, rkau_train_t3 = [], [], []\n",
    "# rkau_test_t1, rkau_test_t2, rkau_test_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# rkau_train_sd_t1, rkau_train_sd_t2, rkau_train_sd_t3 = [], [], []\n",
    "# rkau_test_sd_t1, rkau_test_sd_t2, rkau_test_sd_t3 = [], [], []\n",
    "# # Mask Scores (Mean/Median)\n",
    "# rkau_train_mask_t1, rkau_train_mask_t2, rkau_train_mask_t3 = [], [], []\n",
    "# rkau_test_mask_t1, rkau_test_mask_t2, rkau_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# rkau_train_mask_sd_t1, rkau_train_mask_sd_t2, rkau_train_mask_sd_t3 = [], [], []\n",
    "# rkau_test_mask_sd_t1, rkau_test_mask_sd_t2, rkau_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting RKAU Fast Model Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# current_train_data = None\n",
    "\n",
    "# # --- TRAINING LOOP (50 Epochs) ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- DYNAMIC DATA GENERATION ---\n",
    "#     if epoch % DATA_GENERATION_EPOCH_CYCLE == 0:\n",
    "#         print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "#         # NOTE: Assuming generate_synthetic_faf_dataset function is available\n",
    "#         current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES) \n",
    "#         current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device)\n",
    "#         print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "    \n",
    "#     # --- 1. Training Step (Using RKAU_Net's accumulated loss function) ---\n",
    "#     # NOTE: Function name f_single_epoch_spatiotemporal_accumulated retained, but called with rkau_fast_model/optimizer_rkau\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, rkau_fast_model, optimizer_rkau, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=1e-2, lambda_faf=0.5, lambda_mask=2.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.5, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 2. Evaluation Step (Median/SD) ---\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, rkau_fast_model, args, soft_dice=soft_dice, use_median=True)\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, rkau_fast_model, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 3. Accumulation (REVISED for consistency) ---\n",
    "#     # Residual Scores\n",
    "#     rkau_train_t1.append(res_train_scores[0]); rkau_train_t2.append(res_train_scores[1]); rkau_train_t3.append(res_train_scores[2])\n",
    "#     rkau_test_t1.append(res_test_scores[0]); rkau_test_t2.append(res_test_scores[1]); rkau_test_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     rkau_train_sd_t1.append(res_train_sds[0]); rkau_train_sd_t2.append(res_train_sds[1]); rkau_train_sd_t3.append(res_train_sds[2])\n",
    "#     rkau_test_sd_t1.append(res_test_sds[0]); rkau_test_sd_t2.append(res_test_sds[1]); rkau_test_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     rkau_train_mask_t1.append(msk_train_scores[0]); rkau_train_mask_t2.append(msk_train_scores[1]); rkau_train_mask_t3.append(msk_train_scores[2])\n",
    "#     rkau_test_mask_t1.append(msk_test_scores[0]); rkau_test_mask_t2.append(msk_test_scores[1]); rkau_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     rkau_train_mask_sd_t1.append(msk_train_sds[0]); rkau_train_mask_sd_t2.append(msk_train_sds[1]); rkau_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     rkau_test_mask_sd_t1.append(msk_test_sds[0]); rkau_test_mask_sd_t2.append(msk_test_sds[1]); rkau_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "#     # --- 4. Scheduler & Logging ---\n",
    "#     scheduler_rkau.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_rkau.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     f_display_frames(current_train_data, rkau_fast_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         rkau_train_t1, rkau_train_t2, rkau_train_t3,\n",
    "#         rkau_test_t1, rkau_test_t2, rkau_test_t3,\n",
    "#         rkau_train_sd_t1, rkau_train_sd_t2, rkau_train_sd_t3,\n",
    "#         rkau_test_sd_t1, rkau_test_sd_t2, rkau_test_sd_t3,\n",
    "#         plot_title='RKAU Model Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History\n",
    "#     plot_train_test_dice_history(\n",
    "#         rkau_train_mask_t1, rkau_train_mask_t2, rkau_train_mask_t3,\n",
    "#         rkau_test_mask_t1, rkau_test_mask_t2, rkau_test_mask_t3,\n",
    "#         rkau_train_mask_sd_t1, rkau_train_mask_sd_t2, rkau_train_mask_sd_t3,\n",
    "#         rkau_test_mask_sd_t1, rkau_test_mask_sd_t2, rkau_test_mask_sd_t3,\n",
    "#         plot_title='RKAU Model Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461ed6b",
   "metadata": {},
   "source": [
    "## Axial U-Net Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dad047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update ---\n",
    "# BASE_CHANNELS = 16 # Retaining C=16\n",
    "# args.d_attn1 = 128 # Retaining d_attn1=128\n",
    "# args.d_attn2 = 256 # Retaining d_attn2=256\n",
    "# args.img_channels = 3\n",
    "# args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# args.num_attn_layers = 2 # N=2 stacked layers for Axial Integrators\n",
    "\n",
    "# print(\"\\nInstantiating the AXIALU_NET Baseline (N=2 Layers)...\")\n",
    "\n",
    "# # Instantiate the full model\n",
    "# axialu_model = AxialU_Net(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # --- Calculate parameters for each main component ---\n",
    "\n",
    "# e1_params = count_parameters(axialu_model.E1)\n",
    "# d1_params = count_parameters(axialu_model.D1)\n",
    "\n",
    "# # Axial_Aggregator is the feature aggregation module using Standard Axial Attention\n",
    "# axial_agg_params = count_parameters(axialu_model.Axial_Aggregator) \n",
    "\n",
    "# # P is the DynNet State Evolutionary Predictor\n",
    "# p_params = count_parameters(axialu_model.P) \n",
    "\n",
    "# # CFBs (Enc and Dec)\n",
    "# cfb_enc_params = count_parameters(axialu_model.CFB_enc)\n",
    "# cfb_dec_params = count_parameters(axialu_model.CFB_dec)\n",
    "# cfb_total_params = cfb_enc_params + cfb_dec_params\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + cfb_enc_params + axial_agg_params + p_params + cfb_dec_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (Spatial)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB_enc\", \"Pre-Dynamics Mixer (Refinement)\", f\"{cfb_enc_params:,}\"],\n",
    "#     [\"Axial Aggregator\", \"Temporal Aggregation Core (Standard Axial)\", f\"**{axial_agg_params:,}**\"],\n",
    "#     [\"DynNet (P)\", \"State Evolutionary Predictor\", f\"**{p_params:,}**\"],\n",
    "#     [\"CFB_dec\", \"Post-Dynamics Mixer (Refinement)\", f\"{cfb_dec_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor (Spatial)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**Full AxialU_Net Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### AxialU_Net Baseline Parameter Summary (N=2 Stacked Layers)\\n\")\n",
    "# # Using the tabulate function as requested in the context\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5131f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DYNAMIC DATA GENERATION CONFIG ---\n",
    "# # Define the size of the synthetic training set to generate each time\n",
    "# NUM_TRAIN_SAMPLES = 100 # Samples per dynamic training set\n",
    "# DATA_GENERATION_EPOCH_CYCLE = 5 \n",
    "# # ------------------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS (Matched to RKAU-Net for comparison)\n",
    "# args.num_epochs = 50\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = True # Use Soft Dice for stability\n",
    "# lr = 1E-3 # Initial LR\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # --- MODEL, OPTIMIZER, SCHEDULER (Renamed for AxialU-Net) ---\n",
    "# optimizer_axialu = torch.optim.Adam(axialu_model.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# scheduler_axialu = ReduceLROnPlateau(\n",
    "#     optimizer_axialu, \n",
    "#     mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Assuming they are correctly defined/imported)\n",
    "# # These are only used internally within the utility functions.\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss \n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean')\n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# freeze_batch_norm(axialu_model) \n",
    "\n",
    "# # --- HISTORY INITIALIZATION (Renamed to axialu_...) ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# axialu_all_iteration_losses = [] \n",
    "# axialu_epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median)\n",
    "# axialu_train_t1, axialu_train_t2, axialu_train_t3 = [], [], []\n",
    "# axialu_test_t1, axialu_test_t2, axialu_test_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# axialu_train_sd_t1, axialu_train_sd_t2, axialu_train_sd_t3 = [], [], []\n",
    "# axialu_test_sd_t1, axialu_test_sd_t2, axialu_test_sd_t3 = [], [], []\n",
    "# # Mask Scores (Mean/Median)\n",
    "# axialu_train_mask_t1, axialu_train_mask_t2, axialu_train_mask_t3 = [], [], []\n",
    "# axialu_test_mask_t1, axialu_test_mask_t2, axialu_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# axialu_train_mask_sd_t1, axialu_train_mask_sd_t2, axialu_train_mask_sd_t3 = [], [], []\n",
    "# axialu_test_mask_sd_t1, axialu_test_mask_sd_t2, axialu_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting AxialU_Net Model Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# current_train_data = None\n",
    "\n",
    "# # --- TRAINING LOOP (50 Epochs) ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- 1. DYNAMIC DATA GENERATION ---\n",
    "#     if epoch % DATA_GENERATION_EPOCH_CYCLE == 0:\n",
    "#         print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "#         # NOTE: Assuming generate_synthetic_faf_dataset function is available and returns a NumPy array\n",
    "#         # This function generates the synthetic data on the CPU\n",
    "#         current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES) \n",
    "#         # Move the entire synthetic dataset to the device (e.g., CUDA)\n",
    "#         current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device)\n",
    "#         print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "    \n",
    "#     # --- 2. Training Step (Using Accumulated Loss Function) ---\n",
    "#     # f_single_epoch_spatiotemporal_accumulated handles batching, augmentation, forward pass, and gradient steps\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, axialu_model, optimizer_axialu, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=1e-2, lambda_faf=0.5, lambda_mask=2.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.5, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     axialu_all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     axialu_epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 3. Evaluation Step (Median/SD) ---\n",
    "#     # Evaluate the current state of the model on the test set\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, axialu_model, args, soft_dice=soft_dice, use_median=True)\n",
    "    \n",
    "#     # Evaluate the current state of the model on the fresh synthetic training data\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, axialu_model, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 4. Accumulation (History Tracking) ---\n",
    "#     # Residual Scores\n",
    "#     axialu_train_t1.append(res_train_scores[0]); axialu_train_t2.append(res_train_scores[1]); axialu_train_t3.append(res_train_scores[2])\n",
    "#     axialu_test_t1.append(res_test_scores[0]); axialu_test_t2.append(res_test_scores[1]); axialu_test_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     axialu_train_sd_t1.append(res_train_sds[0]); axialu_train_sd_t2.append(res_train_sds[1]); axialu_train_sd_t3.append(res_train_sds[2])\n",
    "#     axialu_test_sd_t1.append(res_test_sds[0]); axialu_test_sd_t2.append(res_test_sds[1]); axialu_test_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     axialu_train_mask_t1.append(msk_train_scores[0]); axialu_train_mask_t2.append(msk_train_scores[1]); axialu_train_mask_t3.append(msk_train_scores[2])\n",
    "#     axialu_test_mask_t1.append(msk_test_scores[0]); axialu_test_mask_t2.append(msk_test_scores[1]); axialu_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     axialu_train_mask_sd_t1.append(msk_train_sds[0]); axialu_train_mask_sd_t2.append(msk_train_sds[1]); axialu_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     axialu_test_mask_sd_t1.append(msk_test_sds[0]); axialu_test_mask_sd_t2.append(msk_test_sds[1]); axialu_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "#     # --- 5. Scheduler & Logging ---\n",
    "#     scheduler_axialu.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_axialu.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(axialu_all_iteration_losses, axialu_epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     # NOTE: current_train_data is the full dataset on device, f_display_frames expects the full dataset tensor\n",
    "#     f_display_frames(current_train_data, axialu_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         axialu_train_t1, axialu_train_t2, axialu_train_t3,\n",
    "#         axialu_test_t1, axialu_test_t2, axialu_test_t3,\n",
    "#         axialu_train_sd_t1, axialu_train_sd_t2, axialu_train_sd_t3,\n",
    "#         axialu_test_sd_t1, axialu_test_sd_t2, axialu_test_sd_t3,\n",
    "#         plot_title='AxialU_Net Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History\n",
    "#     plot_train_test_dice_history(\n",
    "#         axialu_train_mask_t1, axialu_train_mask_t2, axialu_train_mask_t3,\n",
    "#         axialu_test_mask_t1, axialu_test_mask_t2, axialu_test_mask_t3,\n",
    "#         axialu_train_mask_sd_t1, axialu_train_mask_sd_t2, axialu_train_mask_sd_t3,\n",
    "#         axialu_test_mask_sd_t1, axialu_test_mask_sd_t2, axialu_test_mask_sd_t3,\n",
    "#         plot_title='AxialU_Net Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- AxialU_Net Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "# saved_path = save_model_weights(\n",
    "#     model=axialu_model, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     save_dir=ckpt_save_dir,\n",
    "#     model_name = \"AxialU_Net_pretrain\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e32d4",
   "metadata": {},
   "source": [
    "## Ablate Spatial Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Ablate Spatial Attention\n",
    "\n",
    "\n",
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Reduced from 24 to 16\n",
    "# args.d_attn1 = 128 # Reduced from 192 to 128\n",
    "# args.d_attn2 = 256 # Reduced from 384 to 256\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the **SWAU_Net-CNN Ablation** model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the full model and move it to the device\n",
    "# # The model is SWAU_Net_CNN, which uses CNN_Unet_Enc/Dec and CNN_DynNet.\n",
    "# swau_model = SWAU_Net_CNN(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(swau_model.E1)\n",
    "\n",
    "# # CORRECTED: Calculate parameters for both CFB modules separately\n",
    "# cfb_enc_params = count_parameters(swau_model.CFB_enc) \n",
    "# cfb_dec_params = count_parameters(swau_model.CFB_dec) \n",
    "# cfb_total_params = cfb_enc_params + cfb_dec_params\n",
    "\n",
    "# swa_params = count_parameters(swau_model.SWA) \n",
    "# p_params = count_parameters(swau_model.P)\n",
    "# d1_params = count_parameters(swau_model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + cfb_total_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"CNN_Unet_Enc (E1)\", \"Feature Extractor (No Spatial Attention)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB (Total, 2x Modules)\", \"**Pre/Post-Dynamics Mixer**\", f\"**{cfb_total_params:,}**\"],\n",
    "#     [\"SlidingWindowAttention (SWA)\", \"**Feature Aggregator/Integrator (Temporal Axial Only)**\", f\"**{swa_params:,}**\"], \n",
    "#     [\"CNN_DynNet (P)\", \"Temporal Feature Predictor (No Attention)\", f\"{p_params:,}\"],\n",
    "#     [\"CNN_Unet_Dec (D1)\", \"Frame Reconstructor\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**SWAU_Net-CNN Ablation**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### SWAU_Net-CNN Ablation Component Parameter Summary\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e98842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DYNAMIC DATA GENERATION CONFIG ---\n",
    "# # Define the size of the synthetic training set to generate each time\n",
    "# NUM_TRAIN_SAMPLES = 100 # Example size, adjust as needed\n",
    "# DATA_GENERATION_EPOCH_CYCLE = 5 \n",
    "# # ------------------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS\n",
    "# args.num_epochs = 50\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = True # Use Soft Dice for stability\n",
    "# lr = 1E-3 # Initial LR\n",
    "\n",
    "# # --- MODEL INSTANTIATION (MODIFIED TO SWAU_Net_CNN) ---\n",
    "# # NOTE: BASE_CHANNELS, args.img_channels, args.device, etc., assumed defined globally.\n",
    "# # Instantiate SWAU_Net_CNN to ablate all spatial/DynNet attention\n",
    "# swau_model = SWAU_Net_CNN(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # --- OPTIMIZER, SCHEDULER RENAMING ---\n",
    "# # (The old 'model_baseline' reference will be replaced by 'swau_model' in function calls)\n",
    "# optimizer_swau = torch.optim.Adam(swau_model.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# scheduler_swau = ReduceLROnPlateau(\n",
    "#     optimizer_swau, \n",
    "#     mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss # This relies on your custom dice_loss function\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# freeze_batch_norm(swau_model) # Use swau_model\n",
    "\n",
    "# # --- HISTORY INITIALIZATION (REQUIRED FOR THIS SCOPE) ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median) - RENAMED to swau_...\n",
    "# swau_train_t1, swau_train_t2, swau_train_t3 = [], [], []\n",
    "# swau_test_t1, swau_test_t2, swau_test_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# swau_train_sd_t1, swau_train_sd_t2, swau_train_sd_t3 = [], [], []\n",
    "# swau_test_sd_t1, swau_test_sd_t2, swau_test_sd_t3 = [], [], []\n",
    "# # Mask Scores (Mean/Median)\n",
    "# swau_train_mask_t1, swau_train_mask_t2, swau_train_mask_t3 = [], [], []\n",
    "# swau_test_mask_t1, swau_test_mask_t2, swau_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# swau_train_mask_sd_t1, swau_train_mask_sd_t2, swau_train_mask_sd_t3 = [], [], []\n",
    "# swau_test_mask_sd_t1, swau_test_mask_sd_t2, swau_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting **SWAU_Net-CNN** Model Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# current_train_data = None\n",
    "\n",
    "# # --- TRAINING LOOP (50 Epochs) ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- DYNAMIC DATA GENERATION ---\n",
    "#     if epoch % DATA_GENERATION_EPOCH_CYCLE == 0:\n",
    "#         print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "#         current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES)\n",
    "#         current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device)\n",
    "#         print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "    \n",
    "#     # --- 1. Training Step (Using SWAU_Net's accumulated loss function) ---\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, swau_model, optimizer_swau, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=1e-2, lambda_faf=0.5, lambda_mask=2.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.5, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 2. Evaluation Step (Median/SD) ---\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, swau_model, args, soft_dice=soft_dice, use_median=True)\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, swau_model, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 3. Accumulation (REVISED for consistency) ---\n",
    "#     # Residual Scores\n",
    "#     swau_train_t1.append(res_train_scores[0]); swau_train_t2.append(res_train_scores[1]); swau_train_t3.append(res_train_scores[2])\n",
    "#     swau_test_t1.append(res_test_scores[0]); swau_test_t2.append(res_test_scores[1]); swau_test_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     swau_train_sd_t1.append(res_train_sds[0]); swau_train_sd_t2.append(res_train_sds[1]); swau_train_sd_t3.append(res_train_sds[2])\n",
    "#     swau_test_sd_t1.append(res_test_sds[0]); swau_test_sd_t2.append(res_test_sds[1]); swau_test_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     swau_train_mask_t1.append(msk_train_scores[0]); swau_train_mask_t2.append(msk_train_scores[1]); swau_train_mask_t3.append(msk_train_scores[2])\n",
    "#     swau_test_mask_t1.append(msk_test_scores[0]); swau_test_mask_t2.append(msk_test_scores[1]); swau_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     swau_train_mask_sd_t1.append(msk_train_sds[0]); swau_train_mask_sd_t2.append(msk_train_sds[1]); swau_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     swau_test_mask_sd_t1.append(msk_test_sds[0]); swau_test_mask_sd_t2.append(msk_test_sds[1]); swau_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "#     # --- 4. Scheduler & Logging ---\n",
    "#     scheduler_swau.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_swau.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     f_display_frames(current_train_data, swau_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         swau_train_t1, swau_train_t2, swau_train_t3,\n",
    "#         swau_test_t1, swau_test_t2, swau_test_t3,\n",
    "#         swau_train_sd_t1, swau_train_sd_t2, swau_train_sd_t3,\n",
    "#         swau_test_sd_t1, swau_test_sd_t2, swau_test_sd_t3,\n",
    "#         plot_title='SWAU_Net-CNN Residual Dice History (Median ± SD)' # MODIFIED TITLE\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History\n",
    "#     plot_train_test_dice_history(\n",
    "#         swau_train_mask_t1, swau_train_mask_t2, swau_train_mask_t3,\n",
    "#         swau_test_mask_t1, swau_test_mask_t2, swau_test_mask_t3,\n",
    "#         swau_train_mask_sd_t1, swau_train_mask_sd_t2, swau_train_mask_sd_t3,\n",
    "#         swau_test_mask_sd_t1, swau_test_mask_sd_t2, swau_test_mask_sd_t3,\n",
    "#         plot_title='SWAU_Net-CNN Full Mask Dice History (Median ± SD)' # MODIFIED TITLE\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "# saved_path = save_model_weights(\n",
    "#     model=swau_model, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     save_dir=ckpt_save_dir,\n",
    "#     model_name = \"SWAU_CNN_pretrain\"\n",
    "    \n",
    "# )\n",
    "\n",
    "# # del model_baseline\n",
    "# # torch.cuda.empty_cache()\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a0c56",
   "metadata": {},
   "source": [
    "## Ablate Spatiotemporal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de262bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Reduced from 24 to 16\n",
    "# args.d_attn1 = 128 # Reduced from 192 to 128\n",
    "# args.d_attn2 = 256 # Reduced from 384 to 256\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the **UPredNet (SWA Ablation)** model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the UPredNet model (UPredNet uses E1, CFB_enc, P=DynNet, CFB_dec, D1)\n",
    "# # We assume UPredNet is accessible, and base_channels defaults to 16 if not set.\n",
    "# upred_model = UPredNet(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(upred_model.E1)\n",
    "\n",
    "# # Calculate parameters for both CFB modules separately\n",
    "# cfb_enc_params = count_parameters(upred_model.CFB_enc) \n",
    "# cfb_dec_params = count_parameters(upred_model.CFB_dec) \n",
    "# cfb_total_params = cfb_enc_params + cfb_dec_params\n",
    "\n",
    "# # The UPredNet model does NOT have an 'SWA' module. This parameter should be 0.\n",
    "# swa_params = 0 \n",
    "\n",
    "# # P is the DynNet\n",
    "# p_params = count_parameters(upred_model.P)\n",
    "# d1_params = count_parameters(upred_model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + cfb_total_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (No Spatial Attention)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB (Total, 2x Modules)\", \"**Pre/Post-Dynamics Mixer**\", f\"**{cfb_total_params:,}**\"],\n",
    "#     [\"**SWA Module**\", \"**Ablated**\", f\"**{swa_params:,}**\"], # SWA is 0\n",
    "#     [\"DynNet (P)\", \"Temporal Feature Predictor (Evolution)\", f\"{p_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**UPredNet (SWA Ablation)**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### UPredNet (SWA Ablation) Component Parameter Summary\\n\")\n",
    "# # Assuming the 'tabulate' library and 'BASE_CHANNELS' constant are available in the execution environment\n",
    "# try:\n",
    "#     from tabulate import tabulate\n",
    "#     print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))\n",
    "# except ImportError:\n",
    "#     print(\"Tabulate library not available. Printing raw data.\")\n",
    "#     for row in param_data:\n",
    "#         print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DYNAMIC DATA GENERATION CONFIG ---\n",
    "# # Define the size of the synthetic training set to generate each time\n",
    "# NUM_TRAIN_SAMPLES = 100 # Example size, adjust as needed\n",
    "# DATA_GENERATION_EPOCH_CYCLE = 5 \n",
    "# # ------------------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS\n",
    "# args.num_epochs = 50\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = True # Use Soft Dice for stability\n",
    "# lr = 1E-3 # Initial LR\n",
    "\n",
    "# # --- MODEL INSTANTIATION (MODIFIED TO UPredNet - SWA Ablation) ---\n",
    "# # NOTE: BASE_CHANNELS, args.img_channels, args.device, etc., assumed defined globally.\n",
    "\n",
    "# # Instantiate UPredNet to ablate all explicit Spatio-Temporal Attention (SWA/Axial)\n",
    "# uprednet_model = UPredNet(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # --- OPTIMIZER, SCHEDULER RENAMING (Correcting 'swau' to 'uprednet' references) ---\n",
    "# # NOTE: The provided code had 'uprednet.parameters()' which is incorrect. Using the instantiated model name.\n",
    "# optimizer_uprednet = torch.optim.Adam(uprednet_model.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# scheduler_uprednet = ReduceLROnPlateau(\n",
    "#     optimizer_uprednet, \n",
    "#     mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss # This relies on your custom dice_loss function\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# freeze_batch_norm(uprednet_model) # Use uprednet_model\n",
    "\n",
    "# # --- HISTORY INITIALIZATION (REVISING ALL 'swau' REFERENCES TO 'uprednet') ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median) - Corrected Naming\n",
    "# uprednet_train_t1, uprednet_train_t2, uprednet_train_t3 = [], [], []\n",
    "# uprednet_test_t1, uprednet_test_t2, uprednet_test_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# uprednet_train_sd_t1, uprednet_train_sd_t2, uprednet_train_sd_t3 = [], [], []\n",
    "# uprednet_test_sd_t1, uprednet_test_sd_t2, uprednet_test_sd_t3 = [], [], []\n",
    "# # Mask Scores (Mean/Median)\n",
    "# uprednet_train_mask_t1, uprednet_train_mask_t2, uprednet_train_mask_t3 = [], [], []\n",
    "# uprednet_test_mask_t1, uprednet_test_mask_t2, uprednet_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# uprednet_train_mask_sd_t1, uprednet_train_mask_sd_t2, uprednet_train_mask_sd_t3 = [], [], []\n",
    "# uprednet_test_mask_sd_t1, uprednet_test_mask_sd_t2, uprednet_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting **UPredNet (SWA Ablation)** Model Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# current_train_data = None\n",
    "\n",
    "# # --- TRAINING LOOP (50 Epochs) ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- DYNAMIC DATA GENERATION ---\n",
    "#     if epoch % DATA_GENERATION_EPOCH_CYCLE == 0:\n",
    "#         print(f\"\\n[Epoch {epoch+1}] Regenerating fresh synthetic training data...\")\n",
    "#         # Assuming generate_synthetic_faf_dataset and current_train_data_np are defined elsewhere\n",
    "#         current_train_data_np = generate_synthetic_faf_dataset(num_samples=NUM_TRAIN_SAMPLES)\n",
    "#         current_train_data = torch.from_numpy(current_train_data_np.astype(np.float32)).to(args.device)\n",
    "#         print(f\"Data generation complete. New training set size: {current_train_data.shape[0]} samples.\")\n",
    "    \n",
    "#     # --- 1. Training Step (Using UPredNet's accumulated loss function) ---\n",
    "#     # NOTE: Model and optimizer references are corrected from 'swau' to 'uprednet'\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, uprednet_model, optimizer_uprednet, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=1e-2, lambda_faf=0.5, lambda_mask=2.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.5, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 2. Evaluation Step (Median/SD) ---\n",
    "#     # NOTE: Model reference is corrected\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, uprednet_model, args, soft_dice=soft_dice, use_median=True)\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, uprednet_model, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 3. Accumulation (REVISED to use 'uprednet' history variables) ---\n",
    "#     # Residual Scores\n",
    "#     uprednet_train_t1.append(res_train_scores[0]); uprednet_train_t2.append(res_train_scores[1]); uprednet_train_t3.append(res_train_scores[2])\n",
    "#     uprednet_test_t1.append(res_test_scores[0]); uprednet_test_t2.append(res_test_scores[1]); uprednet_test_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     uprednet_train_sd_t1.append(res_train_sds[0]); uprednet_train_sd_t2.append(res_train_sds[1]); uprednet_train_sd_t3.append(res_train_sds[2])\n",
    "#     uprednet_test_sd_t1.append(res_test_sds[0]); uprednet_test_sd_t2.append(res_test_sds[1]); uprednet_test_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     uprednet_train_mask_t1.append(msk_train_scores[0]); uprednet_train_mask_t2.append(msk_train_scores[1]); uprednet_train_mask_t3.append(msk_train_scores[2])\n",
    "#     uprednet_test_mask_t1.append(msk_test_scores[0]); uprednet_test_mask_t2.append(msk_test_scores[1]); uprednet_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     uprednet_train_mask_sd_t1.append(msk_train_sds[0]); uprednet_train_mask_sd_t2.append(msk_train_sds[1]); uprednet_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     uprednet_test_mask_sd_t1.append(msk_test_sds[0]); uprednet_test_mask_sd_t2.append(msk_test_sds[1]); uprednet_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "#     # --- 4. Scheduler & Logging ---\n",
    "#     # NOTE: Scheduler reference is corrected\n",
    "#     scheduler_uprednet.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_uprednet.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     # NOTE: Model reference is corrected\n",
    "#     f_display_frames(current_train_data, uprednet_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History\n",
    "#     # NOTE: History variables and plot title are corrected\n",
    "#     plot_train_test_dice_history(\n",
    "#         uprednet_train_t1, uprednet_train_t2, uprednet_train_t3,\n",
    "#         uprednet_test_t1, uprednet_test_t2, uprednet_test_t3,\n",
    "#         uprednet_train_sd_t1, uprednet_train_sd_t2, uprednet_train_sd_t3,\n",
    "#         uprednet_test_sd_t1, uprednet_test_sd_t2, uprednet_test_sd_t3,\n",
    "#         plot_title='UPredNet (SWA Ablation) Residual Dice History (Median ± SD)' # MODIFIED TITLE\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History\n",
    "#     # NOTE: History variables and plot title are corrected\n",
    "#     plot_train_test_dice_history(\n",
    "#         uprednet_train_mask_t1, uprednet_train_mask_t2, uprednet_train_mask_t3,\n",
    "#         uprednet_test_mask_t1, uprednet_test_mask_t2, uprednet_test_mask_t3,\n",
    "#         uprednet_train_mask_sd_t1, uprednet_train_mask_sd_t2, uprednet_train_mask_sd_t3,\n",
    "#         uprednet_test_mask_sd_t1, uprednet_test_mask_sd_t2, uprednet_test_mask_sd_t3,\n",
    "#         plot_title='UPredNet (Spatiotemporal Ablation) Full Mask Dice History (Median ± SD)' # MODIFIED TITLE\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d90a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "# saved_path = save_model_weights(\n",
    "#     model=uprednet_model, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     save_dir=ckpt_save_dir,\n",
    "#     model_name = \"UPredNet_pretrain\"\n",
    "    \n",
    "# )\n",
    "\n",
    "# # del model_baseline\n",
    "# # torch.cuda.empty_cache()\n",
    "# # gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
