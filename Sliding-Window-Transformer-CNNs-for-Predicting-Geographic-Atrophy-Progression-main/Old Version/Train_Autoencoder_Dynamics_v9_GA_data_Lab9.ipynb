{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5erGq6DCneF"
   },
   "outputs": [],
   "source": [
    "%cd /Users/Pracioppo/Desktop/VPTR/VPTR/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch import nn\n",
    "import functools\n",
    "from torch.nn import init\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Loading bar\n",
    "from PIL import Image\n",
    "\n",
    "from model import GDL, MSELoss, L1Loss\n",
    "from utils import get_dataloader\n",
    "from utils import visualize_batch_clips, save_ckpt, load_ckpt, set_seed, AverageMeters, init_loss_dict, write_summary, resume_training\n",
    "from utils import set_seed\n",
    "from utils import f_rescale_dataset, f_Residuals, f_reshape_training_data, f_rotate_and_zoom, f_random_crop, f_rotate_and_zoom_all, f_crop_all, f_flip_all, f_augment_dataset2\n",
    "\n",
    "from utils import KTHDataset, BAIRDataset, MovingMNISTDataset\n",
    "\n",
    "set_seed(2023)\n",
    "\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "ckpt_save_dir = Path('/Users/Pracioppo/Desktop//VPTR/GA_Zubens_data//Dyn_ResNetAE_MSEGDLgan_ckpt')\n",
    "tensorboard_save_dir = Path('/Users/Pracioppo/Desktop/VPTR/GA_Zubens_data/Dyn_ResNetAE_MSEGDLgan_tensorboard')\n",
    "                            \n",
    "start_epoch = 0\n",
    "\n",
    "# resume_ckpt = r\"\\Users\\Pracioppo\\Desktop\\VPTR\\GA_2\\GA_full_3chan_gan_ckpt\\epoch_20.tar\"\n",
    "resume_ckpt = None\n",
    "\n",
    "summary_writer = SummaryWriter(tensorboard_save_dir.absolute().as_posix())\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser('ETC')\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.num_past_frames = 1\n",
    "args.num_future_frames = 1\n",
    "# args.encH, args.encW, args.encC = 8, 8, 528 # Original setting\n",
    "# args.encH, args.encW, args.encC = 4, 4, 16 # Increased dimensions for 512 x 512 images\n",
    "# args.encH, args.encW, args.encC = 16, 16, 528 # Increased dimensions for 512 x 512 images\n",
    "args.encH, args.encW, args.encC = 16, 16, 128 # Increased dimensions for 512 x 512 images\n",
    "args.img_channels = 1 #3 channels for BAIR datset\n",
    "args.epochs = 10\n",
    "\n",
    "args.N = 8\n",
    "\n",
    "args.AE_lr = 2e-4\n",
    "\n",
    "args.batch_size = args.N\n",
    "test_past_frames = 1\n",
    "test_future_frames = 1\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "args.device = torch.device('cuda:0')\n",
    "print(f\"Using {args.device} device\")\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# --- SETUP AND ARGS (FOR RUNNABILITY) ---\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(2025)\n",
    "\n",
    "parser = argparse.ArgumentParser('VPTR Model Args')\n",
    "args = parser.parse_args(args=[])\n",
    "# Defining essential arguments for model instantiation\n",
    "args.N = 8 # Batch size\n",
    "args.nhead = 8\n",
    "# CRITICAL LIGHTWEIGHT CHANGE: FFN dimensions drastically reduced\n",
    "args.d_attn1 = 512 # Used for 128-channel features (Level 3 Attn)\n",
    "args.d_attn2 = 512 # Used for 256-channel features (Level 4 Attn)\n",
    "args.img_channels = 3 # <-- CHANGED: Now using 3 channels (GT, Mask, Growth)\n",
    "args.img_H = 256 # <-- NEW: Height for 256x256 input\n",
    "args.img_W = 256 # <-- NEW: Width for 256x256 input\n",
    "args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "BASE_CHANNELS = 32 # Drastically reduced for lightweight design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in GA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class Data(Dataset):\n",
    "    pass\n",
    "\n",
    "train_data = Data()\n",
    "val_data = Data()\n",
    "test_data = Data()\n",
    "\n",
    "N = 4\n",
    "Nv = N\n",
    "batch_size = N\n",
    "ngpus = 1\n",
    "num_workers = 1\n",
    "\n",
    "%cd /Users/Pracioppo/Desktop/VPTR/GA_Zubens_data\n",
    "\n",
    "FAFs = torch.from_numpy(sio.loadmat('fafs_reg3.mat')[\"fafs_reg3\"].astype(np.float32)).reshape(660,1,4,256,256)\n",
    "masks = torch.from_numpy(sio.loadmat('masks_reg3.mat')[\"masks_reg3\"].astype(np.float32)).reshape(660,1,4,256,256)\n",
    "\n",
    "masks /= torch.max(masks)\n",
    "FAFs /= torch.max(FAFs)\n",
    "\n",
    "print(FAFs.size())\n",
    "print(masks.size())\n",
    "\n",
    "\n",
    "train_masks = masks[0:560]\n",
    "test_masks = masks[560:]\n",
    "train_FAFs = FAFs[0:560]\n",
    "test_FAFs = FAFs[560:]\n",
    "\n",
    "\n",
    "train_residuals = f_Residuals(train_masks)\n",
    "test_residuals = f_Residuals(test_masks)\n",
    "\n",
    "train_residuals = train_residuals - torch.min(train_residuals)\n",
    "train_residuals = train_residuals/torch.max(train_residuals)\n",
    "\n",
    "test_residuals = test_residuals - torch.min(test_residuals)\n",
    "test_residuals = test_residuals/torch.max(test_residuals)\n",
    "\n",
    "print(train_residuals.size())\n",
    "\n",
    "\n",
    "args.num_t_steps = 4\n",
    "img_sz = 256\n",
    "\n",
    "Train_Data = torch.concatenate((train_FAFs,train_masks,train_residuals),4).to(args.device)\n",
    "Test_Data = torch.concatenate((test_FAFs,test_masks,test_residuals),4).to(args.device)\n",
    "All_Data = torch.concatenate((Train_Data, Test_Data))\n",
    "\n",
    "print(Train_Data.size())\n",
    "print(Test_Data.size())\n",
    "\n",
    "\n",
    "# Get sharply defined masks\n",
    "All_Data[:,:,:,:,256:] = (All_Data[:,:,:,:,256:] > 0.5)\n",
    "\n",
    "\n",
    "\n",
    "# Get data for K fold cross validation\n",
    "\n",
    "sz = 70\n",
    "k = 0\n",
    "\n",
    "Test_Data = All_Data[k*sz:(k+1)*sz].to(args.device)\n",
    "Train_Data = torch.cat((All_Data[:k*sz], All_Data[(k+1)*sz:]), axis=0).to(args.device)\n",
    "\n",
    "print(Test_Data.size())\n",
    "print(Train_Data.size())\n",
    "\n",
    "Data = Test_Data\n",
    "for i in np.arange(1):\n",
    "    print(i)\n",
    "    i = 2\n",
    "    j = 3\n",
    "    for j in np.arange(4):\n",
    "        img = Data[i*10,0,j,:,256:512].cpu().detach().numpy()\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    for j in np.arange(3):\n",
    "        residual = (Data[i*10,0,j,:,512:]).cpu().detach().numpy()\n",
    "        plt.imshow(residual, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "del train_FAFs, train_masks, train_residuals\n",
    "del test_FAFs, test_masks, test_residuals\n",
    "\n",
    "Train_Data.size()\n",
    "# torch.Size([590, 1, 4, 256, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# --- SETUP AND ARGS (FOR RUNNABILITY) ---\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(2023)\n",
    "\n",
    "parser = argparse.ArgumentParser('VPTR Model Args')\n",
    "args = parser.parse_args(args=[])\n",
    "# Defining essential arguments for model instantiation\n",
    "args.N = 8 # Batch size\n",
    "args.nhead = 8\n",
    "# CRITICAL LIGHTWEIGHT CHANGE: FFN dimensions drastically reduced\n",
    "# Level 3 (128 channels) FFN reduced from 512 to 256 (2x expansion)\n",
    "args.d_attn1 = 192 # <--- FURTHER REDUCED: Used for 128-channel features (Level 3 Attn)\n",
    "# Level 4 (256 channels) FFN kept at 512 (2x expansion)\n",
    "args.d_attn2 = 384 # <--- FURTHER REDUCED: Used for 256-channel features (Level 4 Attn)\n",
    "args.img_channels = 3 # <-- CORRECTED: Setting default input channel to 3 globally\n",
    "args.img_sz = 256 # CRITICAL: Set to 256 as required\n",
    "args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "BASE_CHANNELS = 28 # <--- AGGRESSIVE REDUCTION: Down from 32 to 28 for parameter savings\n",
    "\n",
    "# --- CORE UTILITY MODULES (RoPE IMPLEMENTATION) ---\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates the last dimension of the tensor by half.\"\"\"\n",
    "    # x shape: (..., D_H)\n",
    "    D_H = x.shape[-1]\n",
    "    \n",
    "    # Reshape x to (..., D_H/2, 2)\n",
    "    x = x.reshape(*x.shape[:-1], D_H // 2, 2) \n",
    "    x1, x2 = x.unbind(dim=-1) # x1, x2 shape: (..., D_H/2)\n",
    "    \n",
    "    # Concatenate the rotated parts (-x2, x1) along the last dimension\n",
    "    rotated_x = torch.cat((-x2, x1), dim=-1) # rotated_x shape: (..., D_H)\n",
    "    \n",
    "    # Returns a tensor of the original shape (..., D_H)\n",
    "    return rotated_x\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Generates the sin/cos vectors for RoPE.\"\"\"\n",
    "    def __init__(self, dim, max_seq_len=4096):\n",
    "        super().__init__()\n",
    "        # Calculate inverse frequency (1/10000^(2i/d))\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def forward(self, seq_len, device):\n",
    "        if seq_len > self.max_seq_len:\n",
    "            # Resize cache or handle sequence length exceeding max\n",
    "            pass\n",
    "            \n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "        # Outer product to get frequencies: (L, 1) * (1, D/2) -> (L, D/2)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1) # (L, D) - Repeated sin/cos parts\n",
    "        \n",
    "        # pe: (2, L, D) -> [cos, sin]\n",
    "        pe = torch.stack([emb.cos(), emb.sin()], dim=0).to(device) \n",
    "        return pe # Shape (2, L, D)\n",
    "\n",
    "class RoPEMultiheadAttention(nn.Module):\n",
    "    \"\"\"Custom MHA layer that applies RoPE to Q/K after projection.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., batch_first=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.batch_first = batch_first\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Projection for Q, K, V (3*D)\n",
    "        self.in_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=True) \n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "        \n",
    "        # RoPE applied per head dimension\n",
    "        # Max sequence length for 32x32 is 1024. For 64x64 is 4096. Set to 4096 for safety.\n",
    "        self.rope_emb = RotaryPositionalEmbedding(self.head_dim, max_seq_len=4096) \n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        N, L_q, D = query.shape\n",
    "        L_k = key.shape[1] # Sequence length of key\n",
    "        \n",
    "        is_cross_attn = (query is not key)\n",
    "        \n",
    "        # 1. Project Q, K, V\n",
    "        qkv = self.in_proj(query)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        if is_cross_attn:\n",
    "            # If cross-attention, K and V are derived from the encoder's feature map (key/value input)\n",
    "            # and Q is derived from the decoder's feature map (query input).\n",
    "            kv_proj = self.in_proj(key) # key == value for cross-attention in this architecture\n",
    "            k_proj, v_proj, _ = kv_proj.chunk(3, dim=-1)\n",
    "            k, v = k_proj, v_proj\n",
    "            \n",
    "        # 2. Reshape and split heads: (N, L, D) -> (N, H, L, H_D)\n",
    "        def reshape_heads(x):\n",
    "            return x.reshape(N, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q = reshape_heads(q) # (N, H, L_q, H_D)\n",
    "        k = reshape_heads(k) # (N, H, L_k, H_D)\n",
    "        v = reshape_heads(v)\n",
    "        \n",
    "        # 3. Apply RoPE to Q and K\n",
    "        seq_len = max(L_q, L_k)\n",
    "        \n",
    "        # rot_emb_cos_sin shape: (2, L, H_D)\n",
    "        rot_emb_cos_sin = self.rope_emb(seq_len, query.device) \n",
    "        \n",
    "        # Unpack cos/sin: (2, L, H_D) -> (1, L, H_D)\n",
    "        cos, sin = rot_emb_cos_sin.chunk(2, dim=0) \n",
    "        \n",
    "        # Reshape to (1, 1, L, H_D) for broadcasting over (N, H)\n",
    "        cos = cos.unsqueeze(1) \n",
    "        sin = sin.unsqueeze(1) \n",
    "\n",
    "        # Apply RoPE (RoPE is element-wise on the last dimension)\n",
    "        # Indexing cos/sin for L_q and L_k\n",
    "        q = (q * cos[:, :, :L_q]) + (rotate_half(q) * sin[:, :, :L_q])\n",
    "        k = (k * cos[:, :, :L_k]) + (rotate_half(k) * sin[:, :, :L_k])\n",
    "        \n",
    "        # 4. Scaled Dot-Product Attention\n",
    "        attn_output_weights = torch.matmul(q * self.scaling, k.transpose(-2, -1)) # (N, H, L_q, L_k)\n",
    "        \n",
    "        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "        attn_output_weights = self.dropout(attn_output_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_output_weights, v) # (N, H, L_q, H_D)\n",
    "        \n",
    "        # 5. Concatenate heads and final projection\n",
    "        attn_output = attn_output.transpose(1, 2).flatten(start_dim=-2) # (N, L_q, D)\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output, attn_output_weights.mean(dim=1)\n",
    "\n",
    "class RoPETransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Custom TransformerEncoderLayer using RoPEMultiheadAttention.\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0., batch_first=True):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = RoPEMultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-Attention Block\n",
    "        src_norm = self.norm1(src)\n",
    "        attn_out, _ = self.attn(src_norm, src_norm, src_norm)\n",
    "        src = src + attn_out # Residual\n",
    "        \n",
    "        # FFN Block\n",
    "        src_norm = self.norm2(src)\n",
    "        ffn_out = self.ffn(src_norm)\n",
    "        src = src + ffn_out # Residual\n",
    "        \n",
    "        return src\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Standard Convolutional Block: Conv -> ReLU -> Conv -> ReLU\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# --- ENCODER (U-NET DOWN PATH: CNNs only for high resolution) ---\n",
    "\n",
    "class Unet_Enc(nn.Module):\n",
    "    def __init__(self, args, img_channels=1, base_channels=BASE_CHANNELS):\n",
    "        super(Unet_Enc, self).__init__()\n",
    "        \n",
    "        # --- Downsampling Path (5 Levels Total, 4 Skips + 1 Bottleneck) ---\n",
    "        \n",
    "        # Level 1 (256x256 -> 128x128)\n",
    "        # Input: C -> 28\n",
    "        self.conv1 = ConvBlock(img_channels, base_channels) # 256x256x28 (Skip feats1u)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Level 2 (128x128 -> 64x64)\n",
    "        # Input: 28 -> 56\n",
    "        self.conv2 = ConvBlock(base_channels, base_channels * 2) # 128x128x56 (Skip feats2u)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Level 3 (64x64 -> 32x32)\n",
    "        # Input: 56 -> 112\n",
    "        self.conv3 = ConvBlock(base_channels * 2, base_channels * 4) # 64x64x112 (Skip feats3u)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Level 4 (32x32 -> 16x16)\n",
    "        # Input: 112 -> 224\n",
    "        self.conv4 = ConvBlock(base_channels * 4, base_channels * 8) # 32x32x224 (Skip feats4u)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Attention 1 (Applied on Level 4: 32x32 features, 224 channels)\n",
    "        self.te1_model_dim = base_channels * 8 # 224 channels\n",
    "        # ONLY ONE Self-Attention Layer\n",
    "        self.te_sa4_1 = RoPETransformerEncoderLayer(d_model=self.te1_model_dim, nhead=args.nhead, dim_feedforward=args.d_attn2, dropout=0, batch_first=True)\n",
    "        # self.te_sa4_2 removed for parameter reduction\n",
    "        \n",
    "        # Level 5 (16x16): Bottleneck\n",
    "        # Input: 224 -> 448\n",
    "        self.conv5 = ConvBlock(base_channels * 8, base_channels * 16) # 16x16x448 (Bottleneck_4d)\n",
    "\n",
    "    def forward(self, in_frames):\n",
    "        N = in_frames.size(0)\n",
    "        \n",
    "        # Level 1: 256x256x28 (Pure CNN)\n",
    "        feats1u = self.conv1(in_frames) \n",
    "        x = self.pool1(feats1u)\n",
    "        \n",
    "        # Level 2: 128x128x56 (Pure CNN)\n",
    "        feats2u = self.conv2(x)\n",
    "        x = self.pool2(feats2u)\n",
    "        \n",
    "        # Level 3: 64x64x112 (Pure CNN)\n",
    "        feats3u = self.conv3(x)\n",
    "        x_pre_attn = self.pool3(feats3u) # 32x32x112\n",
    "        \n",
    "        # Level 4: 32x32x224 (CNN + Attention)\n",
    "        feats4u_pre = self.conv4(x_pre_attn)\n",
    "        \n",
    "        # Attention 1 (Self-Attention on L4 features)\n",
    "        tokens4 = feats4u_pre.flatten(2).transpose(1, 2) # (N, 1024, 224)\n",
    "        \n",
    "        # Single Self-Attention Layer\n",
    "        q1 = self.te_sa4_1(tokens4)\n",
    "        # q2 removed\n",
    "        \n",
    "        # Fuse Attention output with residual\n",
    "        q1_4d = q1.transpose(1, 2).reshape(N, self.te1_model_dim, *x_pre_attn.shape[2:])\n",
    "        feats4u = feats4u_pre + q1_4d # Skip feature 4\n",
    "        \n",
    "        # Bottleneck (L5: 16x16x448)\n",
    "        x_bottleneck_pre = self.pool4(feats4u) # 16x16x224\n",
    "        bottleneck_4d = self.conv5(x_bottleneck_pre) # 16x16x448\n",
    "        \n",
    "        # Returns 4 skips + 4D bottleneck feature\n",
    "        return feats1u, feats2u, feats3u, feats4u, bottleneck_4d\n",
    "\n",
    "    \n",
    "# --- DECODER (U-NET UP PATH: Attention in low resolution, CNNs for high) ---\n",
    "\n",
    "class Unet_Dec(nn.Module):\n",
    "    def __init__(self, args, img_channels=1, base_channels=BASE_CHANNELS):\n",
    "        super(Unet_Dec, self).__init__()\n",
    "        \n",
    "        # --- Upsampling Path ---\n",
    "        \n",
    "        # Level 5 to 4 Up: 16x16x448 -> 32x32x224 (Interact with the L4 skip)\n",
    "        self.upconv4 = nn.ConvTranspose2d(base_channels * 16, base_channels * 8, kernel_size=2, stride=2)\n",
    "        # Concat input: 224 (upconv) + 224 (skip feats4u) = 448\n",
    "        self.up_conv4 = ConvBlock(base_channels * 8 + base_channels * 8, base_channels * 8) # 448 in -> 224 out\n",
    "        \n",
    "        # --- Interleaved Attention Layers (Level 4: 32x32x224) ---\n",
    "        \n",
    "        self.te2d_model_dim = base_channels * 8 # 224\n",
    "        # ONLY ONE Self-Attention Layer\n",
    "        self.te_sa4d_1 = RoPETransformerEncoderLayer(d_model=self.te2d_model_dim, nhead=args.nhead, dim_feedforward=args.d_attn2, dropout=0, batch_first=True)\n",
    "        # self.te_sa4d_2 removed for parameter reduction\n",
    "        # Cross-Attention (RoPE)\n",
    "        self.mha4_ca = RoPEMultiheadAttention(embed_dim=self.te2d_model_dim, num_heads=args.nhead, dropout=0, batch_first=True) \n",
    "        \n",
    "        # Level 4 to 3 Up: 32x32x224 -> 64x64x112 (Interact with feats3u)\n",
    "        self.upconv3 = nn.ConvTranspose2d(base_channels * 8, base_channels * 4, kernel_size=2, stride=2)\n",
    "        # Concat input: 112 (upconv) + 112 (skip feats3u) = 224\n",
    "        self.up_conv3 = ConvBlock(base_channels * 4 + base_channels * 4, base_channels * 4) # 224 in -> 112 out\n",
    "\n",
    "        # --- Interleaved Attention Layers (Level 3: 64x64x112) ---\n",
    "        \n",
    "        self.te1d_model_dim = base_channels * 4 # 112\n",
    "        # ONLY ONE Self-Attention Layer\n",
    "        self.te_sa3d_1 = RoPETransformerEncoderLayer(d_model=self.te1d_model_dim, nhead=args.nhead, dim_feedforward=args.d_attn1, dropout=0, batch_first=True)\n",
    "        # self.te_sa3d_2 removed for parameter reduction\n",
    "        # Cross-Attention (RoPE)\n",
    "        self.mha3_ca = RoPEMultiheadAttention(embed_dim=self.te1d_model_dim, num_heads=args.nhead, dropout=0, batch_first=True) \n",
    "\n",
    "        # Level 3 to 2 Up: 64x64x112 -> 128x128x56 (Pure CNN path resumes)\n",
    "        self.upconv2 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)\n",
    "        # Concat input: 56 (upconv) + 56 (skip feats2u) = 112\n",
    "        self.up_conv2 = ConvBlock(base_channels * 2 + base_channels * 2, base_channels * 2) # 112 in -> 56 out\n",
    "\n",
    "        # Level 2 to 1 Up: 128x128x56 -> 256x256x28 (Pure CNN path)\n",
    "        self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=2, stride=2)\n",
    "        # Concat input: 28 (upconv) + 28 (skip feats1u) = 56\n",
    "        self.up_conv1 = ConvBlock(base_channels + base_channels, base_channels) # 56 in -> 28 out\n",
    "        \n",
    "        # Final Output Layer (28 channels -> 3 channel)\n",
    "        self.final_conv = nn.Conv2d(base_channels, img_channels, kernel_size=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, feats1u_enc, feats2u_enc, feats3u_enc, feats4u_enc, bottleneck_4d):\n",
    "        N = bottleneck_4d.size(0)\n",
    "        \n",
    "        # --- Upsampling Path Start (Bottleneck) ---\n",
    "        \n",
    "        # Level 5 to 4 Up: 16x16x448 -> 32x32x224 (Interact with feats4u)\n",
    "        x = self.upconv4(bottleneck_4d) # x shape: (N, 224, 32, 32)\n",
    "        x = torch.cat([x, feats4u_enc], dim=1) # (N, 448, 32, 32)\n",
    "        feats4d = self.up_conv4(x) # (N, 224, 32, 32)\n",
    "        \n",
    "        # --- Level 4 Attention (32x32x224) ---\n",
    "        tokens4d = feats4d.flatten(2).transpose(1, 2) # (N, 1024, 224)\n",
    "        \n",
    "        # Single Self-Attention (RoPE)\n",
    "        b1 = self.te_sa4d_1(tokens4d)\n",
    "        feats4d += b1.transpose(1, 2).reshape(feats4d.size())\n",
    "        \n",
    "        # Cross-Attention (RoPE) (Query uses b1 output, K/V uses Encoder Skip)\n",
    "        tokens4u = feats4u_enc.flatten(2).transpose(1, 2) # (N, 1024, 224)\n",
    "        attn_output4, _ = self.mha4_ca(b1, tokens4u, tokens4u) # Q=b1\n",
    "        feats4d += attn_output4.transpose(1, 2).reshape(feats4d.size())\n",
    "        \n",
    "        # Level 4 to 3 Up: 32x32x224 -> 64x64x112 (Interact with feats3u)\n",
    "        x = self.upconv3(feats4d) # x shape: (N, 112, 64, 64)\n",
    "        x = torch.cat([x, feats3u_enc], dim=1) # (N, 224, 64, 64)\n",
    "        feats3d = self.up_conv3(x) # (N, 112, 64, 64)\n",
    "        \n",
    "        # --- Level 3 Attention (64x64x112) ---\n",
    "        tokens3d = feats3d.flatten(2).transpose(1, 2) # (N, 4096, 112)\n",
    "        \n",
    "        # Single Self-Attention (RoPE)\n",
    "        c1 = self.te_sa3d_1(tokens3d)\n",
    "        feats3d += c1.transpose(1, 2).reshape(feats3d.size())\n",
    "        \n",
    "        # Cross-Attention (RoPE)\n",
    "        tokens3u = feats3u_enc.flatten(2).transpose(1, 2) # (N, 4096, 112)\n",
    "        attn_output3, _ = self.mha3_ca(c1, tokens3u, tokens3u) # Q=c1\n",
    "        feats3d += attn_output3.transpose(1, 2).reshape(feats3d.size())\n",
    "\n",
    "        # Level 3 to 2 Up: 64x64x112 -> 128x128x56 (Pure CNN path resumes)\n",
    "        x = self.upconv2(feats3d) # x shape: (N, 56, 128, 128)\n",
    "        x = torch.cat([x, feats2u_enc], dim=1) # (N, 112, 128, 128)\n",
    "        feats2d = self.up_conv2(x) # (N, 56, 128, 128)\n",
    "\n",
    "        # Level 2 to 1 Up: 128x128x56 -> 256x256x28 (Pure CNN path)\n",
    "        x = self.upconv1(feats2d) # x shape: (N, 28, 256, 256)\n",
    "        x = torch.cat([x, feats1u_enc], dim=1) # (N, 56, 256, 256)\n",
    "        feats1d = self.up_conv1(x) # (N, 28, 256, 256)\n",
    "        \n",
    "        # Final Output\n",
    "        out = self.final_conv(feats1d)\n",
    "        out_frames = self.sig(out)\n",
    "\n",
    "        return out_frames\n",
    "    \n",
    "# --- FULL U-NET MODEL WRAPPER ---\n",
    "class U_Net_Attn(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper class combining the Encoder (Unet_Enc) and the Decoder (Unet_Dec)\n",
    "    to form the complete U-Net with interleaved Rotary Positional Embedding (RoPE) attention.\n",
    "\n",
    "    The model takes an input frame, encodes it, and then decodes it,\n",
    "    returning the reconstructed frame along with all intermediate skip connections\n",
    "    and the 4D bottleneck feature for potential use in auxiliary losses or sequential models.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, img_channels=args.img_channels, base_channels=BASE_CHANNELS):\n",
    "        super(U_Net_Attn, self).__init__()\n",
    "        self.E1 = Unet_Enc(args, img_channels, base_channels)\n",
    "        self.D1 = Unet_Dec(args, img_channels, base_channels)\n",
    "        \n",
    "    def forward(self, in_frames):\n",
    "        # 1. Encoder Pass: Compute 4 skip features and the 4D bottleneck\n",
    "        feats1u, feats2u, feats3u, feats4u, bottleneck_4d = self.E1(in_frames)\n",
    "        \n",
    "        # 2. Decoder Pass: Reconstruct frame using bottleneck and skip connections\n",
    "        out_frames = self.D1(feats1u, feats2u, feats3u, feats4u, bottleneck_4d)\n",
    "        \n",
    "        # Returns reconstructed frame, plus all intermediate features\n",
    "        return out_frames, feats1u, feats2u, feats3u, feats4u, bottleneck_4d\n",
    "\n",
    "# --- EXECUTION AND TESTING ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Initializing U_Net_Attn and running a forward pass test on {args.device}...\")\n",
    "\n",
    "    # *** CRITICAL FIX: Set img_channels BEFORE model instantiation ***\n",
    "    # This ensures the model is built with 3 input/output channels.\n",
    "    args.img_channels = 3 \n",
    "    \n",
    "    # Instantiate the corrected full model\n",
    "    full_unet = U_Net_Attn(args).to(args.device) \n",
    "\n",
    "    # Example Input Test: (N, C, H, W)\n",
    "    dummy_input = torch.randn(args.N, args.img_channels, args.img_sz, args.img_sz).to(args.device)\n",
    "\n",
    "    # Forward Pass\n",
    "    output_frame, f1, f2, f3, f4, btl = full_unet(dummy_input)\n",
    "\n",
    "    # Count Total Parameters\n",
    "    num_params_enc = sum(p.numel() for p in full_unet.E1.parameters())\n",
    "    num_params_dec = sum(p.numel() for p in full_unet.D1.parameters())\n",
    "    num_params_total = num_params_enc + num_params_dec\n",
    "\n",
    "    print(\"\\n--- Full Model Summary ---\")\n",
    "    print(f\"Total Parameters: {num_params_total:,} (Targeting < 10M)\")\n",
    "    print(f\"  Encoder Parameters: {num_params_enc:,}\")\n",
    "    print(f\"  Decoder Parameters: {num_params_dec:,}\")\n",
    "    print(\"\\n--- Parameter Breakdown ---\")\n",
    "    \n",
    "    # Function to print module-specific parameters\n",
    "    def print_module_params(module, name):\n",
    "        total = sum(p.numel() for p in module.parameters())\n",
    "        if total > 0:\n",
    "            print(f\"  {name}: {total:,}\")\n",
    "\n",
    "    # Encoder Breakdown\n",
    "    print(\"\\nEncoder (Unet_Enc) Component Breakdown:\")\n",
    "    print_module_params(full_unet.E1.conv1, \"L1 Conv (3->28)\")\n",
    "    print_module_params(full_unet.E1.conv2, \"L2 Conv (28->56)\")\n",
    "    print_module_params(full_unet.E1.conv3, \"L3 Conv (56->112)\")\n",
    "    print_module_params(full_unet.E1.conv4, \"L4 Conv (112->224)\")\n",
    "    print_module_params(full_unet.E1.te_sa4_1, \"L4 Self-Attn (224D)\")\n",
    "    print_module_params(full_unet.E1.conv5, \"L5 Bottleneck (224->448)\")\n",
    "    \n",
    "    # Decoder Breakdown\n",
    "    print(\"\\nDecoder (Unet_Dec) Component Breakdown:\")\n",
    "    print_module_params(full_unet.D1.upconv4, \"L5->L4 UpConv (448->224)\")\n",
    "    print_module_params(full_unet.D1.up_conv4, \"L4 Concat Conv (448->224)\")\n",
    "    print_module_params(full_unet.D1.te_sa4d_1, \"L4 Self-Attn (224D)\")\n",
    "    print_module_params(full_unet.D1.mha4_ca, \"L4 Cross-Attn (224D)\")\n",
    "    \n",
    "    print_module_params(full_unet.D1.upconv3, \"L4->L3 UpConv (224->112)\")\n",
    "    print_module_params(full_unet.D1.up_conv3, \"L3 Concat Conv (224->112)\")\n",
    "    print_module_params(full_unet.D1.te_sa3d_1, \"L3 Self-Attn (112D)\")\n",
    "    print_module_params(full_unet.D1.mha3_ca, \"L3 Cross-Attn (112D)\")\n",
    "    \n",
    "    print_module_params(full_unet.D1.upconv2, \"L3->L2 UpConv (112->56)\")\n",
    "    print_module_params(full_unet.D1.up_conv2, \"L2 Concat Conv (112->56)\")\n",
    "    print_module_params(full_unet.D1.upconv1, \"L2->L1 UpConv (56->28)\")\n",
    "    print_module_params(full_unet.D1.up_conv1, \"L1 Concat Conv (56->28)\")\n",
    "    print_module_params(full_unet.D1.final_conv, \"L1 Final Conv (28->3)\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Test Results ---\")\n",
    "    print(f\"Output Frame Shape: {output_frame.shape}\")\n",
    "    print(f\"Intermediate Feature Shapes:\")\n",
    "    print(f\"  L1 Skip: {f1.shape}\")\n",
    "    print(f\"  L2 Skip: {f2.shape}\")\n",
    "    print(f\"  L3 Skip: {f3.shape}\")\n",
    "    print(f\"  L4 Skip: {f4.shape}\")\n",
    "    print(f\"  Bottleneck: {btl.shape}\")\n",
    "    print(\"Forward pass test completed successfully with U_Net_Attn.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOSS FUNCTIONS ---\n",
    "\n",
    "def dsc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Dice Similarity Coefficient (DSC) component.\n",
    "    Used for binary segmentation or reconstruction tasks to measure overlap.\n",
    "    \"\"\"\n",
    "    smooth = 1.\n",
    "    y_true_f = torch.flatten(y_true)\n",
    "    y_pred_f = torch.flatten(y_pred)\n",
    "    # This calculation implements a soft mask based on the product\n",
    "    mask = y_true_f * y_pred_f\n",
    "    intersection = torch.sum(mask)\n",
    "    score = (2. * intersection + smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Dice Loss combined with Binary Cross-Entropy (BCE).\n",
    "    This combined loss stabilizes training for tasks involving binary prediction/reconstruction.\n",
    "    The primary loss term is (1 - DSC).\n",
    "    \"\"\"\n",
    "    # Dice Loss: 1 - DSC\n",
    "    dice_l = (1 - dsc(y_true, y_pred))\n",
    "    # Combined with BCE for stable gradient signals\n",
    "    bce_l = nn.functional.binary_cross_entropy(y_pred, y_true)\n",
    "    return dice_l + bce_l\n",
    "\n",
    "\n",
    "# --- TRAINING FUNCTIONS ---\n",
    "\n",
    "# --- 2. TRAINING FUNCTIONS (ADJUSTED FOR DATA SIZE) ---\n",
    "\n",
    "def single_iter_AE(model, optimizer, loss_fn, loss_fn2, in_frames_np, args):\n",
    "    \"\"\"\n",
    "    Runs a single training iteration (batch update) for the Autoencoder (AE) model.\n",
    "    Handles slicing the 768-wide data tensor to get the 256-wide FAF input.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # 1. Prepare Data\n",
    "    # in_frames_np shape: (Batch_Size, 1, T=4, H=256, W_total=768)\n",
    "    batch_size, _, time_steps, H, W_total = in_frames_np.shape\n",
    "    img_sz = args.img_sz\n",
    "    \n",
    "    # Slice FAFs (first 256 columns) and reshape for U-Net input (B*T, C, H, W)\n",
    "    in_frames_sliced = in_frames_np[:, 0, :, :, 0:img_sz] \n",
    "    in_frames = in_frames_sliced.reshape(batch_size * time_steps, 1, H, img_sz)\n",
    "    \n",
    "    # Convert to tensor and move to device\n",
    "    # NOTE: This is safe because in_frames_np was created from a CPU numpy array by DataWrapper\n",
    "    in_frames = torch.from_numpy(in_frames).float().to(args.device)\n",
    "    target_frames = in_frames # Autoencoder target is the input\n",
    "\n",
    "    # 2. Zero Gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 3. Forward Pass (AE_model returns 6 outputs)\n",
    "    out_frames, _, _, _, _, _ = model(in_frames)\n",
    "\n",
    "    # 4. Compute Loss (loss_fn=BCELoss, loss_fn2=L1Loss)\n",
    "    loss_main = loss_fn(out_frames, target_frames)\n",
    "    loss_aux = loss_fn2(out_frames, target_frames)\n",
    "    total_loss = loss_main + loss_aux \n",
    "\n",
    "    # 5. Backward Pass and Optimization\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 6. Return Loss\n",
    "    return total_loss.item()\n",
    "\n",
    "def f_single_epoch_AE(train_data, model, optimizer, loss, loss2, args):\n",
    "    \"\"\" Executes a single training epoch for the Autoencoder model. \"\"\"\n",
    "    model.train()\n",
    "    random_idxs = np.arange(len(train_data.data))\n",
    "    np.random.shuffle(random_idxs)\n",
    "    train_data_shuffle = train_data.data[random_idxs]\n",
    "    \n",
    "    num_samples = len(train_data.data)\n",
    "    batch_size = args.batch_size\n",
    "    args.num_batches = num_samples // batch_size # Set num_batches dynamically\n",
    "\n",
    "    epoch_losses = np.zeros(args.num_batches)\n",
    "\n",
    "    for it in tqdm(range(args.num_batches), desc=\"Batch progress\"):\n",
    "        # Data shape: [Batch_Size, 1, T=4, H=256, W_total=768] (numpy)\n",
    "        in_frames_np = train_data_shuffle[it * batch_size:(it + 1) * batch_size, :, :, :, :]\n",
    "\n",
    "        # Call the single iteration function\n",
    "        epoch_losses[it] = single_iter_AE(model, optimizer, loss, loss2, in_frames_np, args)\n",
    "        \n",
    "    return epoch_losses\n",
    "\n",
    "## Display\n",
    "\n",
    "def f_display_autoencoder(train_data, model, args, i=0):\n",
    "    \n",
    "    random_idxs = np.arange(len(train_data.data))\n",
    "    np.random.shuffle(random_idxs)\n",
    "    train_data_shuffle = train_data.data[random_idxs]\n",
    "    it = 0\n",
    "    in_frames = train_data_shuffle[it*args.batch_size:(it+1)*args.batch_size,:,:,:,:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec_frames, _, _, _, _ = model(in_frames)\n",
    "        \n",
    "    if i == 0:\n",
    "        plt.imshow(in_frames[i,0,0,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "        plt.show()\n",
    "        plt.imshow(rec_frames[i,0,0,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "        plt.show()\n",
    "    else:\n",
    "        for i in np.arange(args.batch_size):\n",
    "            plt.imshow(in_frames[i,0,0,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "            plt.show()\n",
    "            plt.imshow(rec_frames[i,0,0,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "# --- 6. TRAINING EXECUTION ---\n",
    "\n",
    "train_data = DataWrapper(Train_Data.detach().cpu())\n",
    "\n",
    "# Create necessary path for checkpointing\n",
    "resume_AE_ckpt.mkdir(exist_ok=True)\n",
    "print(f\"\\nCheckpoints will be saved to: {resume_AE_ckpt.resolve()}\")\n",
    "\n",
    "# Model Initialization\n",
    "AE_model = U_Net_Attn(args).to(args.device)\n",
    "\n",
    "# Training setup\n",
    "log_mean_epoch_losses = np.zeros(args.num_epochs)\n",
    "\n",
    "# Loss functions from your provided script\n",
    "loss = nn.BCELoss(reduction='mean') # BCE on its own\n",
    "loss2 = nn.L1Loss(reduction='mean') # L1 on its own\n",
    "\n",
    "lr = 1E-3\n",
    "optimizer = torch.optim.Adam(AE_model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "print(f\"\\nStarting training on device: {args.device} for {args.num_epochs} epochs...\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(np.arange(args.num_epochs), desc=\"Training progress...\"):\n",
    "    # Pass the DataWrapper instance\n",
    "    epoch_losses = f_single_epoch_AE(train_data, AE_model, optimizer, loss, loss2, args)\n",
    "    \n",
    "    # Calculate and log the mean epoch loss\n",
    "    mean_epoch_loss = np.mean(epoch_losses)\n",
    "    log_mean_epoch_losses[epoch] = np.log(mean_epoch_loss)\n",
    "    \n",
    "    # Visualization and Plotting (at set intervals)\n",
    "    if np.mod(epoch, args.show_example_epochs) == 0:\n",
    "        print(f\"\\n--- Epoch {epoch}: Mean Loss (log): {log_mean_epoch_losses[epoch]:.4f} ---\")\n",
    "        f_display_autoencoder(train_data, AE_model, args)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(log_mean_epoch_losses[0:epoch])\n",
    "        plt.title('Log Mean Epoch Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Log Loss')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "print(\"\\nTraining complete. Saving final checkpoints...\")\n",
    "\n",
    "# # Final Checkpoint Saving (Adapted to your model structure)\n",
    "# F_Enc_path_save = resume_AE_ckpt.joinpath('U_F_Enc_c16d4_ckpt1.pth')\n",
    "# F_Dec_path_save = resume_AE_ckpt.joinpath('U_F_Dec_c16d4_ckpt1.pth')\n",
    "\n",
    "# # Save the full Unet_Enc and Unet_Dec modules\n",
    "# torch.save(AE_model.E1.state_dict(), F_Enc_path_save)\n",
    "# torch.save(AE_model.D1.state_dict(), F_Dec_path_save)\n",
    "\n",
    "# print(f\"Checkpoints saved: {F_Enc_path_save.name} and {F_Dec_path_save.name}\")\n",
    "# print(\"Script finished execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_single_epoch_FM(Train_Data, model, optimizer, loss, loss2, args):\n",
    "    \n",
    "    random_idxs = np.arange(len(Train_Data))\n",
    "    np.random.shuffle(random_idxs)\n",
    "    train_data_shuffle = Train_Data[random_idxs]\n",
    "\n",
    "    epoch_losses = np.zeros(args.num_batches)\n",
    "\n",
    "    for it in tqdm(range(args.num_batches)):\n",
    "\n",
    "        in_frames_tot = train_data_shuffle[it*args.batch_size:(it+1)*args.batch_size,:,:,:,:]\n",
    "\n",
    "        epoch_losses[it] = single_iter_FM(model, optimizer, loss, loss2, in_frames_tot, args)\n",
    "        \n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_display(Data, R1, args):\n",
    "    \n",
    "    random_idxs = np.arange(len(Data))\n",
    "#     np.random.shuffle(random_idxs)\n",
    "    train_data_shuffle = Data[random_idxs]\n",
    "    \n",
    "    \n",
    "    save_folder = 'C:/Users/Pracioppo/Desktop/Example GA Imgs/'\n",
    "    \n",
    "    for it in range(1):\n",
    "#     for it in range(100):\n",
    "        in_frames_tot = train_data_shuffle[it*args.batch_size:(it+1)*args.batch_size,:,:,:,:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "    #         in_frames = in_frames_tot[:,:,0,:,:].unsqueeze(1)\n",
    "    #         out_frames_tot, _, _, _, _ = R1(in_frames)\n",
    "            out_frames_tot, _, _, _, _, _ = R1(in_frames_tot)\n",
    "\n",
    "\n",
    "        for i in range(in_frames_tot.size()[0]):\n",
    "            plt.figure(figsize=(10,6))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "    #         plt.imshow(in_frames_tot[i,0,j,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "            plt.imshow(in_frames_tot[i,0,-1,:,:-512].detach().cpu().numpy(), cmap='gray')\n",
    "            plt.savefig(save_folder + 'raw_image_' + str(it + i) + '.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "            plt.subplot(1, 3, 2)\n",
    "#             plt.imshow(in_frames_tot[i,0,j,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "            plt.imshow((in_frames_tot[i,0,-1,:,-256:]).detach().cpu().numpy(), cmap='gray')\n",
    "#             plt.imshow((in_frames_tot[i,0,-1,:,-256:]>0.5).detach().cpu().numpy(), cmap='gray')\n",
    "#             plt.savefig(save_folder + 'growth_ground_truth_' + str(it + i) + '.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(out_frames_tot[i,j,0,0,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "            plt.imshow((out_frames_tot[i,-1,0,0,:,-256:]).detach().cpu().numpy(), cmap='gray')\n",
    "#             plt.imshow((out_frames_tot[i,-1,0,0,:,-256:]>0.5).detach().cpu().numpy(), cmap='gray')\n",
    "            plt.savefig(save_folder + 'growth_predicted_' + str(it + i) + '.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            print(out_frames_tot.size())\n",
    "\n",
    "            print(out_frames_tot.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_display2(Data, R1, args):\n",
    "\n",
    "    train_data_shuffle = Data\n",
    "    \n",
    "    save_folder = 'C:/Users/Pracioppo/Desktop/Example GA Imgs/'\n",
    "\n",
    "    for it in range(100):\n",
    "#     for it in range(100):\n",
    "        in_frames_tot = train_data_shuffle[it*args.batch_size:(it+1)*args.batch_size,:,:,:,:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "    #         in_frames = in_frames_tot[:,:,0,:,:].unsqueeze(1)\n",
    "    #         out_frames_tot, _, _, _, _ = R1(in_frames)\n",
    "            out_frames_tot, _, _, _, _, _ = R1(in_frames_tot)\n",
    "\n",
    "#         for i in range(in_frames_tot.size()[0]):\n",
    "#             for i in range(3):\n",
    "            i = it\n",
    "            for j in range(4):\n",
    "#             j = -1\n",
    "    #             plt.figure(figsize=(10,6))\n",
    "\n",
    "    #             plt.subplot(1, 3, 1)\n",
    "        #         plt.imshow(in_frames_tot[i,0,j,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "    #             plt.imshow(in_frames_tot[i,0,-1,:,:-512].detach().cpu().numpy(), cmap='gray')\n",
    "    #             plt.savefig(save_folder + 'raw_image_' + str(it + i) + '.png', bbox_inches='tight', pad_inches=0)\n",
    "                plt.imshow((in_frames_tot[i,0,j,:,-512:-256]>0.5).detach().cpu().numpy(), cmap='gray')\n",
    "                plt.savefig(save_folder + 'mask_ground_truth_' + str(it + i) + '_step' + str(j) + '.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    #             plt.subplot(1, 3, 2)\n",
    "    #             plt.imshow(in_frames_tot[i,0,j,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "#                     plt.imshow((in_frames_tot[i,0,j,:,-256:]).detach().cpu().numpy(), cmap='gray')\n",
    "                plt.imshow((in_frames_tot[i,0,-1,:,-256:]>0.5).detach().cpu().numpy(), cmap='gray')\n",
    "                plt.savefig(save_folder + 'growth_ground_truth_' + str(it + i) + '_step' + str(j) + '.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "#                     plt.imshow((out_frames_tot[i,j,0,0,:,-512:-256]).detach().cpu().numpy(), cmap='gray')\n",
    "                plt.imshow((out_frames_tot[i,-1,0,0,:,-512:-256:]>0.5).detach().cpu().numpy(), cmap='gray')\n",
    "                plt.savefig(save_folder + 'mask_predicted_' + str(it + i) + '_step' + str(j) + '.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    #             plt.subplot(1, 3, 3)\n",
    "    #             plt.imshow(out_frames_tot[i,j,0,0,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "#                     plt.imshow((out_frames_tot[i,j,0,0,:,-256:]).detach().cpu().numpy(), cmap='gray')\n",
    "                plt.imshow((out_frames_tot[i,-1,0,0,:,-256:]>0.5).detach().cpu().numpy(), cmap='gray')\n",
    "                plt.savefig(save_folder + 'growth_predicted_' + str(it + i) + '_step' + str(j) + '.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "#                 plt.show()\n",
    "\n",
    "            print(out_frames_tot.size())\n",
    "\n",
    "            print(out_frames_tot.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_display_all(Data, R1, args):\n",
    "    \n",
    "    random_idxs = np.arange(len(Data))\n",
    "    np.random.shuffle(random_idxs)\n",
    "    train_data_shuffle = Data[random_idxs]\n",
    "    \n",
    "    for it in np.arange(int(len(train_data_shuffle)/args.batch_size)):\n",
    "    \n",
    "        in_frames_tot = train_data_shuffle[it*args.batch_size:(it+1)*args.batch_size,:,:,:,:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            out_frames_tot, _, _, _, _, _ = R1(in_frames_tot)\n",
    "\n",
    "        for i in np.arange(args.batch_size):\n",
    "#             for j in np.arange(4):\n",
    "            j = 3\n",
    "\n",
    "            plt.figure(figsize=(10,6))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(in_frames_tot[i,0,j,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(out_frames_tot[i,j,0,0,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.d_attn1 = 64\n",
    "args.d_attn2 = 64\n",
    "args.d_attn1t = args.d_attn1*3\n",
    "args.d_attn2t = args.d_attn2*3\n",
    "args.nhead = 4\n",
    "\n",
    "U1 = U_Net_F(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idxs = np.arange(len(Train_Data))\n",
    "np.random.shuffle(random_idxs)\n",
    "train_data_shuffle = Train_Data[random_idxs]\n",
    "# it = 0\n",
    "# in_frames_tot = train_data_shuffle[it*args.batch_size:(it+1)*args.batch_size,:,:,:,:]\n",
    "# in_frames_auto = in_frames_tot.flatten(0,2).unsqueeze(1).unsqueeze(2)\n",
    "in_frames_auto = train_data_shuffle.flatten(0,2).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "train_data.data = in_frames_auto\n",
    "args.num_batches = int(len(Train_Data)/args.N)\n",
    "\n",
    "print(train_data.data.size())\n",
    "print(args.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Data.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "30TYawOgDLHs",
    "outputId": "99da647a-d93f-4399-b561-a1fc198a2cb6"
   },
   "outputs": [],
   "source": [
    "#####################Training loop (for autoencoder) ###########################\n",
    "\n",
    "args.num_epochs = 200\n",
    "args.show_example_epochs = 5\n",
    "\n",
    "log_mean_epoch_losses = np.zeros(args.num_epochs)\n",
    "\n",
    "loss = nn.BCELoss() # <-- Use this one\n",
    "loss2 = nn.L1Loss()\n",
    "\n",
    "lr = 1E-3\n",
    "optimizer = torch.optim.Adam(AE_model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# Train for a maximum of max_epochs:\n",
    "for epoch in tqdm(np.arange(args.num_epochs), desc=\"Training progress...\"):\n",
    "    epoch_losses = f_single_epoch_AE(train_data, AE_model, optimizer, loss, loss2, args)\n",
    "    log_mean_epoch_losses[epoch] = np.log(np.mean(epoch_losses))\n",
    "#     coord_errors[epoch] = f_coord_error(train_data,train_data_traj,VPTR_Enc, C1, args)\n",
    "    \n",
    "    if np.mod(epoch,args.show_example_epochs) == 0:\n",
    "        f_display_autoencoder(train_data, AE_model, args)\n",
    "        \n",
    "        plt.plot(log_mean_epoch_losses[0:epoch])\n",
    "        plt.show()\n",
    "        \n",
    "#         plt.plot(coord_errors[0:epoch])\n",
    "#         plt.grid()\n",
    "#         plt.show()\n",
    "\n",
    "F_Enc_path_save = resume_AE_ckpt.joinpath('U_F_Enc_c16d4_ckpt1')\n",
    "F_Dec_path_save = resume_AE_ckpt.joinpath('U_F_Dec_c16d4_ckpt1')\n",
    "Enc_path_save = resume_AE_ckpt.joinpath('U_Enc_c16d4_ckpt1')\n",
    "Dec_path_save = resume_AE_ckpt.joinpath('U_Dec_c16d4_ckpt1')\n",
    "\n",
    "torch.save(AE_model.E1.state_dict(), F_Enc_path_save)\n",
    "torch.save(AE_model.D1.state_dict(), F_Dec_path_save)\n",
    "torch.save(AE_model.E1.Enc1.state_dict(), Enc_path_save)\n",
    "torch.save(AE_model.D1.Dec1.state_dict(), Dec_path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F_Enc_path_save = resume_AE_ckpt.joinpath('U_F_Enc_c16d4_ckpt1')\n",
    "# F_Dec_path_save = resume_AE_ckpt.joinpath('U_F_Dec_c16d4_ckpt1')\n",
    "# Enc_path_save = resume_AE_ckpt.joinpath('U_Enc_c16d4_ckpt1')\n",
    "# Dec_path_save = resume_AE_ckpt.joinpath('U_Dec_c16d4_ckpt1')\n",
    "\n",
    "# torch.save(AE_model.E1.state_dict(), F_Enc_path_save)\n",
    "# torch.save(AE_model.D1.state_dict(), F_Dec_path_save)\n",
    "# torch.save(AE_model.E1.Enc1.state_dict(), Enc_path_save)\n",
    "# torch.save(AE_model.D1.Dec1.state_dict(), Dec_path_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(seq_len, device):\n",
    "    # True where masking should happen\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask  # shape (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_encode(feats1u,args):\n",
    "    feats1u_a = feats1u.flatten(start_dim=2)\n",
    "    feats1u_b = torch.swapaxes(feats1u_a,1,2)\n",
    "    feats1u_c = f_pos_encoding_additive(feats1u_b,args).squeeze()\n",
    "    return feats1u_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn_t(nn.Module):\n",
    "    def __init__(self, dim, args):\n",
    "        super(Attn_t, self).__init__()\n",
    "\n",
    "        self.te1 = torch.nn.TransformerEncoderLayer(d_model=dim, nhead=args.nhead, dim_feedforward=dim, dropout=0, layer_norm_eps=1e-05, batch_first=True).to(args.device)\n",
    "        self.te2 = torch.nn.TransformerEncoderLayer(d_model=dim, nhead=args.nhead, dim_feedforward=dim, dropout=0, layer_norm_eps=1e-05, batch_first=True).to(args.device)\n",
    "\n",
    "    def forward(self, feats1n_tot):\n",
    "        \n",
    "        feats1n_tota = torch.swapaxes(feats1n_tot,1,2)\n",
    "        sz = feats1n_tota.size()\n",
    "        x = feats1n_tota.reshape(sz[0],sz[1],sz[2]*sz[3]*sz[4])\n",
    "        xa = torch.swapaxes(x,1,2)\n",
    "        xb = f_pos_encoding_additive(xa,args).squeeze()\n",
    "        \n",
    "        q1 = self.te1(xb)\n",
    "        \n",
    "        q2 = self.te2(q1)\n",
    "\n",
    "        q2 = torch.swapaxes(q2,1,2)\n",
    "        \n",
    "        return torch.swapaxes(q2.reshape(feats1n_tota.size()),1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sliding_Window_Attn(nn.Module):\n",
    "    def __init__(self, dim, args):\n",
    "        super(Sliding_Window_Attn, self).__init__()\n",
    "        \n",
    "        self.A1 = Attn_t(dim, args)\n",
    "        \n",
    "    def forward(self, feats1u_tot):\n",
    "    \n",
    "        feats1u_out = torch.zeros_like(feats1u_tot)\n",
    "\n",
    "        for i in np.arange(3):\n",
    "            x = f_feats_attn(feats1u_tot, self.A1)\n",
    "            feats1u_out[:,-(i+1),:,:,:] = x[:,-1,:,:,:]\n",
    "            feats1u_tot = torch.roll(feats1u_tot,1)\n",
    "            feats1u_tot[:,0,:,:,:] = 0\n",
    "\n",
    "        return feats1u_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_feats_attn(feats1n_tot,model):\n",
    "\n",
    "    feats1n_tota = torch.swapaxes(feats1n_tot,1,2)\n",
    "    sz = feats1n_tota.size()\n",
    "    x = feats1n_tota.reshape(sz[0],sz[1],sz[2]*sz[3]*sz[4])\n",
    "    xa = torch.swapaxes(x,1,2)\n",
    "    xb = f_pos_encoding_additive(xa,args).squeeze()\n",
    "    q1 = model.te1(xb)\n",
    "    q2 = model.te2(q1)\n",
    "    q2 = torch.swapaxes(q2,1,2)\n",
    "\n",
    "    return torch.swapaxes(q2.reshape(feats1n_tota.size()),1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_t_Attn(nn.Module):\n",
    "    def __init__(self, dim, args):\n",
    "        super(Cross_t_Attn, self).__init__()\n",
    "\n",
    "        self.mha1 = torch.nn.MultiheadAttention(embed_dim=dim, num_heads=args.nhead, batch_first=True).to(args.device)\n",
    "\n",
    "    def forward(self, feats1, feats2):\n",
    "\n",
    "        x0_a = feats1.flatten(start_dim=2)\n",
    "        x1_a = feats2.flatten(start_dim=2)\n",
    "\n",
    "        x0_b = torch.swapaxes(x0_a,1,2)\n",
    "        x1_b = torch.swapaxes(x1_a,1,2)\n",
    "        \n",
    "        x0_f = f_pos_encoding_additive(x0_b,args).squeeze()\n",
    "        x1_f = f_pos_encoding_additive(x1_b,args).squeeze()\n",
    "\n",
    "        attn_output1, _ = self.mha1(x1_f, x0_f, x0_f)\n",
    "        \n",
    "        out = attn_output1.reshape(feats1.size())\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Attn(nn.Module):\n",
    "    def __init__(self, dim, args):\n",
    "        super(Cross_Attn, self).__init__()\n",
    "        \n",
    "#         self.mask = causal_mask(seq_len, args.device)\n",
    "\n",
    "        self.mha10 = torch.nn.MultiheadAttention(embed_dim=dim, num_heads=args.nhead, batch_first=True).to(args.device)\n",
    "        self.mha11 = torch.nn.MultiheadAttention(embed_dim=dim, num_heads=args.nhead, batch_first=True).to(args.device)\n",
    "        self.mha12 = torch.nn.MultiheadAttention(embed_dim=dim, num_heads=args.nhead, batch_first=True).to(args.device)\n",
    "        self.mha20 = torch.nn.MultiheadAttention(embed_dim=dim, num_heads=args.nhead, batch_first=True).to(args.device)\n",
    "        self.mha21 = torch.nn.MultiheadAttention(embed_dim=dim, num_heads=args.nhead, batch_first=True).to(args.device)\n",
    "        self.mha22 = torch.nn.MultiheadAttention(embed_dim=dim, num_heads=args.nhead, batch_first=True).to(args.device)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        \n",
    "        sz = int(feats.size()[-1]/3)\n",
    "\n",
    "        x0 = feats[:,:,:,0:sz]\n",
    "        x1 = feats[:,:,:,sz:2*sz]\n",
    "        x2 = feats[:,:,:,2*sz:3*sz]\n",
    "\n",
    "        x0_a = x0.flatten(start_dim=2)\n",
    "        x1_a = x1.flatten(start_dim=2)\n",
    "        x2_a = x2.flatten(start_dim=2)\n",
    "\n",
    "        x0_b = torch.swapaxes(x0_a,1,2)\n",
    "        x1_b = torch.swapaxes(x1_a,1,2)\n",
    "        x2_b = torch.swapaxes(x2_a,1,2)\n",
    "        \n",
    "        x0_f = f_pos_encoding_additive(x0_b,args).squeeze()\n",
    "        x1_f = f_pos_encoding_additive(x1_b,args).squeeze()\n",
    "        x2_f = f_pos_encoding_additive(x2_b,args).squeeze()\n",
    "        \n",
    "        attn_output10, _ = self.mha10(x1_f, x0_f, x0_f)\n",
    "        attn_output11, _ = self.mha11(x1_f, x1_f, x1_f)\n",
    "        attn_output12, _ = self.mha12(x1_f, x2_f, x2_f)\n",
    "\n",
    "        attn_output20, _ = self.mha20(x2_f, x0_f, x0_f)\n",
    "        attn_output21, _ = self.mha21(x2_f, x1_f, x1_f)\n",
    "        attn_output22, _ = self.mha22(x2_f, x2_f, x2_f)\n",
    "        \n",
    "        y1_f = attn_output10 + attn_output11 + attn_output12\n",
    "        z1_f = attn_output20 + attn_output21 + attn_output22 \n",
    "\n",
    "        out = torch.zeros_like(feats)\n",
    "        out[:,:,:,sz:2*sz] = y1_f.reshape(x1.size())\n",
    "        out[:,:,:,2*sz:3*sz] = z1_f.reshape(x1.size())\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetD(nn.Module):\n",
    "    def __init__(self, dim, args):\n",
    "        super(UnetD, self).__init__()\n",
    "\n",
    "        self.e0 = nn.Conv2d(dim, dim, kernel_size=3, padding=1).to(args.device)\n",
    "        self.e1 = nn.Conv2d(dim, dim, kernel_size=2, stride = 2, padding=0).to(args.device)\n",
    "        self.e2 = nn.Conv2d(dim, dim, kernel_size=2, stride = 2, padding=0).to(args.device)\n",
    "        self.m1 = nn.ConvTranspose2d(dim, dim, kernel_size=2, stride=2).to(args.device)\n",
    "        self.m2 = nn.ConvTranspose2d(dim, dim, kernel_size=2, stride=2).to(args.device)\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "\n",
    "        self.c1 = torch.nn.Parameter(torch.tensor([0.0])).to(args.device)\n",
    "        self.c2 = torch.nn.Parameter(torch.tensor([0.0])).to(args.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.ReLU(self.e0(x)) + x\n",
    "        x1 = self.ReLU(self.e1(x0))\n",
    "        x2 = self.ReLU(self.e2(x1))\n",
    "        x3 = self.c1*self.ReLU(self.m1(x2)) + x1\n",
    "        x4 = self.c2*self.m2(x3)\n",
    "\n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinD(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(LinD, self).__init__()\n",
    "        \n",
    "        # Decoder\n",
    "        self.h1 = nn.Linear(dim, dim).to(args.device)\n",
    "        self.h2 = nn.Linear(dim, dim).to(args.device)\n",
    "        self.h3 = nn.Linear(dim, dim).to(args.device)\n",
    "        self.h4 = nn.Linear(dim, dim).to(args.device)\n",
    "        \n",
    "        self.ReLU = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.ReLU(self.h1(x))\n",
    "        a2 = self.ReLU(self.h2(a1))\n",
    "        a3 = self.ReLU(self.h3(a2))\n",
    "        a4 = self.h4(a3)\n",
    "\n",
    "        return a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, device):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        # Learnable scalar gating parameters\n",
    "        self.w_gamma = nn.Parameter(torch.tensor(1.0, device=device))\n",
    "        self.b_gamma = nn.Parameter(torch.tensor(0.0, device=device))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        # Standard multi-head attention\n",
    "        attn_output, attn_weights = self.mha(query, key, value, attn_mask=attn_mask)\n",
    "        # attn_weights: [batch, seq_len, seq_len], sum along keys gives total precision per query\n",
    "        row_sums = attn_weights.sum(dim=-1, keepdim=True)  # shape [batch, seq_len, 1]\n",
    "\n",
    "        # Compute confidence gate\n",
    "        gamma = self.sigmoid(self.w_gamma * row_sums + self.b_gamma)  # shape [batch, seq_len, 1]\n",
    "\n",
    "        # Modulate the attention output\n",
    "        attn_output = gamma * attn_output + query  # residual connection\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "    \n",
    "class GatedTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, device):\n",
    "        super().__init__()\n",
    "        self.attn = GatedMultiheadAttention(embed_dim=d_model, num_heads=nhead, device=device)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, dim]\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(attn_out)\n",
    "        ff_out = self.linear2(torch.relu(self.linear1(x)))\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DynNet, self).__init__()\n",
    "\n",
    "        self.UD1 = UnetD(128, args)\n",
    "        self.UD1_s = UnetD(128, args)\n",
    "        self.CA1 = Cross_Attn(128, args)\n",
    "        self.CA2 = Cross_Attn(128, args)\n",
    "        self.CA3 = Cross_Attn(64, args)\n",
    "        self.CA4 = Cross_Attn(64, args)\n",
    "        self.UD2 = UnetD(64, args)\n",
    "        self.UD2_s = UnetD(64, args)\n",
    "        self.H4 = LinD(64)\n",
    "        self.H5 = LinD(16)\n",
    "        \n",
    "        self.ea = nn.Conv2d(128, 64, kernel_size=3, padding=1).to(args.device) # output: 136x136x256\n",
    "        self.eb = nn.Conv2d(64, 128, kernel_size=3, padding=1).to(args.device) # output: 136x136x256\n",
    "        \n",
    "        self.te1u_1 = torch.nn.TransformerEncoderLayer(d_model=args.d_attn1, nhead=args.nhead, dim_feedforward=args.d_attn1, dropout=0, layer_norm_eps=1e-05, batch_first=True).to(args.device)\n",
    "#         self.te1u_1 = GatedTransformerEncoderLayer(d_model=args.d_attn1, nhead=args.nhead, dim_feedforward=args.d_attn, args.device)\n",
    "\n",
    "        # self.te1u_2 = torch.nn.TransformerEncoderLayer(d_model=args.d_attn1, nhead=args.nhead, dim_feedforward=args.d_attn1, dropout=0, layer_norm_eps=1e-05, batch_first=True).to(args.device)\n",
    "\n",
    "        self.te2u_1 = torch.nn.TransformerEncoderLayer(d_model=args.d_attn2, nhead=args.nhead, dim_feedforward=args.d_attn2, dropout=0, layer_norm_eps=1e-05, batch_first=True).to(args.device)\n",
    "#         self.te2u_2 = torch.nn.TransformerEncoderLayer(d_model=args.d_attn2, nhead=args.nhead, dim_feedforward=args.d_attn2, dropout=0, layer_norm_eps=1e-05, batch_first=True).to(args.device)\n",
    "        \n",
    "    def forward(self, feats1u, feats2u, feats4u_f, feats5_f):\n",
    "\n",
    "        feats1u_s = torch.zeros_like(feats1u)\n",
    "        feats1u_s[:,:,:,0:16] = feats1u[:,:,:,0:16]\n",
    "        feats1u_s[:,:,:,16:32] = feats1u[:,:,:,32:48]\n",
    "        feats1u_s[:,:,:,32:48] = feats1u[:,:,:,16:32]\n",
    "        \n",
    "        feats1u = feats1u + self.CA1(feats1u)\n",
    "        feats1u_s = feats1u_s + self.CA1(feats1u_s)\n",
    "\n",
    "        v1 = self.UD1(feats1u)\n",
    "        v2 = self.UD1_s(feats1u_s)\n",
    "\n",
    "        V1 = v1 + v2\n",
    "\n",
    "        feats1u = feats1u + V1\n",
    "        feats1u[:,:,:,32:48] = feats1u[:,:,:,32:48] + V1[:,:,:,16:32]\n",
    "#         feats1u[:,:,:,32:48] = V1[:,:,:,16:32]        \n",
    "        \n",
    "        feats1u_b = self.ea(feats1u)\n",
    "        feats1u_a = feats1u_b.flatten(start_dim=2)\n",
    "        feats1u_a = torch.swapaxes(feats1u_a,1,2)\n",
    "        feats1u_a = f_pos_encoding_additive(feats1u_a,args).squeeze()\n",
    "        \n",
    "#         seq_len = feats1u_a.size()[1]\n",
    "#         mask = causal_mask(seq_len, args.device)\n",
    "#         q2 = self.te1u_1(feats1u_a, src_mask=mask, is_causal=True)\n",
    "        q2 = self.te1u_1(feats1u_a)\n",
    "        q2 = torch.swapaxes(q2,1,2)\n",
    "        feats1u_b = feats1u_b + q2.reshape(feats1u_b.size())\n",
    "        feats1u = feats1u + self.eb(feats1u_b)\n",
    "\n",
    "        feats2u_s = torch.zeros_like(feats2u)\n",
    "        feats2u_s[:,:,:,0:4] = feats2u[:,:,:,0:4]\n",
    "        feats2u_s[:,:,:,4:8] = feats2u[:,:,:,8:12]\n",
    "        feats2u_s[:,:,:,8:12] = feats2u[:,:,:,4:8]\n",
    "        \n",
    "        feats2u = feats2u + self.CA3(feats2u)\n",
    "        feats2u_s = feats2u_s + self.CA4(feats2u_s)\n",
    "\n",
    "        v3 = self.UD2(feats2u)\n",
    "        v4 = self.UD2_s(feats2u_s)\n",
    "\n",
    "        V2 = v3 + v4\n",
    "\n",
    "        feats2u = feats2u + V2\n",
    "        feats2u[:,:,:,8:12] = feats2u[:,:,:,8:12] + V2[:,:,:,4:8]\n",
    "#         feats2u[:,:,:,8:12] = V2[:,:,:,4:8]\n",
    "\n",
    "        feats2u_a = feats2u.flatten(start_dim=2)\n",
    "        feats2u_a = torch.swapaxes(feats2u_a,1,2)\n",
    "        feats2u_a = f_pos_encoding_additive(feats2u_a,args).squeeze()\n",
    "        \n",
    "#         seq_len = feats2u_a.size()[1]\n",
    "#         mask = causal_mask(seq_len, args.device)\n",
    "#         q2 = self.te2u_1(feats2u_a, src_mask=mask, is_causal=True)\n",
    "        q2 = self.te2u_1(feats2u_a)\n",
    "        q2 = torch.swapaxes(q2,1,2)\n",
    "        feats2u = feats2u + q2.reshape(feats2u.size())\n",
    "\n",
    "        feats4u_f = feats4u_f + self.H4(feats4u_f)\n",
    "        feats5_f = feats5_f + self.H5(feats5_f)\n",
    "\n",
    "        return feats1u, feats2u, feats4u_f, feats5_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RM_net3(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(RM_net3, self).__init__()\n",
    "        \n",
    "        self.U1 = U_Net_F(args)\n",
    "        self.F1 = DynNet(args)\n",
    "        \n",
    "        self.SWA1 = Sliding_Window_Attn(128,args)\n",
    "        self.SWA2 = Sliding_Window_Attn(64,args)\n",
    "        \n",
    "        self.CtA12_1 = Cross_t_Attn(128,args)\n",
    "        self.CtA12_2 = Cross_t_Attn(64,args)\n",
    "        self.CtA13_1 = Cross_t_Attn(128,args)\n",
    "        self.CtA13_2 = Cross_t_Attn(64,args)\n",
    "\n",
    "    def forward(self, in_frames_tot):\n",
    "        \n",
    "        feats1u_tot = torch.zeros(args.batch_size, 3, 128, 16, 48).to(args.device)\n",
    "        feats2u_tot = torch.zeros(args.batch_size, 3, 64, 4, 12).to(args.device)\n",
    "        feats4u_tot = torch.zeros(args.batch_size, 3, 64).to(args.device)\n",
    "        feats5u_tot = torch.zeros(args.batch_size, 3, 16).to(args.device)\n",
    "        \n",
    "        feats1u_tot_p = torch.zeros_like(feats1u_tot).to(args.device)\n",
    "        feats2u_tot_p = torch.zeros_like(feats2u_tot).to(args.device)\n",
    "        feats4u_tot_p = torch.zeros_like(feats4u_tot).to(args.device)\n",
    "        feats5u_tot_p = torch.zeros_like(feats5u_tot).to(args.device)\n",
    "        \n",
    "        feats1n_tot = torch.zeros(args.batch_size, 4, 128, 16, 48).to(args.device)\n",
    "        feats2n_tot = torch.zeros(args.batch_size, 4, 64, 4, 12).to(args.device)\n",
    "        feats4n_tot = torch.zeros(args.batch_size, 4, 64).to(args.device)\n",
    "        feats5n_tot = torch.zeros(args.batch_size, 4, 16).to(args.device)\n",
    "        \n",
    "        out_frames_tot = torch.swapaxes(torch.zeros_like(in_frames_tot),1,2).unsqueeze(3).to(args.device)\n",
    "        out_frames1 = torch.swapaxes(torch.zeros_like(in_frames_tot),1,2).unsqueeze(3).to(args.device)\n",
    "        out_frames_reconstruct = torch.zeros_like(in_frames_tot)\n",
    "\n",
    "        for i in np.arange(3):\n",
    "            in_framesi = in_frames_tot[:,:,i,:,:].unsqueeze(1)\n",
    "            feats1u, feats2u, feats4u_f, feats5_f, x4_sz = self.U1.E1(in_framesi)\n",
    "            \n",
    "            feats1u_tot[:,i,:,:,:] = feats1u\n",
    "            feats2u_tot[:,i,:,:,:] = feats2u\n",
    "            feats4u_tot[:,i,:] = feats4u_f\n",
    "            feats5u_tot[:,i,:] = feats5_f\n",
    "        \n",
    "        feats1u_tot[:,0,:,:,:] = feats1u_tot[:,0,:,:,:] + self.CtA12_1(feats1u_tot[:,0,:,:,:],torch.zeros_like(feats1u_tot[:,0,:,:,:]))\n",
    "        feats2u_tot[:,0,:,:,:] = feats2u_tot[:,0,:,:,:] + self.CtA12_2(feats2u_tot[:,0,:,:,:],torch.zeros_like(feats2u_tot[:,0,:,:,:]))\n",
    "        feats1u_tot[:,0,:,:,:] = feats1u_tot[:,0,:,:,:] + self.CtA13_1(feats1u_tot[:,0,:,:,:],torch.zeros_like(feats1u_tot[:,0,:,:,:]))\n",
    "        feats2u_tot[:,0,:,:,:] = feats2u_tot[:,0,:,:,:] + self.CtA13_2(feats2u_tot[:,0,:,:,:],torch.zeros_like(feats2u_tot[:,0,:,:,:]))\n",
    "    \n",
    "        feats1u_tot[:,1,:,:,:] = feats1u_tot[:,1,:,:,:] + self.CtA12_1(feats1u_tot[:,1,:,:,:],feats1u_tot[:,0,:,:,:])\n",
    "        feats2u_tot[:,1,:,:,:] = feats2u_tot[:,1,:,:,:] + self.CtA12_2(feats2u_tot[:,1,:,:,:],feats2u_tot[:,0,:,:,:])\n",
    "        feats1u_tot[:,1,:,:,:] = feats1u_tot[:,1,:,:,:] + self.CtA13_1(feats1u_tot[:,1,:,:,:],torch.zeros_like(feats1u_tot[:,1,:,:,:]))\n",
    "        feats2u_tot[:,1,:,:,:] = feats2u_tot[:,1,:,:,:] + self.CtA13_2(feats2u_tot[:,1,:,:,:],torch.zeros_like(feats2u_tot[:,1,:,:,:]))\n",
    "        \n",
    "        feats1u_tot[:,2,:,:,:] = feats1u_tot[:,2,:,:,:] + self.CtA12_1(feats1u_tot[:,2,:,:,:],feats1u_tot[:,1,:,:,:])\n",
    "        feats2u_tot[:,2,:,:,:] = feats2u_tot[:,2,:,:,:] + self.CtA12_2(feats2u_tot[:,2,:,:,:],feats2u_tot[:,1,:,:,:])\n",
    "        feats1u_tot[:,2,:,:,:] = feats1u_tot[:,2,:,:,:] + self.CtA13_1(feats1u_tot[:,2,:,:,:],feats1u_tot[:,0,:,:,:])\n",
    "        feats2u_tot[:,2,:,:,:] = feats2u_tot[:,2,:,:,:] + self.CtA13_2(feats2u_tot[:,2,:,:,:],feats2u_tot[:,0,:,:,:])\n",
    "        \n",
    "        feats1u_tot = feats1u_tot + self.SWA1(feats1u_tot)\n",
    "        feats2u_tot = feats2u_tot + self.SWA2(feats2u_tot)\n",
    "        \n",
    "        for i in np.arange(3):\n",
    "            \n",
    "            feats1u = feats1u_tot[:,i,:,:,:]\n",
    "            feats2u = feats2u_tot[:,i,:,:,:]\n",
    "            feats4u_f = feats4u_tot[:,i,:]\n",
    "            feats5_f = feats5u_tot[:,i,:]\n",
    "            \n",
    "            feats1u_p, feats2u_p, feats4u_f_p, feats5_f_p = self.F1(feats1u, feats2u, feats4u_f, feats5_f)\n",
    "\n",
    "            feats1u_tot_p[:,i,:,:,:] = feats1u_p\n",
    "            feats2u_tot_p[:,i,:,:,:] = feats2u_p\n",
    "            feats4u_tot_p[:,i,:] = feats4u_f_p\n",
    "            feats5u_tot_p[:,i,:] = feats5_f_p\n",
    "            \n",
    "#             print(out_frames_reconstruct.size())\n",
    "            estimates, _, _, _, _ = self.U1.D1(feats1u, feats2u, feats4u_f, feats5_f, x4_sz)\n",
    "#             print(estimates.size())\n",
    "            out_frames_reconstruct[:,:,i,:,:] = estimates.squeeze(1)\n",
    "            \n",
    "            if i < 3:\n",
    "                out_frames_tot[:,i,:,:,:], feats1n_tot[:,i,:,:,:], feats2n_tot[:,i,:,:,:], feats4n_tot[:,i,:], feats5n_tot[:,i,:] = self.U1.D1(feats1u_p, feats2u_p, feats4u_f_p, feats5_f_p, x4_sz)\n",
    "        \n",
    "        return out_frames_tot, out_frames_reconstruct, feats1n_tot, feats2n_tot, feats4n_tot, feats5n_tot, out_frames1[:,1:-1,:,:,:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_Baseline(nn.Module):\n",
    "    \"\"\"\n",
    "    A fair comparison baseline model using LSTMs for temporal modeling.\n",
    "    \n",
    "    This model utilizes the identical U-Net Encoder (U1.E1) and Decoder (U1.D1)\n",
    "    as the RM_net3 model, replacing the SWA, CtA, and DynNet components\n",
    "    with standard LSTM layers for recurrent temporal processing. This isolates\n",
    "    the performance difference to the choice of temporal aggregation (LSTM vs. Attention/SWA).\n",
    "    \n",
    "    Note: To maintain a parameter count comparable to the attention-based model, \n",
    "    we apply LSTMs to the highly-compressed feature vectors (feats4u_f and feats5_f) \n",
    "    at the U-Net bottleneck, as is standard practice for efficiency.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, U1):\n",
    "        super(CNN_LSTM_Baseline, self).__init__()\n",
    "        \n",
    "        # 1. Share the U-Net Encoder and Decoder from the main model\n",
    "        self.U1 = U1 # U1 must contain E1 (Encoder) and D1 (Decoder)\n",
    "        self.args = args\n",
    "\n",
    "        # 2. LSTM Modules for Temporal Processing\n",
    "        # We replace SWA/CtA/DynNet with standard LSTMs for time-series aggregation.\n",
    "        # LSTM input size matches the feature dimension. \n",
    "        # Hidden size is set equal to input size for parameter efficiency.\n",
    "        \n",
    "        # LSTM for the deepest feature vector (feats5u_tot, dim=16)\n",
    "        # Using 2 layers for comparable complexity to a deep attention stack\n",
    "        self.lstm5 = nn.LSTM(\n",
    "            input_size=16, \n",
    "            hidden_size=16, \n",
    "            num_layers=2, \n",
    "            batch_first=True\n",
    "        ).to(args.device)\n",
    "\n",
    "        # LSTM for the bottleneck feature vector (feats4u_tot, dim=64)\n",
    "        self.lstm4 = nn.LSTM(\n",
    "            input_size=64, \n",
    "            hidden_size=64, \n",
    "            num_layers=2, \n",
    "            batch_first=True\n",
    "        ).to(args.device)\n",
    "        \n",
    "        # Since this model is sequential, we need linear layers to map the LSTM \n",
    "        # output (final hidden state) back to the prediction space, simulating the \n",
    "        # DynNet's role without its complexity.\n",
    "        self.linear_pred5 = nn.Linear(16, 16).to(args.device)\n",
    "        self.linear_pred4 = nn.Linear(64, 64).to(args.device)\n",
    "\n",
    "\n",
    "    def forward(self, in_frames_tot):\n",
    "        \n",
    "        B, C, T, H, W = in_frames_tot.size() # Bx3x3x256x768 (example)\n",
    "        \n",
    "        # Tensors to store extracted features from the shared encoder\n",
    "        # These are structured identically to RM_net3\n",
    "        feats1u_tot = torch.zeros(B, T, 128, 16, 48).to(self.args.device)\n",
    "        feats2u_tot = torch.zeros(B, T, 64, 4, 12).to(self.args.device)\n",
    "        feats4u_tot = torch.zeros(B, T, 64).to(self.args.device)\n",
    "        feats5u_tot = torch.zeros(B, T, 16).to(self.args.device)\n",
    "\n",
    "        # 1. Feature Extraction (Shared Encoder)\n",
    "        # Loop through the T=3 input frames to extract features sequentially\n",
    "        for i in np.arange(T):\n",
    "            in_framesi = in_frames_tot[:,:,i,:,:].unsqueeze(1)\n",
    "            # U1.E1 is the shared Unet_Enc module\n",
    "            feats1u, feats2u, feats4u_f, feats5_f, x4_sz = self.U1.E1(in_framesi)\n",
    "            \n",
    "            # Store the extracted features (no CtA or SWA applied yet)\n",
    "            feats1u_tot[:,i,:,:,:] = feats1u\n",
    "            feats2u_tot[:,i,:,:,:] = feats2u\n",
    "            feats4u_tot[:,i,:] = feats4u_f\n",
    "            feats5u_tot[:,i,:] = feats5_f\n",
    "\n",
    "        # 2. Temporal Modeling (LSTM Aggregation)\n",
    "        # We use the final extracted features as the time sequence input\n",
    "        \n",
    "        # LSTM for deepest features (feats5u_tot: B x T x 16)\n",
    "        lstm_out5, (h_n5, c_n5) = self.lstm5(feats5u_tot)\n",
    "        # The predicted feature is the output from the final time step\n",
    "        feats5u_p = self.linear_pred5(lstm_out5[:, -1, :])\n",
    "\n",
    "        # LSTM for bottleneck features (feats4u_tot: B x T x 64)\n",
    "        lstm_out4, (h_n4, c_n4) = self.lstm4(feats4u_tot)\n",
    "        feats4u_p = self.linear_pred4(lstm_out4[:, -1, :])\n",
    "        \n",
    "        # For the high-resolution features (feats1u and feats2u), \n",
    "        # we will simply use the features from the LAST INPUT frame (t=2) \n",
    "        # as the state estimate for the decoder, as LSTMs are not typically used \n",
    "        # here without massive parameter cost (ConvLSTMs).\n",
    "        feats1u_p = feats1u_tot[:, -1, :, :, :]\n",
    "        feats2u_p = feats2u_tot[:, -1, :, :, :]\n",
    "        \n",
    "        # We use the last available x4_sz for the decoder input size\n",
    "        x4_sz = feats1u.size() \n",
    "\n",
    "\n",
    "        # 3. Prediction (Shared Decoder)\n",
    "        # The baseline only predicts the NEXT frame (t=3), unlike RM_net3 \n",
    "        # which reconstructs all inputs and predicts the next frame.\n",
    "        \n",
    "        # We use the shared decoder D1 with the predicted features\n",
    "        out_frame_next, _, _, _, _ = self.U1.D1(\n",
    "            feats1u_p, \n",
    "            feats2u_p, \n",
    "            feats4u_p, \n",
    "            feats5u_p, \n",
    "            x4_sz\n",
    "        )\n",
    "        \n",
    "        # To match the output structure of RM_net3 (B x T+1 x C x H x W), we \n",
    "        # generate placeholders for the features and the first T frames.\n",
    "        \n",
    "        # Placeholders for the reconstructed input frames (used only for loss/visualization in RM_net3)\n",
    "        # We just use the last input frame as the \"reconstructed\" T=2 frame\n",
    "        out_frames_tot = torch.zeros(B, T + 1, *out_frame_next.size()[1:]).to(self.args.device)\n",
    "        out_frames_tot[:, T, :, :, :] = out_frame_next.squeeze(1) # Predicted next frame is at index T=3\n",
    "\n",
    "        # Placeholders for feature output sequences\n",
    "        feats1n_tot = torch.zeros(B, T + 1, 128, 16, 48).to(self.args.device)\n",
    "        feats2n_tot = torch.zeros(B, T + 1, 64, 4, 12).to(self.args.device)\n",
    "        feats4n_tot = torch.zeros(B, T + 1, 64).to(self.args.device)\n",
    "        feats5n_tot = torch.zeros(B, T + 1, 16).to(self.args.device)\n",
    "        \n",
    "        # Return the output in a format that mirrors RM_net3\n",
    "        return out_frames_tot, out_frames_reconstruct, feats1n_tot, feats2n_tot, feats4n_tot, feats5n_tot, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RM_net2(args)\n",
    "model = RM_net3(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # from pathlib import Path\n",
    "\n",
    "resume_AE_ckpt = Path('C:/Users/Pracioppo/Desktop/Dynamics/Models')\n",
    "\n",
    "# F_Enc_path_load = resume_AE_ckpt.joinpath('U_F_Enc_c16d4_ckpt1')\n",
    "# F_Dec_path_load = resume_AE_ckpt.joinpath('U_F_Dec_c16d4_ckpt1')\n",
    "# Enc_path_load = resume_AE_ckpt.joinpath('U_Enc_c16d4_ckpt2')\n",
    "# Dec_path_load = resume_AE_ckpt.joinpath('U_Dec_c16d4_ckpt2')\n",
    "\n",
    "# model.U1.E1.load_state_dict(torch.load(F_Enc_path_load))\n",
    "# model.U1.D1.load_state_dict(torch.load(F_Dec_path_load))\n",
    "# # model.U1.E1.Enc1.load_state_dict(torch.load(Enc_path_load))\n",
    "# # model.U1.D1.Dec1.load_state_dict(torch.load(Dec_path_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsc(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = torch.flatten(y_true)\n",
    "    y_pred_f = torch.flatten(y_pred)\n",
    "#     mask = 1.*((y_true_f * y_pred_f)>0)\n",
    "    mask = y_true_f * y_pred_f\n",
    "    intersection = torch.sum(mask)\n",
    "    score = (2. * intersection + smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return (1 - dsc(y_true, y_pred)) + nn.functional.binary_cross_entropy(y_true, y_pred)\n",
    "\n",
    "from model import GDL\n",
    "gdl_loss = GDL(alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_iter_FM(R1, optimizer, loss, loss2, in_frames_tot, args, train_flag = True, w_p = 1):\n",
    "\n",
    "    optimizer.zero_grad() # Zero out gradients\n",
    "\n",
    "    out_frames_tot, out_frames_reconstruct, feats1n_tot, feats2n_tot, feats4n_tot, feats5n_tot, out_frames1 = R1(in_frames_tot)\n",
    "\n",
    "    loss_i = 2*loss(out_frames_tot, in_frames_tot[1:]) # Prediction loss\n",
    "    \n",
    "    loss_i += loss(out_frames_reconstruct, in_frames_tot) # Reconstruction loss\n",
    "#     loss_i = 3*dice_loss(out_frames_toti[:,:,:,-256:], in_frames_toti[:,:,:,-256:])\n",
    "#     loss_i += dice_loss(out_frames_toti, in_frames_toti)\n",
    "    \n",
    "    loss_i.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idxs = np.arange(len(Train_Data))\n",
    "np.random.shuffle(random_idxs)\n",
    "train_data_shuffle = Train_Data[random_idxs]\n",
    "\n",
    "epoch_losses = np.zeros(args.num_batches)\n",
    "\n",
    "#     for it in tqdm(range(args.num_batches)):\n",
    "for it in tqdm(range(args.num_batches)):\n",
    "\n",
    "    in_frames_tot = train_data_shuffle[it*args.batch_size:(it+1)*args.batch_size,:,:,:,:]\n",
    "    break\n",
    "\n",
    "out_frames_tot, out_frames_reconstruct, feats1n_tot, feats2n_tot, feats4n_tot, feats5n_tot, out_frames1 = model(in_frames_tot)\n",
    "\n",
    "print(in_frames_tot.size())\n",
    "out_frames_tot.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dice_jac(mask1,mask2):\n",
    "\n",
    "    union = 1*((mask1 + mask2) > 0)\n",
    "\n",
    "    intersect = (mask1*mask2)\n",
    "\n",
    "    sum1 = np.sum(mask1) + np.sum(mask2)\n",
    "\n",
    "    dice = 2*np.sum(intersect)/sum1\n",
    "    jaccard = np.sum(intersect)/np.sum(union)\n",
    "\n",
    "    return dice, jaccard, union, np.sum(intersect), sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Tot_Dice(Data,args):\n",
    "\n",
    "#     random_idxs = np.arange(len(Data))\n",
    "#     np.random.shuffle(random_idxs)\n",
    "#     test_data_shuffle = Data[random_idxs]\n",
    "    test_data_shuffle = Data\n",
    "\n",
    "    num_batches = int(len(Data)/args.N)\n",
    "\n",
    "    tot_avg_dice = 0\n",
    "    \n",
    "    union_tot = 0\n",
    "    intersect_tot = 0\n",
    "    sum_tot = 0\n",
    "#     for it in tqdm(range(num_batches)):\n",
    "    for it in range(num_batches):\n",
    "\n",
    "        in_frames_tot = test_data_shuffle[it*args.batch_size:(it+1)*args.batch_size,:,:,:,:]\n",
    "\n",
    "        gt_frames = in_frames_tot[:,:,-1,:,:].unsqueeze(1)\n",
    "\n",
    "        past_frames = in_frames_tot[:,:,-2,:,:].unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_frames_tot, _, _, _, _, _, _ = model(in_frames_tot)\n",
    "\n",
    "    #     out_frames_tot.size()\n",
    "\n",
    "        out_frames = out_frames_tot[:,-1,:,:,:,:]\n",
    "    #     out_frames.size()\n",
    "\n",
    "        dice_tot = 0\n",
    "        \n",
    "        for i in np.arange(args.batch_size):\n",
    "            frame_p = past_frames[i].squeeze().detach().cpu().numpy()[:,256:512]\n",
    "            frame_x = gt_frames[i].squeeze().detach().cpu().numpy()[:,256:512]\n",
    "            frame_y = out_frames[i].squeeze().detach().cpu().numpy()[:,256:512]\n",
    "\n",
    "            diff1 = frame_x - frame_p\n",
    "            diff2 = frame_y - frame_p\n",
    "            mask1 = (diff1 > 0.5)\n",
    "            mask2 = (diff2 > 0.5)\n",
    "\n",
    "            dice, jaccard, union, intersect, sum1 = f_dice_jac(mask1,mask2)\n",
    "        \n",
    "            dice_tot += dice\n",
    "            intersect_tot += intersect\n",
    "            union_tot += union\n",
    "            sum_tot += sum1\n",
    "            \n",
    "        avg_dice = dice_tot/args.batch_size\n",
    "        tot_avg_dice += avg_dice\n",
    "\n",
    "    tot_avg_dice /= num_batches\n",
    "    tot_avg_dice2 = 2*intersect_tot/sum_tot\n",
    "\n",
    "    return tot_avg_dice, tot_avg_dice2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Test_Data\n",
    "\n",
    "avg_dice, avg_dice2 = f_Tot_Dice(Data,args)\n",
    "print(avg_dice)\n",
    "print(avg_dice2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Data.size()\n",
    "# Test_Data.size()\n",
    "args.num_batches = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################Training loop (for full model) ###########################\n",
    "\n",
    "Data = Train_Data\n",
    "args.training_mode = 1\n",
    "args.num_batches = int(len(Data)/args.N)\n",
    "\n",
    "RM_path_save = resume_AE_ckpt.joinpath('RM3_ckpt_1')\n",
    "\n",
    "args.num_epochs = 500\n",
    "args.show_example_epochs = 1\n",
    "args.save_epochs = 1\n",
    "\n",
    "log_mean_epoch_losses = np.zeros(args.num_epochs)\n",
    "\n",
    "loss = nn.BCELoss() # <-- Use this one\n",
    "loss2 = nn.L1Loss()\n",
    "\n",
    "lr = 1E-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "test_dice_v = []\n",
    "test_dice_v2 = []\n",
    "\n",
    "# Train for a maximum of max_epochs:\n",
    "for epoch in tqdm(np.arange(args.num_epochs), desc=\"Training progress...\"):\n",
    "    epoch_losses = f_single_epoch_FM(Data, model, optimizer, loss, loss2, args)\n",
    "    log_mean_epoch_losses[epoch] = np.log(np.mean(epoch_losses))\n",
    "#     coord_errors[epoch] = f_coord_error(train_data,train_data_traj,VPTR_Enc, C1, args)\n",
    "    \n",
    "    if np.mod(epoch,args.show_example_epochs) == 0:\n",
    "        f_display(Data, model, args)\n",
    "        \n",
    "        plt.plot(log_mean_epoch_losses[0:epoch])\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        test_dice, test_dice2 = f_Tot_Dice(Test_Data,args)\n",
    "        print(test_dice)\n",
    "        print(test_dice2)\n",
    "        test_dice_v.append(test_dice)\n",
    "        test_dice_v2.append(test_dice2)\n",
    "        plt.plot(test_dice_v)\n",
    "        plt.plot(test_dice_v2)\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_display(Data, model, args)\n",
    "f_display2(Data, model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RM_path_save = resume_AE_ckpt.joinpath('RM3_ckpt_1')\n",
    "torch.save(model.state_dict(), RM_path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1_path_save = resume_AE_ckpt.joinpath('RM3_ckpt1_U1')\n",
    "F1_path_save = resume_AE_ckpt.joinpath('RM3_ckpt1_F1')\n",
    "\n",
    "torch.save(model.U1.state_dict(), U1_path_save)\n",
    "torch.save(model.F1.state_dict(), F1_path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
