{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64dc314",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "\n",
    "%cd /Users/Pracioppo/Desktop/GA Forecasting\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import functools\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming these utilities are available as imported\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset \n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "from preprocessing_utils import f_rescale_dataset, f_Residuals, f_reshape_training_data, f_rotate_and_zoom, f_random_crop, f_rotate_and_zoom_all, f_crop_all, f_flip_all, f_augment_dataset2\n",
    "\n",
    "from data_utils import DataWrapper, visualize_sample, compare_split_masks\n",
    "\n",
    "from visualization import f_display_autoencoder, plot_log_loss, f_display_frames\n",
    "\n",
    "from models import init_weights, count_parameters\n",
    "from models import rotate_half, RotaryPositionalEmbedding, RoPEMultiheadAttention, RoPETransformerEncoderLayer, ResidualBlock, ChannelReducer, Unet_Enc, Unet_Dec, U_Net_AE\n",
    "\n",
    "from augmentation_utils import f_augment_spatial_and_intensity\n",
    "from training_utils import dsc, dice_loss, GDLoss\n",
    "from training_utils import f_single_epoch_AE, f_single_epoch_UPredNet, calculate_total_loss, f_single_epoch_UPredNet_Accumulated\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models')\n",
    "tensorboard_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models')\n",
    "                            \n",
    "start_epoch = 0\n",
    "\n",
    "resume_ckpt = None\n",
    "\n",
    "summary_writer = SummaryWriter(tensorboard_save_dir.absolute().as_posix())\n",
    "\n",
    "\n",
    "# --- SETUP ---\n",
    "# Define a simple placeholder for command-line arguments and configuration\n",
    "parser = argparse.ArgumentParser('AE Model Args')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# Defining essential arguments (set to match your intended lightweight AE setup)\n",
    "args.N = 4                       # CRITICAL FIX: Batch size is 4 for memory safety\n",
    "args.nhead = 4                   # CRITICAL FIX: Reduced heads from 8 to 4\n",
    "args.d_attn1 = 192               # FFN dimension for L3 (112 channels)\n",
    "args.d_attn2 = 384               # FFN dimension for L4 (224 channels)\n",
    "args.img_channels = 3            # Three grayscale images (FAF, masks, growth masks)\n",
    "args.img_sz = 256                # Image size 256x256\n",
    "args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "BASE_CHANNELS = 24               # Reduced from 28 to 24 for extra parameter savings\n",
    "\n",
    "# Training loop arguments\n",
    "args.num_epochs = 10             # UPDATED: Set to 1 epoch as requested\n",
    "args.show_example_epochs = 5\n",
    "args.batch_size = args.N        # Batch size for iteration is args.N\n",
    "args.num_t_steps = 4            # Time steps (used only for data simulation/flattening)\n",
    "\n",
    "# Initialize paths (for saving checkpoints)\n",
    "resume_AE_ckpt = Path('./ae_checkpoints')\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "args.device = torch.device('cuda:0')\n",
    "print(f\"Using {args.device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe79a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d65e2",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59159621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading and Global Preprocessing ---\n",
    "\n",
    "# WARNING: Replace this with your actual data directory\n",
    "DATA_DIR = Path('/Users/Pracioppo/Desktop/GA Forecasting/GA_Zubens_data') \n",
    "print(\"Loading data...\")\n",
    "\n",
    "FAFS_PATH = DATA_DIR / 'fafs_reg3.mat'\n",
    "MASKS_PATH = DATA_DIR / 'masks_reg3.mat'\n",
    "\n",
    "FAFs = torch.from_numpy(sio.loadmat(FAFS_PATH)[\"fafs_reg3\"].astype(np.float32)).reshape(660,1,4,256,256)\n",
    "masks = torch.from_numpy(sio.loadmat(MASKS_PATH)[\"masks_reg3\"].astype(np.float32)).reshape(660,1,4,256,256)\n",
    "\n",
    "# Global normalization\n",
    "masks /= torch.max(masks) if torch.max(masks) > 0 else 1.0\n",
    "FAFs /= torch.max(FAFs) if torch.max(FAFs) > 0 else 1.0\n",
    "\n",
    "# Residual calculation and normalization (applied globally to all 660 samples)\n",
    "all_residuals = f_Residuals(masks)\n",
    "\n",
    "# --- 3. K-Fold Split using Subsets ---\n",
    "\n",
    "# Instantiate the Dataset with all pre-processed feature tensors\n",
    "all_data = DataWrapper(FAFs, masks, all_residuals)\n",
    "print(f\"\\nTotal Dataset size (samples): {len(all_data)}\")\n",
    "\n",
    "# K-FOLD GROUP SETUP (To avoid data leakage from augmented samples)\n",
    "SAMPLES_PER_GROUP = 10\n",
    "N_SAMPLES = len(all_data) # N_SAMPLES = 660\n",
    "N_GROUPS = N_SAMPLES // SAMPLES_PER_GROUP # N_GROUPS = 66\n",
    "K_FOLDS = 5 \n",
    "\n",
    "# Calculate fold sizes based on groups (66 groups in 5 folds: 4 folds of 13 groups, 1 fold of 14 groups)\n",
    "group_fold_size_base = N_GROUPS // K_FOLDS # 13\n",
    "group_fold_remainder = N_GROUPS % K_FOLDS # 1\n",
    "\n",
    "k = 0 # The current fold index (fixed at 0 for this script)\n",
    "\n",
    "# Calculate group indices for the current fold (k=0 gets the larger remainder group)\n",
    "G_all_indices = np.arange(N_GROUPS)\n",
    "\n",
    "# Determine the size of the first 'remainder' folds\n",
    "# For k=0, this will be 13 + 1 = 14 groups\n",
    "current_group_fold_size = group_fold_size_base + (1 if k < group_fold_remainder else 0) \n",
    "\n",
    "G_start_idx = k * group_fold_size_base + min(k, group_fold_remainder) # 0\n",
    "G_end_idx = G_start_idx + current_group_fold_size # 14\n",
    "\n",
    "G_test_indices = G_all_indices[G_start_idx:G_end_idx]\n",
    "G_train_indices = np.concatenate([G_all_indices[:G_start_idx], G_all_indices[G_end_idx:]])\n",
    "\n",
    "# Map Group indices back to Sample indices (0-659)\n",
    "def map_group_to_sample_indices(group_indices, samples_per_group):\n",
    "    # For each group index i, generate indices [i*10, i*10 + 1, ..., i*10 + 9]\n",
    "    # Using np.concatenate for efficiency over Python loops for large arrays\n",
    "    sample_indices = np.concatenate([\n",
    "        np.arange(g * samples_per_group, (g + 1) * samples_per_group)\n",
    "        for g in group_indices\n",
    "    ])\n",
    "    return sample_indices\n",
    "\n",
    "train_indices = map_group_to_sample_indices(G_train_indices, SAMPLES_PER_GROUP)\n",
    "test_indices = map_group_to_sample_indices(G_test_indices, SAMPLES_PER_GROUP)\n",
    "\n",
    "sz = len(test_indices) # Should be 140 for k=0\n",
    "\n",
    "# Create Subsets for training and testing\n",
    "train_dataset = Subset(all_data, train_indices)\n",
    "test_dataset = Subset(all_data, test_indices)\n",
    "\n",
    "print(\"--- K-Fold Split Result (Group-Aware) ---\")\n",
    "print(f\"Total Groups: {N_GROUPS}, Samples/Group: {SAMPLES_PER_GROUP}\")\n",
    "print(f\"Fold {k} Groups: {len(G_test_indices)} (Fold Size: {sz})\")\n",
    "print(f\"Train Dataset Samples: {len(train_dataset)} (Expected 520)\")\n",
    "print(f\"Test Dataset Samples: {len(test_dataset)} (Expected 140)\")\n",
    "\n",
    "# --- 4. DATA ASSEMBLY FOR MANUAL ITERATION ---\n",
    "\n",
    "# We use a DataLoader only to efficiently stack the training Subset items into one tensor.\n",
    "N_TRAIN_SAMPLES = len(train_dataset)\n",
    "\n",
    "temp_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=N_TRAIN_SAMPLES,\n",
    "    shuffle=False, # Must be False for sequential index fetching\n",
    "    num_workers=0 \n",
    ")\n",
    "\n",
    "# Fetch the entire training dataset into a single CPU tensor\n",
    "# Shape: [N_TRAIN_SAMPLES, C, T, H, W]\n",
    "for batch in temp_loader:\n",
    "    full_clean_data_tensor_cpu = batch \n",
    "    break\n",
    "\n",
    "print(f\"\\nASSEMBLED TENSOR: full_clean_data_tensor_cpu size: {full_clean_data_tensor_cpu.size()}\")\n",
    "\n",
    "# --- DataLoader Setup (Only for the Test/Validation Set) ---\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=args.N, # Use args.N (batch_size for GPU)\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# --- UPDATED VISUALIZATION EXAMPLE (Side-by-Side) ---\n",
    "\n",
    "# Example function calls\n",
    "visualize_sample(train_dataset, test_dataset, sample_idx=20, dataset_name='test')\n",
    "# visualize_sample(sample_idx=500, dataset_name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(52):\n",
    "#     visualize_sample(sample_idx=idx*10, dataset_name='train')\n",
    "    \n",
    "# for idx in range(14):\n",
    "#     visualize_sample(sample_idx=idx*10, dataset_name='test')\n",
    "\n",
    "# Run the full visual split comparison\n",
    "compare_split_masks(train_dataset, test_dataset, channel_type='mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8577d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_split_masks(train_dataset, test_dataset, channel_type='residual', time_step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf617f40",
   "metadata": {},
   "source": [
    "## Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TRAINING EXECUTION ---\n",
    "\n",
    "# Create necessary path for checkpointing\n",
    "resume_AE_ckpt.mkdir(exist_ok=True)\n",
    "print(f\"\\nCheckpoints will be saved to: {resume_AE_ckpt.resolve()}\")\n",
    "\n",
    "# Model Initialization\n",
    "AE_model = U_Net_AE(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "\n",
    "total_params = count_parameters(AE_model)\n",
    "encoder_params = count_parameters(AE_model.E1)\n",
    "decoder_params = count_parameters(AE_model.D1)\n",
    "\n",
    "# Create a table for clean output\n",
    "param_data = [\n",
    "    [\"Total AE Model (U_Net_AE)\", f\"{total_params:,}\"],\n",
    "    [\"Encoder (E1)\", f\"{encoder_params:,}\"],\n",
    "    [\"Decoder (D1)\", f\"{decoder_params:,}\"],\n",
    "]\n",
    "\n",
    "print(tabulate(param_data, headers=[\"Component\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "print(f\"\\nModel Initialized with a total of {total_params:,} trainable parameters.\")\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "# Training setup\n",
    "log_mean_epoch_losses = np.zeros(args.num_epochs)\n",
    "\n",
    "# Loss functions \n",
    "loss_fn_bce = nn.BCELoss(reduction='mean') # FAF BCE Loss\n",
    "loss_fn_l1 = nn.L1Loss(reduction='mean') # Used for LLR-Loss\n",
    "loss_fn_l2 = nn.MSELoss(reduction='mean') # Used for Bottleneck Regularization\n",
    "loss_fn_dice = dice_loss # Custom Dice Loss (includes BCE)\n",
    "loss_fn_gdl = GDLoss(alpha=1, beta=1) # Instantiating GDL with defaults\n",
    "\n",
    "lr = 1E-3\n",
    "optimizer = torch.optim.Adam(AE_model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "print(f\"\\nStarting training on device: {args.device} for {args.num_epochs} epochs using real data...\")\n",
    "\n",
    "LLR_WEIGHT = 1e-5 # L1 Penalty to encourage sparsity\n",
    "BOTTLENECK_L2_WEIGHT = 1e-6 # Bottleneck L2 weight\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(np.arange(args.num_epochs), desc=\"Training progress...\"):\n",
    "    # CORRECTION: Replaced 'train_loader' with 'full_clean_data_tensor_cpu' \n",
    "    # and added 'args.batch_size' as the required argument.\n",
    "    epoch_losses = f_single_epoch_AE(\n",
    "        full_clean_data_tensor_cpu, AE_model, optimizer, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size,\n",
    "        lambda_gdl=1e-3, lambda_residual=5.0, lambda_llr=LLR_WEIGHT, lambda_bottleneck=BOTTLENECK_L2_WEIGHT\n",
    "    )\n",
    "    \n",
    "    # Calculate and log the mean epoch loss\n",
    "    mean_epoch_loss = np.mean(epoch_losses)\n",
    "    if mean_epoch_loss > 0 and not np.isnan(mean_epoch_loss) and np.isfinite(mean_epoch_loss):\n",
    "        log_mean_epoch_losses[epoch] = np.log(mean_epoch_loss)\n",
    "    else:\n",
    "        log_mean_epoch_losses[epoch] = -100 \n",
    "\n",
    "    # Visualization and Plotting (at set intervals)\n",
    "    if np.mod(epoch, args.show_example_epochs) == 0:\n",
    "        print(f\"\\n--- Epoch {epoch}: Mean Loss (log): {log_mean_epoch_losses[epoch]:.4f} ---\")\n",
    "        # Visualizing the first sample (index 0) at the first time step (time_step 0).\n",
    "        for j in range(4):\n",
    "            f_display_autoencoder(train_dataset, AE_model, args, time_step=j)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(log_mean_epoch_losses[0:epoch+1][log_mean_epoch_losses[0:epoch+1] > -99]) \n",
    "        plt.title('Log Mean Epoch Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Log Loss')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\nTraining complete. Saving final checkpoints...\")\n",
    "\n",
    "# Final Checkpoint Saving\n",
    "F_Enc_path_save = resume_AE_ckpt.joinpath('U_F_Enc_real_data_ckpt1.pth')\n",
    "F_Dec_path_save = resume_AE_ckpt.joinpath('U_F_Dec_real_data_ckpt1.pth')\n",
    "\n",
    "# Save the full Unet_Enc and Unet_Dec modules\n",
    "torch.save(AE_model.E1.state_dict(), F_Enc_path_save)\n",
    "torch.save(AE_model.D1.state_dict(), F_Dec_path_save)\n",
    "\n",
    "print(f\"Checkpoints saved: {F_Enc_path_save.name} and {F_Dec_path_save.name}\")\n",
    "print(\"Script finished execution.\")\n",
    "\n",
    "for j in range(4):\n",
    "    f_display_autoencoder(train_dataset, AE_model, args, time_step=j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff209e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
