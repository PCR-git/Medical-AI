{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64dc314",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "\n",
    "%cd /Users/Pracioppo/Desktop/GA Forecasting\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import functools\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset \n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "from preprocessing_utils import f_rescale_dataset, f_Residuals, f_reshape_training_data, f_rotate_and_zoom, f_random_crop, f_rotate_and_zoom_all, f_crop_all, f_flip_all, f_augment_dataset2\n",
    "\n",
    "from data_utils import DataWrapper, visualize_sample, compare_split_masks\n",
    "\n",
    "from visualization import f_display_autoencoder, plot_log_loss, f_display_frames\n",
    "\n",
    "from models import init_weights, count_parameters\n",
    "from models import rotate_half, RotaryPositionalEmbedding, RoPEMultiheadAttention, RoPETransformerEncoderLayer, ResidualBlock, ChannelReducer, Unet_Enc, Unet_Dec, U_Net_AE\n",
    "\n",
    "from augmentation_utils import f_augment_spatial_and_intensity\n",
    "from training_utils import dsc, dice_loss, GDLoss\n",
    "from training_utils import freeze_batch_norm, f_single_epoch_AE, f_single_epoch_spatiotemporal, calculate_total_loss, f_single_epoch_spatiotemporal_accumulated\n",
    "from training_utils import save_model_weights, load_model\n",
    "from training_utils import save_final_experiment_data\n",
    "from eval_utils import f_eval_pred_dice_test_set, f_eval_pred_dice_train_set, plot_train_test_dice_history, soft_dice_score, f_get_individual_dice, f_plot_individual_dice\n",
    "from eval_utils import analyze_kfold_results, corrected_paired_ttest_nadeau, paired_ttest_student, compare_models_nadeau_bengio, compare_models\n",
    "\n",
    "# from models import TemporalDeltaBlock\n",
    "from models import DynNet, CausalConvAggregator, UPredNet\n",
    "from models import FusionBlockBottleneck, ChannelFusionBlock, LocalSpatioTemporalMixer, SpatioTemporalGatedMixer, AxialTemporalSWAInterleavedLayer, InterleavedAxialTemporalSWAIntegrator, FullGlobalSWAIntegrator, SlidingWindowAttention, SWAU_Net, SWAU_CFB_Ablation, SWAU_DynNet_Ablation\n",
    "from models import ConvLSTMCell, ConvLSTMCore, ConvLSTMBaseline, ConvLSTM_Simple\n",
    "from models import create_causal_mask, create_block_causal_mask, RKA_MultiheadAttention, AxialTemporalRKAInterleavedLayer, plot_attention_matrix\n",
    "from models import AxialMultiheadAttention, StandardAxialInterleavedLayer, StandardAxialIntegrator, AxialU_Net\n",
    "from models import CNN_Unet_Enc, CNN_Unet_Dec, CNN_U_Net_AE, CNN_DynNet, SWAU_Net_CNN\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models')\n",
    "tensorboard_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models')\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "resume_ckpt = None\n",
    "\n",
    "summary_writer = SummaryWriter(tensorboard_save_dir.absolute().as_posix())\n",
    "\n",
    "\n",
    "# --- SETUP ---\n",
    "# Define a simple placeholder for command-line arguments and configuration\n",
    "parser = argparse.ArgumentParser('AE Model Args')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# Defining essential arguments (set to match your intended lightweight AE setup)\n",
    "args.N = 4                       # CRITICAL FIX: Batch size is 4 for memory safety\n",
    "args.nhead = 4                   # CRITICAL FIX: Reduced heads from 8 to 4\n",
    "args.d_attn1 = 192               # FFN dimension for L3 (112 channels)\n",
    "args.d_attn2 = 384               # FFN dimension for L4 (224 channels)\n",
    "args.img_channels = 3            # Three grayscale images (FAF, masks, growth masks)\n",
    "args.img_sz = 256                # Image size 256x256\n",
    "args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "BASE_CHANNELS = 24               # Reduced from 28 to 24 for extra parameter savings\n",
    "\n",
    "# Training loop arguments\n",
    "args.num_epochs = 1             # UPDATED: Set to 1 epoch as requested\n",
    "args.show_example_epochs = 5\n",
    "args.batch_size = args.N        # Batch size for iteration is args.N\n",
    "args.num_t_steps = 4            # Time steps (used only for data simulation/flattening)\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "args.device = torch.device('cuda:0')\n",
    "print(f\"Using {args.device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe79a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d65e2",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3bec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading and Global Preprocessing ---\n",
    "\n",
    "# WARNING: Replace this with your actual data directory\n",
    "DATA_DIR = Path('/Users/Pracioppo/Desktop/GA Forecasting/GA_Zubens_data') \n",
    "print(\"Loading data...\")\n",
    "\n",
    "FAFS_PATH = DATA_DIR / 'fafs_reg3.mat'\n",
    "MASKS_PATH = DATA_DIR / 'masks_reg3.mat'\n",
    "\n",
    "FAFs = torch.from_numpy(sio.loadmat(FAFS_PATH)[\"fafs_reg3\"].astype(np.float32)).reshape(660,1,4,256,256)\n",
    "masks = torch.from_numpy(sio.loadmat(MASKS_PATH)[\"masks_reg3\"].astype(np.float32)).reshape(660,1,4,256,256)\n",
    "\n",
    "# Global normalization\n",
    "masks /= torch.max(masks) if torch.max(masks) > 0 else 1.0\n",
    "FAFs /= torch.max(FAFs) if torch.max(FAFs) > 0 else 1.0\n",
    "\n",
    "# Residual calculation and normalization (applied globally to all 660 samples)\n",
    "all_residuals = f_Residuals(masks)\n",
    "\n",
    "# --- 3. K-Fold Split using Subsets ---\n",
    "\n",
    "# Instantiate the Dataset with all pre-processed feature tensors\n",
    "all_data = DataWrapper(FAFs, masks, all_residuals)\n",
    "print(f\"\\nTotal Dataset size (samples): {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59159621",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITH DATA AUGMENTATIONS\n",
    "\n",
    "# K-FOLD GROUP SETUP (To avoid data leakage from augmented samples)\n",
    "SAMPLES_PER_GROUP = 10\n",
    "N_SAMPLES = len(all_data) # N_SAMPLES = 660\n",
    "N_GROUPS = N_SAMPLES // SAMPLES_PER_GROUP # N_GROUPS = 66\n",
    "K_FOLDS = 5 \n",
    "\n",
    "# Calculate fold sizes based on groups (66 groups in 5 folds: 4 folds of 13 groups, 1 fold of 14 groups)\n",
    "group_fold_size_base = N_GROUPS // K_FOLDS # 13\n",
    "group_fold_remainder = N_GROUPS % K_FOLDS # 1\n",
    "\n",
    "k = 4 # The current fold index \n",
    "\n",
    "# Calculate group indices for the current fold (k=0 gets the larger remainder group)\n",
    "G_all_indices = np.arange(N_GROUPS)\n",
    "\n",
    "# Determine the size of the first 'remainder' folds\n",
    "# For k=0, this will be 13 + 1 = 14 groups\n",
    "current_group_fold_size = group_fold_size_base + (1 if k < group_fold_remainder else 0) \n",
    "\n",
    "G_start_idx = k * group_fold_size_base + min(k, group_fold_remainder) # 0\n",
    "G_end_idx = G_start_idx + current_group_fold_size # 14\n",
    "\n",
    "G_test_indices = G_all_indices[G_start_idx:G_end_idx]\n",
    "G_train_indices = np.concatenate([G_all_indices[:G_start_idx], G_all_indices[G_end_idx:]])\n",
    "\n",
    "# Map Group indices back to Sample indices (0-659)\n",
    "def map_group_to_sample_indices(group_indices, samples_per_group):\n",
    "    # For each group index i, generate indices [i*10, i*10 + 1, ..., i*10 + 9]\n",
    "    # Using np.concatenate for efficiency over Python loops for large arrays\n",
    "    sample_indices = np.concatenate([\n",
    "        np.arange(g * samples_per_group, (g + 1) * samples_per_group)\n",
    "        for g in group_indices\n",
    "    ])\n",
    "    return sample_indices\n",
    "\n",
    "train_indices = map_group_to_sample_indices(G_train_indices, SAMPLES_PER_GROUP)\n",
    "test_indices = map_group_to_sample_indices(G_test_indices, SAMPLES_PER_GROUP)\n",
    "\n",
    "sz = len(test_indices) # Should be 140 for k=0\n",
    "\n",
    "# Create Subsets for training and testing\n",
    "train_dataset = Subset(all_data, train_indices)\n",
    "test_dataset = Subset(all_data, test_indices)\n",
    "\n",
    "print(\"--- K-Fold Split Result (Group-Aware) ---\")\n",
    "print(f\"Total Groups: {N_GROUPS}, Samples/Group: {SAMPLES_PER_GROUP}\")\n",
    "print(f\"Fold {k} Groups: {len(G_test_indices)} (Fold Size: {sz})\")\n",
    "print(f\"Train Dataset Samples: {len(train_dataset)} (Expected 520)\")\n",
    "print(f\"Test Dataset Samples: {len(test_dataset)} (Expected 140)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REMOVE ALL DATA AUGMENTATIONS\n",
    "\n",
    "# # --- Original Data Parameters (DO NOT CHANGE) ---\n",
    "# SAMPLES_PER_GROUP = 10\n",
    "# N_SAMPLES = 660 # Total dataset size\n",
    "# N_GROUPS = 66   # Total number of unique patients/original images\n",
    "# K_FOLDS = 5\n",
    "\n",
    "# # Instantiate the Dataset with all pre-processed feature tensors (660 samples)\n",
    "# # all_data = DataWrapper(FAFs, masks, all_residuals) # Keep your original DataWrapper setup\n",
    "\n",
    "# # --- Identify and Map to Original 66 Samples ---\n",
    "# # The original 66 images are the first sample in each group of 10.\n",
    "# # Indices: 0, 10, 20, ..., 650\n",
    "# original_sample_indices = np.arange(0, N_SAMPLES, SAMPLES_PER_GROUP)\n",
    "# # len(original_sample_indices) is 66\n",
    "\n",
    "# # --- K-Fold Group Setup (Preserving the Original Logic) ---\n",
    "# k = 4 # The current fold index (Example: k=2)\n",
    "\n",
    "# # Calculate fold sizes based on groups (66 groups in 5 folds: 4 folds of 13 groups, 1 fold of 14 groups)\n",
    "# group_fold_size_base = N_GROUPS // K_FOLDS # 13\n",
    "# group_fold_remainder = N_GROUPS % K_FOLDS # 1\n",
    "\n",
    "# # Calculate group indices for the current fold (k=0 gets the larger remainder group)\n",
    "# G_all_indices = np.arange(N_GROUPS) # [0, 1, ..., 65]\n",
    "\n",
    "# # Determine the size of the current fold (13 or 14 groups)\n",
    "# current_group_fold_size = group_fold_size_base + (1 if k < group_fold_remainder else 0)\n",
    "\n",
    "# # Calculate start and end index for the test group indices\n",
    "# G_start_idx = k * group_fold_size_base + min(k, group_fold_remainder)\n",
    "# G_end_idx = G_start_idx + current_group_fold_size\n",
    "\n",
    "# G_test_indices = G_all_indices[G_start_idx:G_end_idx]\n",
    "# G_train_indices = np.concatenate([G_all_indices[:G_start_idx], G_all_indices[G_end_idx:]])\n",
    "\n",
    "# # --- Map Group Indices to Sample Indices (THE MODIFICATION) ---\n",
    "\n",
    "# def map_group_to_repeated_original_sample_indices(group_indices, samples_per_group):\n",
    "#     \"\"\"\n",
    "#     Maps group indices (0-65) to sample indices (0-659) such that:\n",
    "#     1. It identifies the ORIGINAL sample index (g * SAMPLES_PER_GROUP).\n",
    "#     2. It creates 'samples_per_group' copies of that SINGLE index.\n",
    "    \n",
    "#     This ensures that each group (10 samples) in the final dataset\n",
    "#     is a repetition of the single original image corresponding to that group.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Identify the ORIGINAL sample index for each group\n",
    "#     original_indices = group_indices * samples_per_group\n",
    "    \n",
    "#     # Repeat each original index 'samples_per_group' times\n",
    "#     # e.g., [0, 10] -> [0,0,0,0,0,0,0,0,0,0, 10,10,10,10,10,10,10,10,10,10]\n",
    "#     repeated_indices = np.repeat(original_indices, samples_per_group)\n",
    "    \n",
    "#     return repeated_indices\n",
    "\n",
    "# # Create the new training and testing indices\n",
    "# # These indices *now refer to the original samples, repeated 10 times*\n",
    "# train_indices_orig_repeated = map_group_to_repeated_original_sample_indices(G_train_indices, SAMPLES_PER_GROUP)\n",
    "# test_indices_orig_repeated = map_group_to_repeated_original_sample_indices(G_test_indices, SAMPLES_PER_GROUP)\n",
    "\n",
    "# sz_test = len(test_indices_orig_repeated) # Will still be 140 for k=2\n",
    "# sz_train = len(train_indices_orig_repeated) # Will still be 520\n",
    "\n",
    "# # Create Subsets for training and testing using the new indices\n",
    "# # NOTE: The Subset needs to be applied to the original 'all_data' which holds the 660 features\n",
    "# train_dataset = Subset(all_data, train_indices_orig_repeated)\n",
    "# test_dataset = Subset(all_data, test_indices_orig_repeated)\n",
    "\n",
    "# # --- K-Fold Split Result (Original-Sample-Aware) ---\n",
    "# print(\"\\n--- K-Fold Split Result (Original-Sample-Aware) ---\")\n",
    "# print(f\"Current Fold k: {k}\")\n",
    "# print(f\"Test Group Indices: {G_test_indices}\")\n",
    "# print(f\"Test Samples: {sz_test}\")\n",
    "# print(f\"Train Dataset Samples: {sz_train} (Expected {N_SAMPLES - sz_test})\") # Corrected Expected value\n",
    "# print(f\"Test Dataset Samples: {sz_test} (Expected {current_group_fold_size * SAMPLES_PER_GROUP})\") # Corrected Expected value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf57e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATA ASSEMBLY FOR MANUAL ITERATION ---\n",
    "\n",
    "# We use a DataLoader only to efficiently stack the training Subset items into one tensor.\n",
    "N_TRAIN_SAMPLES = len(train_dataset)\n",
    "\n",
    "temp_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=N_TRAIN_SAMPLES,\n",
    "    shuffle=False, # Must be False for sequential index fetching\n",
    "    num_workers=0 \n",
    ")\n",
    "\n",
    "# Fetch the entire training dataset into a single CPU tensor\n",
    "# Shape: [N_TRAIN_SAMPLES, C, T, H, W]\n",
    "for batch in temp_loader:\n",
    "    full_clean_data_tensor_cpu = batch \n",
    "    break\n",
    "\n",
    "print(f\"\\nASSEMBLED TENSOR: full_clean_data_tensor_cpu size: {full_clean_data_tensor_cpu.size()}\")\n",
    "\n",
    "# --- DataLoader Setup (Only for the Test/Validation Set) ---\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=args.N, # Use args.N (batch_size for GPU)\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# --- UPDATED VISUALIZATION EXAMPLE (Side-by-Side) ---\n",
    "\n",
    "# Example function calls\n",
    "visualize_sample(train_dataset, test_dataset, sample_idx=20, dataset_name='test')\n",
    "# visualize_sample(sample_idx=500, dataset_name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(52):\n",
    "#     visualize_sample(sample_idx=idx*10, dataset_name='train')\n",
    "    \n",
    "# for idx in range(14):\n",
    "#     visualize_sample(sample_idx=idx*10, dataset_name='test')\n",
    "\n",
    "# Run the full visual split comparison\n",
    "compare_split_masks(train_dataset, test_dataset, channel_type='mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8577d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_split_masks(train_dataset, test_dataset, channel_type='residual', time_step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646b581",
   "metadata": {},
   "source": [
    "## Sliding Window Attention U-Net (SWAU-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87196d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Reduced from 24 to 16\n",
    "# args.d_attn1 = 128 # Reduced from 192 to 128\n",
    "# args.d_attn2 = 256 # Reduced from 384 to 256\n",
    "\n",
    "# args.num_attn_layers = 2\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the full SWAU_Net model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the full model and move it to the device\n",
    "# # The model is SWAU_Net, which owns E1, CFB_enc, CFB_dec, SWA, P, and D1.\n",
    "# model = SWAU_Net(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(model.E1)\n",
    "\n",
    "# # CORRECTED: Calculate parameters for both CFB modules separately\n",
    "# cfb_enc_params = count_parameters(model.CFB_enc) \n",
    "# cfb_dec_params = count_parameters(model.CFB_dec) \n",
    "# cfb_total_params = cfb_enc_params + cfb_dec_params\n",
    "\n",
    "# swa_params = count_parameters(model.SWA) \n",
    "# p_params = count_parameters(model.P)\n",
    "# d1_params = count_parameters(model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + cfb_total_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (Time t)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB (Total, 2x Modules)\", \"**Pre/Post-Dynamics Mixer**\", f\"**{cfb_total_params:,}**\"], # NEW LINE: Aggregate CFB\n",
    "#     [\"SlidingWindowAttention (SWA)\", \"**Feature Aggregator/Integrator**\", f\"**{swa_params:,}**\"], \n",
    "#     [\"DynNet (P)\", \"Temporal Feature Predictor (M_t → Evolved_t)\", f\"{p_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor (Time t+1)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**Full SWAU_Net Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### SWAU_Net Component Parameter Summary\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4068587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Pretrained Model\n",
    "\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # Load the model\n",
    "# MODEL_FILENAME = \"SWAU_synthetic_pretrain_epoch50_20251119_193158.pth\"\n",
    "# MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# loaded_model, loaded_epoch = load_model(\n",
    "#     model=model, \n",
    "#     model_path=MODEL_PATH, \n",
    "#     device=args.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- INITIALIZATION FOR LOGGING ---\n",
    "# # Set to higher epochs to clearly observe overfitting\n",
    "# args.num_epochs = 50\n",
    "\n",
    "# # # --- OVERFITTING TEST ACTIVATED ---\n",
    "# # # Create an overfit dataset by replicating the first sample (index 0) 100 times\n",
    "# # overfit_data = torch.cat([full_clean_data_tensor_cpu[0].unsqueeze(0)] * 100, dim=0)\n",
    "# # current_train_data = overfit_data \n",
    "# # # ----------------------------------\n",
    "\n",
    "# # --- TRAIN WITH FULL DATASET ---\n",
    "# current_train_data = full_clean_data_tensor_cpu\n",
    "# # ----------------------------------\n",
    "\n",
    "# # History lists\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = [] # Stores number of iterations in each epoch\n",
    "\n",
    "# # --- RESIDUAL HISTORY LISTS (Scores and SDs) ---\n",
    "# train_dice_t1, train_dice_t2, train_dice_t3 = [], [], []\n",
    "# test_dice_t1, test_dice_t2, test_dice_t3 = [], [], []\n",
    "\n",
    "# # Standard Deviation (SD) Lists for Residuals\n",
    "# train_sd_t1, train_sd_t2, train_sd_t3 = [], [], []\n",
    "# test_sd_t1, test_sd_t2, test_sd_t3 = [], [], []\n",
    "\n",
    "# # --- MASK HISTORY LISTS (Scores and SDs) ---\n",
    "# train_mask_t1, train_mask_t2, train_mask_t3 = [], [], []\n",
    "# test_mask_t1, test_mask_t2, test_mask_t3 = [], [], []\n",
    "\n",
    "# # Standard Deviation (SD) Lists for Masks\n",
    "# train_mask_sd_t1, train_mask_sd_t2, train_mask_sd_t3 = [], [], []\n",
    "# test_mask_sd_t1, test_mask_sd_t2, test_mask_sd_t3 = [], [], []\n",
    "\n",
    "# # --- TRAINING SETUP (Reconfirming) ---\n",
    "# # Loss functions (re-instantiated if necessary, matching previous definitions)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "# loss_fn_l1 = nn.L1Loss()\n",
    "# loss_fn_l2 = nn.MSELoss()\n",
    "# ACCUMULATION_STEPS = 8\n",
    "# soft_dice = False\n",
    "\n",
    "# lr = 1E-4\n",
    "# # --- OPTIMIZER DEFINITION ---\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# # --- NEW: LEARNING RATE SCHEDULER INITIALIZATION ---\n",
    "# # Use ReduceLROnPlateau to decrease LR when training loss plateaus.\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',         # Monitor minimum loss\n",
    "#     factor=0.5,         # Reduce LR by 50%\n",
    "#     patience=15,         # Wait 5 epochs for improvement before reducing\n",
    "#     verbose=True, \n",
    "#     min_lr=1e-6         # Stop reducing LR at 1e-6\n",
    "# )\n",
    "# # ---------------------------------------------------\n",
    "\n",
    "# # -----------------\n",
    "# # FREEZE BATCH NORM\n",
    "# freeze_batch_norm(model)\n",
    "# # ----------------\n",
    "\n",
    "# print(f\"\\nStarting SWAU_Net training on device: {args.device} for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# # --- TRAINING LOOP ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "# #     # --- 1. Training Step ---\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, model, optimizer, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, lambda_gdl=0, lambda_faf=0.5, lambda_mask=1.0, lambda_residual=5.0, lambda_recon=0.5, use_augmentation=True\n",
    "#     )\n",
    "    \n",
    "#     # Store iteration loss and count for plot_log_loss\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "    \n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- NEW: SCHEDULER STEP ---\n",
    "#     # Tell the scheduler to check the current loss and adjust the LR if necessary\n",
    "#     scheduler.step(mean_epoch_loss)\n",
    "    \n",
    "#     # --- 3. Logging and Checkpoint ---\n",
    "    \n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary ---\")\n",
    "#     print(f\"Mean Loss: {mean_epoch_loss:.6f}\")\n",
    "\n",
    "#     # --- 4. Per-Epoch Visualizations (MOVED INSIDE LOOP) ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # C. Plot a sample prediction clip (e.g., test sample index 20)\n",
    "#     f_display_frames(current_train_data, model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#      # EVALUATION\n",
    "#     use_median = True\n",
    "#     if use_median==True:\n",
    "#         print('Using DICE Median.')\n",
    "#     else:\n",
    "#         print('Using DICE Aggregate Mean.')\n",
    "        \n",
    "#     if soft_dice==True:\n",
    "#         print('Using Soft DICE.')\n",
    "#     else:\n",
    "#         print('Using hard DICE.')\n",
    "        \n",
    "#     # --- Evaluation Step ---\n",
    "    \n",
    "#     # 1. Unpack Test Results: Returns ((Res Scores, Res SDs), (Msk Scores, Msk SDs))\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#     f_eval_pred_dice_test_set(test_loader, model, args, soft_dice=soft_dice, use_median=use_median)\n",
    "\n",
    "#     # 2. Unpack Train Results\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#     f_eval_pred_dice_train_set(current_train_data, model, args, args.batch_size, soft_dice=soft_dice, use_median=use_median)\n",
    "    \n",
    "#     # --- 1. Residual Scores and SDs Accumulation ---\n",
    "\n",
    "#     # Accumulate Scores\n",
    "#     train_dice_t1.append(res_train_scores[0]); train_dice_t2.append(res_train_scores[1]); train_dice_t3.append(res_train_scores[2])\n",
    "#     test_dice_t1.append(res_test_scores[0]); test_dice_t2.append(res_test_scores[1]); test_dice_t3.append(res_test_scores[2])\n",
    "\n",
    "#     # Accumulate Standard Deviations (SDs)\n",
    "#     train_sd_t1.append(res_train_sds[0]); train_sd_t2.append(res_train_sds[1]); train_sd_t3.append(res_train_sds[2])\n",
    "#     test_sd_t1.append(res_test_sds[0]); test_sd_t2.append(res_test_sds[1]); test_sd_t3.append(res_test_sds[2])\n",
    "\n",
    "#     # --- 2. Mask Scores and SDs Accumulation ---\n",
    "\n",
    "#     # Accumulate Scores\n",
    "#     train_mask_t1.append(msk_train_scores[0]); train_mask_t2.append(msk_train_scores[1]); train_mask_t3.append(msk_train_scores[2])\n",
    "#     test_mask_t1.append(msk_test_scores[0]); test_mask_t2.append(msk_test_scores[1]); test_mask_t3.append(msk_test_scores[2])\n",
    "\n",
    "#     # Accumulate Standard Deviations (SDs)\n",
    "#     train_mask_sd_t1.append(msk_test_sds[0]); train_mask_sd_t2.append(msk_test_sds[1]); train_mask_sd_t3.append(msk_test_sds[2])\n",
    "#     test_mask_sd_t1.append(msk_test_sds[0]); test_mask_sd_t2.append(msk_test_sds[1]); test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "\n",
    "#     # 1. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         train_dice_t1, train_dice_t2, train_dice_t3,\n",
    "#         test_dice_t1, test_dice_t2, test_dice_t3,\n",
    "#         train_sd_t1, train_sd_t2, train_sd_t3,       # PASSING SDs HERE\n",
    "#         test_sd_t1, test_sd_t2, test_sd_t3,         # PASSING SDs HERE\n",
    "#         plot_title='Residual Mask Dice Score History (T=1, T=2, T=3)'\n",
    "#     )\n",
    "\n",
    "#     # 2. Plot Mask History (NEW PLOT)\n",
    "#     plot_train_test_dice_history(\n",
    "#         train_mask_t1, train_mask_t2, train_mask_t3,\n",
    "#         test_mask_t1, test_mask_t2, test_mask_t3,\n",
    "#         train_mask_sd_t1, train_mask_sd_t2, train_mask_sd_t3, # PASSING SDs HERE\n",
    "#         test_mask_sd_t1, test_mask_sd_t2, test_mask_sd_t3,   # PASSING SDs HERE\n",
    "#         plot_title='Full Mask Dice Score History (T=1, T=2, T=3)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//SWAU_Net')\n",
    "# # ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//No Pretraining')\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//No Augmentation')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "\n",
    "# # saved_path = save_model_weights(\n",
    "# #     model=model, \n",
    "# #     final_epoch=FINAL_EPOCH, \n",
    "# #     save_dir=ckpt_save_dir,\n",
    "# #     model_name = \"SWAU_without pretraining\" + '_k_fold_' + str(k) + '_'\n",
    "# # )\n",
    "\n",
    "# saved_path = save_final_experiment_data(\n",
    "#     model=model, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     base_save_dir=ckpt_save_dir, \n",
    "#     k_fold_index=k,\n",
    "    \n",
    "#     # --- Pass all history lists from the main training loop ---\n",
    "#     all_iteration_losses=all_iteration_losses,\n",
    "#     epoch_iteration_counts=epoch_iteration_counts,\n",
    "    \n",
    "#     # Residual\n",
    "#     train_dice_t1=train_dice_t1, train_dice_t2=train_dice_t2, train_dice_t3=train_dice_t3,\n",
    "#     test_dice_t1=test_dice_t1, test_dice_t2=test_dice_t2, test_dice_t3=test_dice_t3,\n",
    "#     train_sd_t1=train_sd_t1, train_sd_t2=train_sd_t2, train_sd_t3=train_sd_t3,\n",
    "#     test_sd_t1=test_sd_t1, test_sd_t2=test_sd_t2, test_sd_t3=test_sd_t3,\n",
    "    \n",
    "#     # Mask\n",
    "#     train_mask_t1=train_mask_t1, train_mask_t2=train_mask_t2, train_mask_t3=train_mask_t3,\n",
    "#     test_mask_t1=test_mask_t1, test_mask_t2=test_mask_t2, test_mask_t3=test_mask_t3,\n",
    "#     train_mask_sd_t1=train_mask_sd_t1, train_mask_sd_t2=train_mask_sd_t2, train_mask_sd_t3=train_mask_sd_t3,\n",
    "#     test_mask_sd_t1=test_mask_sd_t1, test_mask_sd_t2=test_mask_sd_t2, test_mask_sd_t3=test_mask_sd_t3,\n",
    "\n",
    "#     # Set model name prefix\n",
    "#     model_name_prefix=\"SWAU_WITHOUT_augmentation\"\n",
    "# )\n",
    "\n",
    "# if saved_path:\n",
    "#     print(f\"\\n All experiment data saved successfully to: {saved_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache() # Clears unused cached memory\n",
    "#     # Also clear any gradients potentially left from the interrupted batch\n",
    "#     optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 4. Final Visualizations ---\n",
    "# print(\"\\n--- Generating Final Training Visualizations ---\")\n",
    "\n",
    "# # A. Plot Loss History\n",
    "# plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "# # 1. Plot Residual History\n",
    "# plot_train_test_dice_history(\n",
    "#     train_dice_t1, train_dice_t2, train_dice_t3,\n",
    "#     test_dice_t1, test_dice_t2, test_dice_t3,\n",
    "#     train_sd_t1, train_sd_t2, train_sd_t3,       # PASSING SDs HERE\n",
    "#     test_sd_t1, test_sd_t2, test_sd_t3,         # PASSING SDs HERE\n",
    "#     plot_title='Residual Mask Dice Score History (T=1, T=2, T=3)'\n",
    "# )\n",
    "\n",
    "# # 2. Plot Mask History (NEW PLOT)\n",
    "# plot_train_test_dice_history(\n",
    "#     train_mask_t1, train_mask_t2, train_mask_t3,\n",
    "#     test_mask_t1, test_mask_t2, test_mask_t3,\n",
    "#     train_mask_sd_t1, train_mask_sd_t2, train_mask_sd_t3, # PASSING SDs HERE\n",
    "#     test_mask_sd_t1, test_mask_sd_t2, test_mask_sd_t3,   # PASSING SDs HERE\n",
    "#     plot_title='Full Mask Dice Score History (T=1, T=2, T=3)'\n",
    "# )\n",
    "\n",
    "# # C. Plot a sample prediction clip (e.g., test sample index 20)\n",
    "# f_display_frames(current_train_data, model, args, sample_idx=20, T_total=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(len(current_train_data)):\n",
    "#     f_display_frames(current_train_data, model, args, sample_idx=j, T_total=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d131fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Setup for Plotting ---\n",
    "\n",
    "# ## Plot with soft DICE (Residual and Mask)\n",
    "# soft_dice = True\n",
    "# metric_type_str = \"Soft Dice\"\n",
    "\n",
    "# # 1. Evaluate: Unpack the score arrays and discard the Mean/SD tuples\n",
    "# (res_scores_train, msk_scores_train), _ = f_get_individual_dice(\n",
    "#     train_dataset, model, args, is_train_set=True, soft_dice=soft_dice\n",
    "# )\n",
    "# (res_scores_test, msk_scores_test), _ = f_get_individual_dice(\n",
    "#     test_dataset, model, args, is_train_set=False, soft_dice=soft_dice\n",
    "# )\n",
    "\n",
    "# # 2. Plot Residuals (Soft) - (Plotting logic remains correct)\n",
    "# f_plot_individual_dice(res_scores_train, res_scores_test, metric_type_str, channel_name='Residual Mask')\n",
    "\n",
    "# # 3. Plot Masks (Soft)\n",
    "# f_plot_individual_dice(msk_scores_train, msk_scores_test, metric_type_str, channel_name='Full Mask')\n",
    "\n",
    "\n",
    "# # --- Horizontal Rule to Separate Soft/Hard Plots ---\n",
    "# print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# ## Plot with hard DICE (Residual and Mask)\n",
    "# soft_dice = False\n",
    "# metric_type_str = \"Hard Dice\"\n",
    "\n",
    "# # 1. Evaluate: Unpack the score arrays and discard the Mean/SD tuples\n",
    "# (res_scores_train, msk_scores_train), _ = f_get_individual_dice(\n",
    "#     train_dataset, model, args, is_train_set=True, soft_dice=soft_dice\n",
    "# )\n",
    "# (res_scores_test, msk_scores_test), _ = f_get_individual_dice(\n",
    "#     test_dataset, model, args, is_train_set=False, soft_dice=soft_dice\n",
    "# )\n",
    "\n",
    "# # 2. Plot Residuals (Hard)\n",
    "# f_plot_individual_dice(res_scores_train, res_scores_test, metric_type_str, channel_name='Residual Mask')\n",
    "\n",
    "# # 3. Plot Masks (Hard)\n",
    "# f_plot_individual_dice(msk_scores_train, msk_scores_test, metric_type_str, channel_name='Full Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# is_train_set = 1\n",
    "\n",
    "# # Create a DataLoader specifically for batch size 1 iteration\n",
    "# data_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=1, # CRITICAL: Batch size of 1 for individual scores\n",
    "#     shuffle=False,\n",
    "#     num_workers=0\n",
    "# )\n",
    "\n",
    "# all_clip_dice_scores = []\n",
    "# device = args.device\n",
    "# T_pred = 3\n",
    "\n",
    "# # Determine which metric tensor to use for evaluation\n",
    "# if soft_dice:\n",
    "#     desc_label = \"Soft Dice (Individual)\"\n",
    "# else:\n",
    "#     desc_label = \"Hard Dice (Individual)\"\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # tqdm description is based on whether we are processing the train tensor or test loader\n",
    "#     if is_train_set:\n",
    "#         iterator = tqdm(data_loader, desc=f\"Evaluating Train Clips ({desc_label})\")\n",
    "#     else:\n",
    "#         iterator = tqdm(data_loader, desc=f\"Evaluating Test Clips ({desc_label})\")\n",
    "\n",
    "#     for data_batch in iterator:\n",
    "#         # data_batch shape: [1, C, T, H, W]\n",
    "\n",
    "#         predictions, targets, *discards = model(data_batch.to(device))\n",
    "#         break\n",
    "        \n",
    "# predictions.size()\n",
    "\n",
    "# targets.size()\n",
    "\n",
    "# print(targets[0,0,2,:,:])\n",
    "\n",
    "# print(predictions[0,0,2,:,:])\n",
    "\n",
    "# plt.imshow(targets[0,2,2,:,:].detach().cpu().numpy())\n",
    "# plt.show()\n",
    "# plt.imshow(predictions[0,2,2,:,:].detach().cpu().numpy())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "##########################################################################################################################\n",
    "\n",
    "##########################################################################################################################\n",
    "##########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111e287",
   "metadata": {},
   "source": [
    "## Train CFB Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Reduced from 24 to 16\n",
    "# args.d_attn1 = 128 # Reduced from 192 to 128\n",
    "# args.d_attn2 = 256 # Reduced from 384 to 256\n",
    "\n",
    "# args.num_attn_layers = 2\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the full SWAU_Net model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the full model and move it to the device\n",
    "# # The model is SWAU_Net, which owns E1, CFB_enc, CFB_dec, SWA, P, and D1.\n",
    "# model = SWAU_CFB_Ablation(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(model.E1)\n",
    "\n",
    "# swa_params = count_parameters(model.SWA) \n",
    "# p_params = count_parameters(model.P)\n",
    "# d1_params = count_parameters(model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (Time t)\", f\"{e1_params:,}\"],\n",
    "#     [\"SlidingWindowAttention (SWA)\", \"**Feature Aggregator/Integrator**\", f\"**{swa_params:,}**\"], \n",
    "#     [\"DynNet (P)\", \"Temporal Feature Predictor (M_t → Evolved_t)\", f\"{p_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor (Time t+1)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**Full SWAU_Net Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### SWAU_Net Component Parameter Summary\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73544daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Pretrained Model\n",
    "\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # Load the model\n",
    "# MODEL_FILENAME = \"CFB_Ablation_synthetic_pretrain_epoch50_20251120_145522.pth\"\n",
    "# MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# loaded_model, loaded_epoch = load_model(\n",
    "#     model=model, \n",
    "#     model_path=MODEL_PATH, \n",
    "#     device=args.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- INITIALIZATION FOR LOGGING ---\n",
    "# # Set to higher epochs to clearly observe overfitting\n",
    "# args.num_epochs = 50\n",
    "\n",
    "# # # --- OVERFITTING TEST ACTIVATED ---\n",
    "# # # Create an overfit dataset by replicating the first sample (index 0) 100 times\n",
    "# # overfit_data = torch.cat([full_clean_data_tensor_cpu[0].unsqueeze(0)] * 100, dim=0)\n",
    "# # current_train_data = overfit_data \n",
    "# # # ----------------------------------\n",
    "\n",
    "# # --- TRAIN WITH FULL DATASET ---\n",
    "# current_train_data = full_clean_data_tensor_cpu\n",
    "# # ----------------------------------\n",
    "\n",
    "# # History lists\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = [] # Stores number of iterations in each epoch\n",
    "\n",
    "# # --- RESIDUAL HISTORY LISTS (Scores and SDs) ---\n",
    "# train_dice_t1, train_dice_t2, train_dice_t3 = [], [], []\n",
    "# test_dice_t1, test_dice_t2, test_dice_t3 = [], [], []\n",
    "\n",
    "# # Standard Deviation (SD) Lists for Residuals\n",
    "# train_sd_t1, train_sd_t2, train_sd_t3 = [], [], []\n",
    "# test_sd_t1, test_sd_t2, test_sd_t3 = [], [], []\n",
    "\n",
    "# # --- MASK HISTORY LISTS (Scores and SDs) ---\n",
    "# train_mask_t1, train_mask_t2, train_mask_t3 = [], [], []\n",
    "# test_mask_t1, test_mask_t2, test_mask_t3 = [], [], []\n",
    "\n",
    "# # Standard Deviation (SD) Lists for Masks\n",
    "# train_mask_sd_t1, train_mask_sd_t2, train_mask_sd_t3 = [], [], []\n",
    "# test_mask_sd_t1, test_mask_sd_t2, test_mask_sd_t3 = [], [], []\n",
    "\n",
    "# # --- TRAINING SETUP (Reconfirming) ---\n",
    "# # Loss functions (re-instantiated if necessary, matching previous definitions)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "# loss_fn_l1 = nn.L1Loss()\n",
    "# loss_fn_l2 = nn.MSELoss()\n",
    "# ACCUMULATION_STEPS = 8\n",
    "# soft_dice = False\n",
    "\n",
    "# lr = 1E-4\n",
    "# # --- OPTIMIZER DEFINITION ---\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# # --- NEW: LEARNING RATE SCHEDULER INITIALIZATION ---\n",
    "# # Use ReduceLROnPlateau to decrease LR when training loss plateaus.\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',         # Monitor minimum loss\n",
    "#     factor=0.5,         # Reduce LR by 50%\n",
    "#     patience=15,         # Wait 5 epochs for improvement before reducing\n",
    "#     verbose=True, \n",
    "#     min_lr=1e-6         # Stop reducing LR at 1e-6\n",
    "# )\n",
    "# # ---------------------------------------------------\n",
    "\n",
    "# # -----------------\n",
    "# # FREEZE BATCH NORM\n",
    "# freeze_batch_norm(model)\n",
    "# # ----------------\n",
    "\n",
    "# print(f\"\\nStarting SWAU_Net training on device: {args.device} for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# # --- TRAINING LOOP ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "# #     # --- 1. Training Step ---\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, model, optimizer, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, lambda_gdl=0, lambda_faf=0.5, lambda_mask=1.0, lambda_residual=5.0, lambda_recon=0.5, use_augmentation=True\n",
    "#     )\n",
    "    \n",
    "#     # Store iteration loss and count for plot_log_loss\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "    \n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- NEW: SCHEDULER STEP ---\n",
    "#     # Tell the scheduler to check the current loss and adjust the LR if necessary\n",
    "#     scheduler.step(mean_epoch_loss)\n",
    "    \n",
    "#     # --- 3. Logging and Checkpoint ---\n",
    "    \n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary ---\")\n",
    "#     print(f\"Mean Loss: {mean_epoch_loss:.6f}\")\n",
    "\n",
    "#     # --- 4. Per-Epoch Visualizations (MOVED INSIDE LOOP) ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # C. Plot a sample prediction clip (e.g., test sample index 20)\n",
    "#     f_display_frames(current_train_data, model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#      # EVALUATION\n",
    "#     use_median = True\n",
    "#     if use_median==True:\n",
    "#         print('Using DICE Median.')\n",
    "#     else:\n",
    "#         print('Using DICE Aggregate Mean.')\n",
    "        \n",
    "#     if soft_dice==True:\n",
    "#         print('Using Soft DICE.')\n",
    "#     else:\n",
    "#         print('Using hard DICE.')\n",
    "        \n",
    "#     # --- Evaluation Step ---\n",
    "    \n",
    "#     # 1. Unpack Test Results: Returns ((Res Scores, Res SDs), (Msk Scores, Msk SDs))\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#     f_eval_pred_dice_test_set(test_loader, model, args, soft_dice=soft_dice, use_median=use_median)\n",
    "\n",
    "#     # 2. Unpack Train Results\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#     f_eval_pred_dice_train_set(current_train_data, model, args, args.batch_size, soft_dice=soft_dice, use_median=use_median)\n",
    "    \n",
    "#     # --- 1. Residual Scores and SDs Accumulation ---\n",
    "\n",
    "#     # Accumulate Scores\n",
    "#     train_dice_t1.append(res_train_scores[0]); train_dice_t2.append(res_train_scores[1]); train_dice_t3.append(res_train_scores[2])\n",
    "#     test_dice_t1.append(res_test_scores[0]); test_dice_t2.append(res_test_scores[1]); test_dice_t3.append(res_test_scores[2])\n",
    "\n",
    "#     # Accumulate Standard Deviations (SDs)\n",
    "#     train_sd_t1.append(res_train_sds[0]); train_sd_t2.append(res_train_sds[1]); train_sd_t3.append(res_train_sds[2])\n",
    "#     test_sd_t1.append(res_test_sds[0]); test_sd_t2.append(res_test_sds[1]); test_sd_t3.append(res_test_sds[2])\n",
    "\n",
    "#     # --- 2. Mask Scores and SDs Accumulation ---\n",
    "\n",
    "#     # Accumulate Scores\n",
    "#     train_mask_t1.append(msk_train_scores[0]); train_mask_t2.append(msk_train_scores[1]); train_mask_t3.append(msk_train_scores[2])\n",
    "#     test_mask_t1.append(msk_test_scores[0]); test_mask_t2.append(msk_test_scores[1]); test_mask_t3.append(msk_test_scores[2])\n",
    "\n",
    "#     # Accumulate Standard Deviations (SDs)\n",
    "#     train_mask_sd_t1.append(msk_test_sds[0]); train_mask_sd_t2.append(msk_test_sds[1]); train_mask_sd_t3.append(msk_test_sds[2])\n",
    "#     test_mask_sd_t1.append(msk_test_sds[0]); test_mask_sd_t2.append(msk_test_sds[1]); test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "\n",
    "#     # 1. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         train_dice_t1, train_dice_t2, train_dice_t3,\n",
    "#         test_dice_t1, test_dice_t2, test_dice_t3,\n",
    "#         train_sd_t1, train_sd_t2, train_sd_t3,       # PASSING SDs HERE\n",
    "#         test_sd_t1, test_sd_t2, test_sd_t3,         # PASSING SDs HERE\n",
    "#         plot_title='Residual Mask Dice Score History (T=1, T=2, T=3)'\n",
    "#     )\n",
    "\n",
    "#     # 2. Plot Mask History (NEW PLOT)\n",
    "#     plot_train_test_dice_history(\n",
    "#         train_mask_t1, train_mask_t2, train_mask_t3,\n",
    "#         test_mask_t1, test_mask_t2, test_mask_t3,\n",
    "#         train_mask_sd_t1, train_mask_sd_t2, train_mask_sd_t3, # PASSING SDs HERE\n",
    "#         test_mask_sd_t1, test_mask_sd_t2, test_mask_sd_t3,   # PASSING SDs HERE\n",
    "#         plot_title='Full Mask Dice Score History (T=1, T=2, T=3)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//SWAU_Net')\n",
    "# # ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//No Pretraining')\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//CFB Ablation')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "\n",
    "# # saved_path = save_model_weights(\n",
    "# #     model=model, \n",
    "# #     final_epoch=FINAL_EPOCH, \n",
    "# #     save_dir=ckpt_save_dir,\n",
    "# #     model_name = \"SWAU_without pretraining\" + '_k_fold_' + str(k) + '_'\n",
    "# # )\n",
    "\n",
    "# saved_path = save_final_experiment_data(\n",
    "#     model=model, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     base_save_dir=ckpt_save_dir, \n",
    "#     k_fold_index=k,\n",
    "    \n",
    "#     # --- Pass all history lists from the main training loop ---\n",
    "#     all_iteration_losses=all_iteration_losses,\n",
    "#     epoch_iteration_counts=epoch_iteration_counts,\n",
    "    \n",
    "#     # Residual\n",
    "#     train_dice_t1=train_dice_t1, train_dice_t2=train_dice_t2, train_dice_t3=train_dice_t3,\n",
    "#     test_dice_t1=test_dice_t1, test_dice_t2=test_dice_t2, test_dice_t3=test_dice_t3,\n",
    "#     train_sd_t1=train_sd_t1, train_sd_t2=train_sd_t2, train_sd_t3=train_sd_t3,\n",
    "#     test_sd_t1=test_sd_t1, test_sd_t2=test_sd_t2, test_sd_t3=test_sd_t3,\n",
    "    \n",
    "#     # Mask\n",
    "#     train_mask_t1=train_mask_t1, train_mask_t2=train_mask_t2, train_mask_t3=train_mask_t3,\n",
    "#     test_mask_t1=test_mask_t1, test_mask_t2=test_mask_t2, test_mask_t3=test_mask_t3,\n",
    "#     train_mask_sd_t1=train_mask_sd_t1, train_mask_sd_t2=train_mask_sd_t2, train_mask_sd_t3=train_mask_sd_t3,\n",
    "#     test_mask_sd_t1=test_mask_sd_t1, test_mask_sd_t2=test_mask_sd_t2, test_mask_sd_t3=test_mask_sd_t3,\n",
    "\n",
    "#     # Set model name prefix\n",
    "#     model_name_prefix=\"SWAU_WITHOUT_augmentation\"\n",
    "# )\n",
    "\n",
    "# if saved_path:\n",
    "#     print(f\"\\n All experiment data saved successfully to: {saved_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906a9959",
   "metadata": {},
   "source": [
    "## Train DynNet Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523fa6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Base channel width\n",
    "# args.d_attn1 = 128 # Feed-forward dim for L3\n",
    "# args.d_attn2 = 256 # Feed-forward dim for L4/L5\n",
    "\n",
    "# args.num_attn_layers = 2 # Number of SWA layers\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the SWAU_DynNet_Ablation model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the ablation model and move it to the device\n",
    "# # NOTE: The CFB components are now included inside E1 and D1 or explicitly defined.\n",
    "# # Using the SWAU_Net_NoDynNet_Decoupled class.\n",
    "# swau_dynnet_ablation_model = SWAU_DynNet_Ablation(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(swau_dynnet_ablation_model.E1)\n",
    "# cfb_enc_params = count_parameters(swau_dynnet_ablation_model.CFB_enc)\n",
    "# cfb_dec_params = count_parameters(swau_dynnet_ablation_model.CFB_dec)\n",
    "# swa_params = count_parameters(swau_dynnet_ablation_model.SWA)\n",
    "\n",
    "# # DynNet is removed, so its parameter count is 0.\n",
    "# p_params = 0 \n",
    "\n",
    "# d1_params = count_parameters(swau_dynnet_ablation_model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count (including CFB blocks)\n",
    "# total_params = e1_params + cfb_enc_params + cfb_dec_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (Time t)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB_enc\", \"Pre-Aggregation Channel Refinement\", f\"{cfb_enc_params:,}\"],\n",
    "#     [\"SlidingWindowAttention (SWA)\", \"**Feature Aggregator/Estimator**\", f\"**{swa_params:,}**\"],\n",
    "#     [\"DynNet (P)\", \"Temporal Evolution Module\", f\"**{p_params:,} (Removed)**\"], # P_params = 0\n",
    "#     [\"CFB_dec\", \"Post-Aggregation Channel Refinement\", f\"{cfb_dec_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor (Time t+1)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**SWAU_DynNet_Ablation Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### SWAU_DynNet_Ablation Component Parameter Summary\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0194bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Pretrained Model\n",
    "\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # Load the model\n",
    "# MODEL_FILENAME = \"DynNet_Ablation_synthetic_pretrain_epoch50_20251123_123558.pth\"\n",
    "# MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# loaded_model, loaded_epoch = load_model(\n",
    "#     model=swau_dynnet_ablation_model, \n",
    "#     model_path=MODEL_PATH, \n",
    "#     device=args.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16403394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- INITIALIZATION FOR LOGGING ---\n",
    "# # Set to higher epochs to clearly observe overfitting\n",
    "# args.num_epochs = 50\n",
    "\n",
    "# # --- TRAIN WITH FULL DATASET ---\n",
    "# current_train_data = full_clean_data_tensor_cpu\n",
    "# # ----------------------------------\n",
    "\n",
    "# # History lists\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = [] # Stores number of iterations in each epoch\n",
    "\n",
    "# # --- RESIDUAL HISTORY LISTS (Scores and SDs) - USING NEW NAMING CONVENTION ---\n",
    "# # These lists are now redundant as the main training loop (from the previous prompt) \n",
    "# # uses the 'swau_' prefixes. We initialize the *final* lists here.\n",
    "# swau_train_t1, swau_train_t2, swau_train_t3 = [], [], []\n",
    "# swau_test_t1, swau_test_t2, swau_test_t3 = [], [], []\n",
    "# swau_train_sd_t1, swau_train_sd_t2, swau_train_sd_t3 = [], [], []\n",
    "# swau_test_sd_t1, swau_test_sd_t2, swau_test_sd_t3 = [], [], []\n",
    "\n",
    "# # --- MASK HISTORY LISTS (Scores and SDs) - USING NEW NAMING CONVENTION ---\n",
    "# swau_train_mask_t1, swau_train_mask_t2, swau_train_mask_t3 = [], [], []\n",
    "# swau_test_mask_t1, swau_test_mask_t2, swau_test_mask_t3 = [], [], []\n",
    "# swau_train_mask_sd_t1, swau_train_mask_sd_t2, swau_train_mask_sd_t3 = [], [], []\n",
    "# swau_test_mask_sd_t1, swau_test_mask_sd_t2, swau_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# # --- TRAINING SETUP (Reconfirming) ---\n",
    "# # Loss functions (re-instantiated if necessary, matching previous definitions)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "# loss_fn_l1 = nn.L1Loss()\n",
    "# loss_fn_l2 = nn.MSELoss()\n",
    "# ACCUMULATION_STEPS = 8\n",
    "# soft_dice = False\n",
    "\n",
    "# lr = 1E-4\n",
    "# # --- OPTIMIZER DEFINITION (Using the working model instance: swau_dynnet_ablation_model) ---\n",
    "# optimizer = torch.optim.Adam(swau_dynnet_ablation_model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# # --- LEARNING RATE SCHEDULER INITIALIZATION ---\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',         # Monitor minimum loss\n",
    "#     factor=0.5,         \n",
    "#     patience=15,        \n",
    "#     verbose=True, \n",
    "#     min_lr=1e-6         \n",
    "# )\n",
    "# # ---------------------------------------------------\n",
    "\n",
    "# # -----------------\n",
    "# # FREEZE BATCH NORM (Using the working model instance: swau_dynnet_ablation_model)\n",
    "# freeze_batch_norm(swau_dynnet_ablation_model)\n",
    "# # ----------------\n",
    "\n",
    "# print(f\"\\nStarting SWAU_DynNet_Ablation training on device: {args.device} for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# # --- TRAINING LOOP ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "# #     # --- 1. Training Step ---\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, swau_dynnet_ablation_model, optimizer, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, lambda_gdl=0, lambda_faf=0.5, lambda_mask=1.0, lambda_residual=5.0, lambda_recon=0.5, use_augmentation=True\n",
    "#     )\n",
    "    \n",
    "#     # Store iteration loss and count for plot_log_loss\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "    \n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- NEW: SCHEDULER STEP ---\n",
    "#     scheduler.step(mean_epoch_loss)\n",
    "    \n",
    "#     # --- 3. Logging and Checkpoint ---\n",
    "    \n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary ---\")\n",
    "#     print(f\"Mean Loss: {mean_epoch_loss:.6f}\")\n",
    "\n",
    "#     # --- 4. Per-Epoch Visualizations (MOVED INSIDE LOOP) ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # C. Plot a sample prediction clip \n",
    "#     f_display_frames(current_train_data, swau_dynnet_ablation_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#      # EVALUATION\n",
    "#     use_median = True\n",
    "#     if use_median==True:\n",
    "#         print('Using DICE Median.')\n",
    "#     else:\n",
    "#         print('Using DICE Aggregate Mean.')\n",
    "        \n",
    "#     if soft_dice==True:\n",
    "#         print('Using Soft DICE.')\n",
    "#     else:\n",
    "#         print('Using hard DICE.')\n",
    "        \n",
    "#     # --- Evaluation Step (Using the working model instance: swau_dynnet_ablation_model) ---\n",
    "    \n",
    "#     # 1. Unpack Test Results: Returns ((Res Scores, Res SDs), (Msk Scores, Msk SDs))\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#     f_eval_pred_dice_test_set(test_loader, swau_dynnet_ablation_model, args, soft_dice=soft_dice, use_median=use_median)\n",
    "\n",
    "#     # 2. Unpack Train Results\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#     f_eval_pred_dice_train_set(current_train_data, swau_dynnet_ablation_model, args, args.batch_size, soft_dice=soft_dice, use_median=use_median)\n",
    "    \n",
    "#     # --- 1. Residual Scores and SDs Accumulation (Using swau_ prefixes) ---\n",
    "\n",
    "#     # Accumulate Scores\n",
    "#     swau_train_t1.append(res_train_scores[0]); swau_train_t2.append(res_train_scores[1]); swau_train_t3.append(res_train_scores[2])\n",
    "#     swau_test_t1.append(res_test_scores[0]); swau_test_t2.append(res_test_scores[1]); swau_test_t3.append(res_test_scores[2])\n",
    "\n",
    "#     # Accumulate Standard Deviations (SDs)\n",
    "#     swau_train_sd_t1.append(res_train_sds[0]); swau_train_sd_t2.append(res_train_sds[1]); swau_train_sd_t3.append(res_train_sds[2])\n",
    "#     swau_test_sd_t1.append(res_test_sds[0]); swau_test_sd_t2.append(res_test_sds[1]); swau_test_sd_t3.append(res_test_sds[2])\n",
    "\n",
    "#     # --- 2. Mask Scores and SDs Accumulation (Using swau_ prefixes) ---\n",
    "\n",
    "#     # Accumulate Scores\n",
    "#     swau_train_mask_t1.append(msk_train_scores[0]); swau_train_mask_t2.append(msk_train_scores[1]); swau_train_mask_t3.append(msk_train_scores[2])\n",
    "#     swau_test_mask_t1.append(msk_test_scores[0]); swau_test_mask_t2.append(msk_test_scores[1]); swau_test_mask_t3.append(msk_test_scores[2])\n",
    "\n",
    "#     # Accumulate Standard Deviations (SDs)\n",
    "#     swau_train_mask_sd_t1.append(msk_train_sds[0]); swau_train_mask_sd_t2.append(msk_train_sds[1]); swau_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     swau_test_mask_sd_t1.append(msk_test_sds[0]); swau_test_mask_sd_t2.append(msk_test_sds[1]); swau_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "\n",
    "#     # 1. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         swau_train_t1, swau_train_t2, swau_train_t3,\n",
    "#         swau_test_t1, swau_test_t2, swau_test_t3,\n",
    "#         swau_train_sd_t1, swau_train_sd_t2, swau_train_sd_t3,       \n",
    "#         swau_test_sd_t1, swau_test_sd_t2, swau_test_sd_t3,         \n",
    "#         plot_title='SWAU_DynNet_Ablation Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # 2. Plot Mask History \n",
    "#     plot_train_test_dice_history(\n",
    "#         swau_train_mask_t1, swau_train_mask_t2, swau_train_mask_t3,\n",
    "#         swau_test_mask_t1, swau_test_mask_t2, swau_test_mask_t3,\n",
    "#         swau_train_mask_sd_t1, swau_train_mask_sd_t2, swau_train_mask_sd_t3, \n",
    "#         swau_test_mask_sd_t1, swau_test_mask_sd_t2, swau_test_mask_sd_t3,   \n",
    "#         plot_title='SWAU_DynNet_Ablation Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DIRECTORY PATH (Change this for your execution environment) ---\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//DynNet_Ablation')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "\n",
    "# # --- SAVE FINAL EXPERIMENT DATA ---\n",
    "\n",
    "# # NOTE: We use the working model instance 'swau_dynnet_ablation_model' for saving.\n",
    "# saved_path = save_final_experiment_data(\n",
    "#     model=swau_dynnet_ablation_model, # Use the model instance from the training loop\n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     base_save_dir=ckpt_save_dir, \n",
    "#     k_fold_index=k,\n",
    "    \n",
    "#     # --- Pass all history lists from the main training loop (using 'swau_' prefixes) ---\n",
    "#     all_iteration_losses=all_iteration_losses,\n",
    "#     epoch_iteration_counts=epoch_iteration_counts,\n",
    "    \n",
    "#     # Residual\n",
    "#     train_dice_t1=swau_train_t1, train_dice_t2=swau_train_t2, train_dice_t3=swau_train_t3,\n",
    "#     test_dice_t1=swau_test_t1, test_dice_t2=swau_test_t2, test_dice_t3=swau_test_t3,\n",
    "#     train_sd_t1=swau_train_sd_t1, train_sd_t2=swau_train_sd_t2, train_sd_t3=swau_train_sd_t3,\n",
    "#     test_sd_t1=swau_test_sd_t1, test_sd_t2=swau_test_sd_t2, test_sd_t3=swau_test_sd_t3,\n",
    "    \n",
    "#     # Mask\n",
    "#     train_mask_t1=swau_train_mask_t1, train_mask_t2=swau_train_mask_t2, train_mask_t3=swau_train_mask_t3,\n",
    "#     test_mask_t1=swau_test_mask_t1, test_mask_t2=swau_test_mask_t2, test_mask_t3=swau_test_mask_t3,\n",
    "#     train_mask_sd_t1=swau_train_mask_sd_t1, train_mask_sd_t2=swau_train_mask_sd_t2, train_mask_sd_t3=swau_train_mask_sd_t3,\n",
    "#     test_mask_sd_t1=swau_test_mask_sd_t1, test_mask_sd_t2=swau_test_mask_sd_t2, test_mask_sd_t3=swau_test_mask_sd_t3,\n",
    "\n",
    "#     # Set model name prefix to reflect the specific ablation\n",
    "#     model_name_prefix=\"SWAU_DYNNET_ABLATION\" \n",
    "# )\n",
    "\n",
    "# if saved_path:\n",
    "#     print(f\"\\n All experiment data saved successfully to: {saved_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7b71a",
   "metadata": {},
   "source": [
    "## Train Axial U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f119ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Train Axial U-Net\n",
    "\n",
    "# args.num_attn_layers = 2\n",
    "\n",
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Reduced from 24 to 16\n",
    "# args.d_attn1 = 128 # Reduced from 192 to 128\n",
    "# args.d_attn2 = 256 # Reduced from 384 to 256\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the full AxialU_Net model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the full model and move it to the device\n",
    "# # We use AxialU_Net for the instantiation but maintain SWAU_Net names in comments/output\n",
    "# model = AxialU_Net(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(model.E1)\n",
    "\n",
    "# # CORRECTED: Calculate parameters for both CFB modules separately\n",
    "# cfb_enc_params = count_parameters(model.CFB_enc) \n",
    "# cfb_dec_params = count_parameters(model.CFB_dec) \n",
    "# cfb_total_params = cfb_enc_params + cfb_dec_params\n",
    "\n",
    "# # *** CRITICAL FIX: Assigning the Axial Aggregator params to the SWA variable name ***\n",
    "# swa_params = count_parameters(model.Axial_Aggregator) \n",
    "\n",
    "# p_params = count_parameters(model.P)\n",
    "# d1_params = count_parameters(model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + cfb_total_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (Time t)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB (Total, 2x Modules)\", \"**Pre/Post-Dynamics Mixer**\", f\"**{cfb_total_params:,}**\"], # NEW LINE: Aggregate CFB\n",
    "#     [\"SlidingWindowAttention (SWA)\", \"**Feature Aggregator/Integrator**\", f\"**{swa_params:,}**\"], # Using SWA name, but value is Axial Aggregator\n",
    "#     [\"DynNet (P)\", \"Temporal Feature Predictor (M_t → Evolved_t)\", f\"{p_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor (Time t+1)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**Full SWAU_Net Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### SWAU_Net Component Parameter Summary\\n\")\n",
    "# # CRITICAL FIX: Using the custom 'tabulate' replacement function for compliance\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a14b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Pretrained Model\n",
    "\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # Load the model\n",
    "# MODEL_FILENAME = \"AxialU_Net_pretrain_epoch50_20251118_223001.pth\"\n",
    "# MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# loaded_model, loaded_epoch = load_model(\n",
    "#     model=model, \n",
    "#     model_path=MODEL_PATH, \n",
    "#     device=args.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ddf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- INITIALIZATION FOR LOGGING ---\n",
    "# # Set to higher epochs to clearly observe overfitting\n",
    "# args.num_epochs = 60\n",
    "\n",
    "# # # --- OVERFITTING TEST ACTIVATED ---\n",
    "# # # Create an overfit dataset by replicating the first sample (index 0) 100 times\n",
    "# # overfit_data = torch.cat([full_clean_data_tensor_cpu[0].unsqueeze(0)] * 100, dim=0)\n",
    "# # current_train_data = overfit_data \n",
    "# # # ----------------------------------\n",
    "\n",
    "# # --- TRAIN WITH FULL DATASET ---\n",
    "# current_train_data = full_clean_data_tensor_cpu\n",
    "# # ----------------------------------\n",
    "\n",
    "# # History lists\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = [] # Stores number of iterations in each epoch\n",
    "\n",
    "# # --- RESIDUAL HISTORY LISTS (Scores and SDs) ---\n",
    "# train_dice_t1, train_dice_t2, train_dice_t3 = [], [], []\n",
    "# test_dice_t1, test_dice_t2, test_dice_t3 = [], [], []\n",
    "\n",
    "# # Standard Deviation (SD) Lists for Residuals\n",
    "# train_sd_t1, train_sd_t2, train_sd_t3 = [], [], []\n",
    "# test_sd_t1, test_sd_t2, test_sd_t3 = [], [], []\n",
    "\n",
    "# # --- MASK HISTORY LISTS (Scores and SDs) ---\n",
    "# train_mask_t1, train_mask_t2, train_mask_t3 = [], [], []\n",
    "# test_mask_t1, test_mask_t2, test_mask_t3 = [], [], []\n",
    "\n",
    "# # Standard Deviation (SD) Lists for Masks\n",
    "# train_mask_sd_t1, train_mask_sd_t2, train_mask_sd_t3 = [], [], []\n",
    "# test_mask_sd_t1, test_mask_sd_t2, test_mask_sd_t3 = [], [], []\n",
    "\n",
    "# # --- TRAINING SETUP (Reconfirming) ---\n",
    "# # Loss functions (re-instantiated if necessary, matching previous definitions)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "# loss_fn_l1 = nn.L1Loss()\n",
    "# loss_fn_l2 = nn.MSELoss()\n",
    "# ACCUMULATION_STEPS = 8\n",
    "# soft_dice = False\n",
    "\n",
    "# lr = 1E-4\n",
    "# # --- OPTIMIZER DEFINITION ---\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# # --- NEW: LEARNING RATE SCHEDULER INITIALIZATION ---\n",
    "# # Use ReduceLROnPlateau to decrease LR when training loss plateaus.\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',         # Monitor minimum loss\n",
    "#     factor=0.5,         # Reduce LR by 50%\n",
    "#     patience=15,         # Wait 5 epochs for improvement before reducing\n",
    "#     verbose=True, \n",
    "#     min_lr=1e-6         # Stop reducing LR at 1e-6\n",
    "# )\n",
    "# # ---------------------------------------------------\n",
    "\n",
    "# # -----------------\n",
    "# # FREEZE BATCH NORM\n",
    "# freeze_batch_norm(model)\n",
    "# # ----------------\n",
    "\n",
    "# print(f\"\\nStarting AxialU_Net training on device: {args.device} for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# # --- TRAINING LOOP ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "# #     # --- 1. Training Step ---\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, model, optimizer, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, lambda_gdl=0, lambda_faf=0.5, lambda_mask=1.0, lambda_residual=5.0, lambda_recon=0.5, use_augmentation=True\n",
    "#     )\n",
    "    \n",
    "#     # Store iteration loss and count for plot_log_loss\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "    \n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- NEW: SCHEDULER STEP ---\n",
    "#     # Tell the scheduler to check the current loss and adjust the LR if necessary\n",
    "#     scheduler.step(mean_epoch_loss)\n",
    "    \n",
    "#     # --- 3. Logging and Checkpoint ---\n",
    "    \n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary ---\")\n",
    "#     print(f\"Mean Loss: {mean_epoch_loss:.6f}\")\n",
    "\n",
    "#     # --- 4. Per-Epoch Visualizations (MOVED INSIDE LOOP) ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # C. Plot a sample prediction clip (e.g., test sample index 20)\n",
    "#     f_display_frames(current_train_data, model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#      # EVALUATION\n",
    "#     use_median = True\n",
    "#     if use_median==True:\n",
    "#         print('Using DICE Median.')\n",
    "#     else:\n",
    "#         print('Using DICE Aggregate Mean.')\n",
    "        \n",
    "#     if soft_dice==True:\n",
    "#         print('Using Soft DICE.')\n",
    "#     else:\n",
    "#         print('Using hard DICE.')\n",
    "        \n",
    "#     # --- Evaluation Step ---\n",
    "    \n",
    "#     # 1. Unpack Test Results: Returns ((Res Scores, Res SDs), (Msk Scores, Msk SDs))\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#     f_eval_pred_dice_test_set(test_loader, model, args, soft_dice=soft_dice, use_median=use_median)\n",
    "\n",
    "#     # 2. Unpack Train Results\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#     f_eval_pred_dice_train_set(current_train_data, model, args, args.batch_size, soft_dice=soft_dice, use_median=use_median)\n",
    "    \n",
    "#     # --- 1. Residual Scores and SDs Accumulation ---\n",
    "\n",
    "#     # Accumulate Scores\n",
    "#     train_dice_t1.append(res_train_scores[0]); train_dice_t2.append(res_train_scores[1]); train_dice_t3.append(res_train_scores[2])\n",
    "#     test_dice_t1.append(res_test_scores[0]); test_dice_t2.append(res_test_scores[1]); test_dice_t3.append(res_test_scores[2])\n",
    "\n",
    "#     # Accumulate Standard Deviations (SDs)\n",
    "#     train_sd_t1.append(res_train_sds[0]); train_sd_t2.append(res_train_sds[1]); train_sd_t3.append(res_train_sds[2])\n",
    "#     test_sd_t1.append(res_test_sds[0]); test_sd_t2.append(res_test_sds[1]); test_sd_t3.append(res_test_sds[2])\n",
    "\n",
    "#     # --- 2. Mask Scores and SDs Accumulation ---\n",
    "\n",
    "#     # Accumulate Scores\n",
    "#     train_mask_t1.append(msk_train_scores[0]); train_mask_t2.append(msk_train_scores[1]); train_mask_t3.append(msk_train_scores[2])\n",
    "#     test_mask_t1.append(msk_test_scores[0]); test_mask_t2.append(msk_test_scores[1]); test_mask_t3.append(msk_test_scores[2])\n",
    "\n",
    "#     # Accumulate Standard Deviations (SDs)\n",
    "#     train_mask_sd_t1.append(msk_test_sds[0]); train_mask_sd_t2.append(msk_test_sds[1]); train_mask_sd_t3.append(msk_test_sds[2])\n",
    "#     test_mask_sd_t1.append(msk_test_sds[0]); test_mask_sd_t2.append(msk_test_sds[1]); test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "\n",
    "#     # 1. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         train_dice_t1, train_dice_t2, train_dice_t3,\n",
    "#         test_dice_t1, test_dice_t2, test_dice_t3,\n",
    "#         train_sd_t1, train_sd_t2, train_sd_t3,       # PASSING SDs HERE\n",
    "#         test_sd_t1, test_sd_t2, test_sd_t3,         # PASSING SDs HERE\n",
    "#         plot_title='Residual Mask Dice Score History (T=1, T=2, T=3)'\n",
    "#     )\n",
    "\n",
    "#     # 2. Plot Mask History (NEW PLOT)\n",
    "#     plot_train_test_dice_history(\n",
    "#         train_mask_t1, train_mask_t2, train_mask_t3,\n",
    "#         test_mask_t1, test_mask_t2, test_mask_t3,\n",
    "#         train_mask_sd_t1, train_mask_sd_t2, train_mask_sd_t3, # PASSING SDs HERE\n",
    "#         test_mask_sd_t1, test_mask_sd_t2, test_mask_sd_t3,   # PASSING SDs HERE\n",
    "#         plot_title='Full Mask Dice Score History (T=1, T=2, T=3)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Axial_UNet')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "\n",
    "# saved_path = save_final_experiment_data(\n",
    "#     model=model, \n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     base_save_dir=ckpt_save_dir, \n",
    "#     k_fold_index=k,\n",
    "    \n",
    "#     # --- Pass all history lists from the main training loop ---\n",
    "#     all_iteration_losses=all_iteration_losses,\n",
    "#     epoch_iteration_counts=epoch_iteration_counts,\n",
    "    \n",
    "#     # Residual\n",
    "#     train_dice_t1=train_dice_t1, train_dice_t2=train_dice_t2, train_dice_t3=train_dice_t3,\n",
    "#     test_dice_t1=test_dice_t1, test_dice_t2=test_dice_t2, test_dice_t3=test_dice_t3,\n",
    "#     train_sd_t1=train_sd_t1, train_sd_t2=train_sd_t2, train_sd_t3=train_sd_t3,\n",
    "#     test_sd_t1=test_sd_t1, test_sd_t2=test_sd_t2, test_sd_t3=test_sd_t3,\n",
    "    \n",
    "#     # Mask\n",
    "#     train_mask_t1=train_mask_t1, train_mask_t2=train_mask_t2, train_mask_t3=train_mask_t3,\n",
    "#     test_mask_t1=test_mask_t1, test_mask_t2=test_mask_t2, test_mask_t3=test_mask_t3,\n",
    "#     train_mask_sd_t1=train_mask_sd_t1, train_mask_sd_t2=train_mask_sd_t2, train_mask_sd_t3=train_mask_sd_t3,\n",
    "#     test_mask_sd_t1=test_mask_sd_t1, test_mask_sd_t2=test_mask_sd_t2, test_mask_sd_t3=test_mask_sd_t3,\n",
    "\n",
    "#     # Set model name prefix\n",
    "#     model_name_prefix=\"Axial_UNet\"\n",
    "# )\n",
    "\n",
    "# if saved_path:\n",
    "#     print(f\"\\n All experiment data saved successfully to: {saved_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf0d7b",
   "metadata": {},
   "source": [
    "## Ablate Spatial Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bcd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Ablate Spatial Attention\n",
    "\n",
    "\n",
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Reduced from 24 to 16\n",
    "# args.d_attn1 = 128 # Reduced from 192 to 128\n",
    "# args.d_attn2 = 256 # Reduced from 384 to 256\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the **SWAU_Net-CNN Ablation** model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the full model and move it to the device\n",
    "# # The model is SWAU_Net_CNN, which uses CNN_Unet_Enc/Dec and CNN_DynNet.\n",
    "# swau_model = SWAU_Net_CNN(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(swau_model.E1)\n",
    "\n",
    "# # CORRECTED: Calculate parameters for both CFB modules separately\n",
    "# cfb_enc_params = count_parameters(swau_model.CFB_enc) \n",
    "# cfb_dec_params = count_parameters(swau_model.CFB_dec) \n",
    "# cfb_total_params = cfb_enc_params + cfb_dec_params\n",
    "\n",
    "# swa_params = count_parameters(swau_model.SWA) \n",
    "# p_params = count_parameters(swau_model.P)\n",
    "# d1_params = count_parameters(swau_model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + cfb_total_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"CNN_Unet_Enc (E1)\", \"Feature Extractor (No Spatial Attention)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB (Total, 2x Modules)\", \"**Pre/Post-Dynamics Mixer**\", f\"**{cfb_total_params:,}**\"],\n",
    "#     [\"SlidingWindowAttention (SWA)\", \"**Feature Aggregator/Integrator (Temporal Axial Only)**\", f\"**{swa_params:,}**\"], \n",
    "#     [\"CNN_DynNet (P)\", \"Temporal Feature Predictor (No Attention)\", f\"{p_params:,}\"],\n",
    "#     [\"CNN_Unet_Dec (D1)\", \"Frame Reconstructor\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**SWAU_Net-CNN Ablation**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### SWAU_Net-CNN Ablation Component Parameter Summary\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Pretrained Model\n",
    "\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # Load the model\n",
    "# MODEL_FILENAME = \"SWAU_CNN_pretrain_epoch50_20251115_205035.pth\"\n",
    "# MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# loaded_model, loaded_epoch = load_model(\n",
    "#     model=swau_model, \n",
    "#     model_path=MODEL_PATH, \n",
    "#     device=args.device\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bdaadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- INITIALIZATION AND HYPERPARAMETER SETUP ---\n",
    "\n",
    "# # # --- OVERFITTING TEST ACTIVATED ---\n",
    "# # # Create an overfit dataset by replicating the first sample (index 0) 100 times\n",
    "# # overfit_data = torch.cat([full_clean_data_tensor_cpu[0].unsqueeze(0)] * 400, dim=0)\n",
    "# # current_train_data = overfit_data \n",
    "# # # ----------------------------------\n",
    "\n",
    "# # --- TRAIN WITH FULL DATASET ---\n",
    "# current_train_data = full_clean_data_tensor_cpu\n",
    "# # ----------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS\n",
    "# args.num_epochs = 60\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = False # Use Soft Dice for stability\n",
    "# lr = 1E-4 # Initial LR\n",
    "\n",
    "# # --- MODEL SWAP: Instantiate SWAU_Net_CNN instead of model_baseline ---\n",
    "# # This is the SWAU_Net Ablation (SWA temporal regularization, no spatial attention)\n",
    "# swau_cnn_model = SWAU_Net_CNN(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# use_median = True\n",
    "# if use_median==True:\n",
    "#     print('Using DICE Median.')\n",
    "# else:\n",
    "#     print('Using DICE Aggregate Mean.')\n",
    "\n",
    "# if soft_dice==True:\n",
    "#     print('Using Soft DICE.')\n",
    "# else:\n",
    "#     print('Using hard DICE.')\n",
    "\n",
    "# # Update optimizer and scheduler to use the new model instance\n",
    "# optimizer_swau = torch.optim.Adam(swau_cnn_model.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# scheduler_swau = ReduceLROnPlateau(\n",
    "#     optimizer_swau, \n",
    "#     mode='min', factor=0.5, patience=15, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss # This relies on your custom dice_loss function\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# # LLR_WEIGHT and BOTTLENECK_L2_WEIGHT are correctly used below\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# freeze_batch_norm(swau_cnn_model)\n",
    "\n",
    "# # --- BASELINE HISTORY INITIALIZATION (REQUIRED FOR THIS SCOPE) ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median) - Use generic 'ablated' naming now\n",
    "# ablated_train_residual_t1, ablated_train_residual_t2, ablated_train_residual_t3 = [], [], []\n",
    "# ablated_test_residual_t1, ablated_test_residual_t2, ablated_test_residual_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# ablated_train_residual_sd_t1, ablated_train_residual_sd_t2, ablated_train_residual_sd_t3 = [], [], []\n",
    "# ablated_test_residual_sd_t1, ablated_test_residual_sd_t2, ablated_test_residual_sd_t3 = [], [], []\n",
    "\n",
    "# # Mask Scores (Mean/Median)\n",
    "# ablated_train_mask_t1, ablated_train_mask_t2, ablated_train_mask_t3 = [], [], []\n",
    "# ablated_test_mask_t1, ablated_test_mask_t2, ablated_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# ablated_train_mask_sd_t1, ablated_train_mask_sd_t2, ablated_train_mask_sd_t3 = [], [], []\n",
    "# ablated_test_mask_sd_t1, ablated_test_mask_sd_t2, ablated_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting SWAU-Net-CNN (No Spatial Attention) Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# # --- TRAINING LOOP (100 Epochs) ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- 1. Training Step (Using SWAU_Net's accumulated loss function) ---\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, swau_cnn_model, optimizer_swau, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=0, lambda_faf=0.5, lambda_mask=1.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.2, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 2. Evaluation Step (Median/SD) ---\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, swau_cnn_model, args, soft_dice=soft_dice, use_median=True)\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, swau_cnn_model, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 3. Accumulation ---\n",
    "#     # Residual Scores\n",
    "#     ablated_train_residual_t1.append(res_train_scores[0]); ablated_train_residual_t2.append(res_train_scores[1]); ablated_train_residual_t3.append(res_train_scores[2])\n",
    "#     ablated_test_residual_t1.append(res_test_scores[0]); ablated_test_residual_t2.append(res_test_scores[1]); ablated_test_residual_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     ablated_train_residual_sd_t1.append(res_train_sds[0]); ablated_train_residual_sd_t2.append(res_train_sds[1]); ablated_train_residual_sd_t3.append(res_train_sds[2])\n",
    "#     ablated_test_residual_sd_t1.append(res_test_sds[0]); ablated_test_residual_sd_t2.append(res_test_sds[1]); ablated_test_residual_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     ablated_train_mask_t1.append(msk_train_scores[0]); ablated_train_mask_t2.append(msk_train_scores[1]); ablated_train_mask_t3.append(msk_train_scores[2])\n",
    "#     ablated_test_mask_t1.append(msk_test_scores[0]); ablated_test_mask_t2.append(msk_test_scores[1]); ablated_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     ablated_train_mask_sd_t1.append(msk_train_sds[0]); ablated_train_mask_sd_t2.append(msk_train_sds[1]); ablated_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     ablated_test_mask_sd_t1.append(msk_test_sds[0]); ablated_test_mask_sd_t2.append(msk_test_sds[1]); ablated_test_mask_sd_t3.append(msk_test_sds[2]) \n",
    "\n",
    "#     # --- 4. Scheduler & Logging ---\n",
    "#     scheduler_swau.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_swau.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     f_display_frames(current_train_data, swau_cnn_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History\n",
    "#     plot_train_test_dice_history(\n",
    "#         ablated_train_residual_t1, ablated_train_residual_t2, ablated_train_residual_t3,\n",
    "#         ablated_test_residual_t1, ablated_test_residual_t2, ablated_test_residual_t3,\n",
    "#         ablated_train_residual_sd_t1, ablated_train_residual_sd_t2, ablated_train_residual_sd_t3,\n",
    "#         ablated_test_residual_sd_t1, ablated_test_residual_sd_t2, ablated_test_residual_sd_t3,\n",
    "#         plot_title='SWAU-Net-CNN (No Spatial Attn) Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History\n",
    "#     plot_train_test_dice_history(\n",
    "#         ablated_train_mask_t1, ablated_train_mask_t2, ablated_train_mask_t3,\n",
    "#         ablated_test_mask_t1, ablated_test_mask_t2, ablated_test_mask_t3,\n",
    "#         ablated_train_mask_sd_t1, ablated_train_mask_sd_t2, ablated_train_mask_sd_t3,\n",
    "#         ablated_test_mask_sd_t1, ablated_test_mask_sd_t2, ablated_test_mask_sd_t3,\n",
    "#         plot_title='SWAU-Net-CNN (No Spatial Attn) Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4651410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//CNN_Ablation')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "\n",
    "# saved_path = save_final_experiment_data(\n",
    "#     model=swau_cnn_model,  # <-- Using the ablated model instance\n",
    "#     final_epoch=FINAL_EPOCH,  \n",
    "#     base_save_dir=ckpt_save_dir,  # <-- Updated save directory name\n",
    "#     k_fold_index=k,\n",
    "    \n",
    "#     # --- Pass all history lists from the main training loop ---\n",
    "#     all_iteration_losses=all_iteration_losses,\n",
    "#     epoch_iteration_counts=epoch_iteration_counts,\n",
    "    \n",
    "#     # Residual (Using ablated history variables)\n",
    "#     train_dice_t1=ablated_train_residual_t1, train_dice_t2=ablated_train_residual_t2, train_dice_t3=ablated_train_residual_t3,\n",
    "#     test_dice_t1=ablated_test_residual_t1, test_dice_t2=ablated_test_residual_t2, test_dice_t3=ablated_test_residual_t3,\n",
    "#     train_sd_t1=ablated_train_residual_sd_t1, train_sd_t2=ablated_train_residual_sd_t2, train_sd_t3=ablated_train_residual_sd_t3,\n",
    "#     test_sd_t1=ablated_test_residual_sd_t1, test_sd_t2=ablated_test_residual_sd_t2, test_sd_t3=ablated_test_residual_sd_t3,\n",
    "    \n",
    "#     # Mask (Using ablated history variables)\n",
    "#     train_mask_t1=ablated_train_mask_t1, train_mask_t2=ablated_train_mask_t2, train_mask_t3=ablated_train_mask_t3,\n",
    "#     test_mask_t1=ablated_test_mask_t1, test_mask_t2=ablated_test_mask_t2, test_mask_t3=ablated_test_mask_t3,\n",
    "#     train_mask_sd_t1=ablated_train_mask_sd_t1, train_mask_sd_t2=ablated_train_mask_sd_t2, train_mask_sd_t3=ablated_train_mask_sd_t3,\n",
    "#     test_mask_sd_t1=ablated_test_mask_sd_t1, test_mask_sd_t2=ablated_test_mask_sd_t2, test_mask_sd_t3=ablated_test_mask_sd_t3,\n",
    "\n",
    "#     # Set model name prefix\n",
    "#     model_name_prefix=\"SWAUNet_CNN_ablation\" # <-- Updated prefix\n",
    "# )\n",
    "\n",
    "# if saved_path:\n",
    "#     print(f\"\\n All experiment data saved successfully to: {saved_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d13d3",
   "metadata": {},
   "source": [
    "## Train Conv LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Update (Reconfirmed) ---\n",
    "BASE_CHANNELS = 16 # 16 channels\n",
    "args.img_channels = 3 # 3 channels (FAF, Mask, Residual)\n",
    "\n",
    "# Function to count trainable parameters (Provided in setup)\n",
    "def count_parameters(model):\n",
    "    \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "print(\"\\nInstantiating the ConvLSTMBaseline model...\")\n",
    "\n",
    "# Instantiate the baseline model (assumes Unet_Enc, Unet_Dec, ConvLSTMCore are defined)\n",
    "model_baseline = ConvLSTMBaseline(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# Calculate parameters for each main component\n",
    "e1_params = count_parameters(model_baseline.E1)\n",
    "p_lstm_params = count_parameters(model_baseline.P_LSTM) # ConvLSTM Core\n",
    "d1_params = count_parameters(model_baseline.D1)\n",
    "total_params = e1_params + p_lstm_params + d1_params\n",
    "\n",
    "# --- Create Table Data ---\n",
    "param_data = [\n",
    "    [\"Unet_Enc (E1)\", \"Feature Extractor\", f\"{e1_params:,}\"],\n",
    "    [\"ConvLSTMCore (P_LSTM)\", \"**Recurrent Temporal Core**\", f\"**{p_lstm_params:,}**\"],\n",
    "    [\"Unet_Dec (D1)\", \"Frame Reconstructor\", f\"{d1_params:,}\"],\n",
    "    [\"\", \"\", \"\"], # Separator\n",
    "    [\"**TOTAL**\", \"**ConvLSTMBaseline Model**\", f\"**{total_params:,}**\"],\n",
    "]\n",
    "\n",
    "# --- Print Table ---\n",
    "print(\"\\n### ConvLSTMBaseline Parameter Summary\\n\")\n",
    "print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1550bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Pretrained Model\n",
    "\n",
    "ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# Load the model\n",
    "# MODEL_FILENAME = \"ConvLSTM_baseline_pretrain_epoch50_20251107_105643.pth\" \n",
    "# MODEL_FILENAME = \"ConvLSTM_baseline_pretrain_epoch50_20251109_130141.pth\"\n",
    "MODEL_FILENAME = \"ConvLSTM_baseline_pretrain_epoch50_20251112_235650.pth\"\n",
    "MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "loaded_model, loaded_epoch = load_model(\n",
    "    model=model_baseline, \n",
    "    model_path=MODEL_PATH, \n",
    "    device=args.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INITIALIZATION AND HYPERPARAMETER SETUP ---\n",
    "\n",
    "# # --- OVERFITTING TEST ACTIVATED ---\n",
    "# # Create an overfit dataset by replicating the first sample (index 0) 100 times\n",
    "# overfit_data = torch.cat([full_clean_data_tensor_cpu[0].unsqueeze(0)] * 400, dim=0)\n",
    "# current_train_data = overfit_data \n",
    "# # ----------------------------------\n",
    "\n",
    "# --- TRAIN WITH FULL DATASET ---\n",
    "current_train_data = full_clean_data_tensor_cpu\n",
    "# ----------------------------------\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "args.num_epochs = 100\n",
    "ACCUMULATION_STEPS = 8 \n",
    "soft_dice = False # Use Soft Dice for stability\n",
    "lr = 1E-4 # Initial LR\n",
    "\n",
    "use_median = True\n",
    "if use_median==True:\n",
    "    print('Using DICE Median.')\n",
    "else:\n",
    "    print('Using DICE Aggregate Mean.')\n",
    "\n",
    "if soft_dice==True:\n",
    "    print('Using Soft DICE.')\n",
    "else:\n",
    "    print('Using hard DICE.')\n",
    "\n",
    "optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "scheduler_baseline = ReduceLROnPlateau(\n",
    "    optimizer_baseline, \n",
    "    mode='min', factor=0.5, patience=15, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "loss_fn_dice = dice_loss # This relies on your custom dice_loss function\n",
    "loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# LLR_WEIGHT and BOTTLENECK_L2_WEIGHT are correctly used below\n",
    "BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# Freeze Batch Norm layers (essential for small batches)\n",
    "freeze_batch_norm(model_baseline)\n",
    "\n",
    "# --- BASELINE HISTORY INITIALIZATION (REQUIRED FOR THIS SCOPE) ---\n",
    "\n",
    "# Loss/Iteration Tracking\n",
    "all_iteration_losses = [] \n",
    "epoch_iteration_counts = []\n",
    "\n",
    "# Residual Scores (Mean/Median)\n",
    "baseline_train_residual_t1, baseline_train_residual_t2, baseline_train_residual_t3 = [], [], []\n",
    "baseline_test_residual_t1, baseline_test_residual_t2, baseline_test_residual_t3 = [], [], []\n",
    "# Residual SDs\n",
    "baseline_train_residual_sd_t1, baseline_train_residual_sd_t2, baseline_train_residual_sd_t3 = [], [], []\n",
    "baseline_test_residual_sd_t1, baseline_test_residual_sd_t2, baseline_test_residual_sd_t3 = [], [], []\n",
    "\n",
    "# Mask Scores (Mean/Median)\n",
    "baseline_train_mask_t1, baseline_train_mask_t2, baseline_train_mask_t3 = [], [], []\n",
    "baseline_test_mask_t1, baseline_test_mask_t2, baseline_test_mask_t3 = [], [], []\n",
    "# Mask SDs\n",
    "baseline_train_mask_sd_t1, baseline_train_mask_sd_t2, baseline_train_mask_sd_t3 = [], [], []\n",
    "baseline_test_mask_sd_t1, baseline_test_mask_sd_t2, baseline_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "print(f\"\\n Starting ConvLSTM Baseline Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# --- TRAINING LOOP (100 Epochs) ---\n",
    "for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "    # --- 1. Training Step (Using SWAU_Net's accumulated loss function) ---\n",
    "    epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "        current_train_data, model_baseline, optimizer_baseline, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "        accumulation_steps=ACCUMULATION_STEPS, \n",
    "        lambda_gdl=0, lambda_faf=0.5, lambda_mask=1.0, lambda_residual=5.0, \n",
    "        lambda_recon=0.2, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "    )\n",
    "\n",
    "    all_iteration_losses.extend(epoch_losses.tolist())\n",
    "    epoch_iteration_counts.append(len(epoch_losses))\n",
    "    mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "    # --- 2. Evaluation Step (Median/SD) ---\n",
    "    (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "        f_eval_pred_dice_test_set(test_loader, model_baseline, args, soft_dice=soft_dice, use_median=True)\n",
    "    (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "        f_eval_pred_dice_train_set(current_train_data, model_baseline, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "    # --- 3. Accumulation ---\n",
    "    # Residual Scores\n",
    "    baseline_train_residual_t1.append(res_train_scores[0]); baseline_train_residual_t2.append(res_train_scores[1]); baseline_train_residual_t3.append(res_train_scores[2])\n",
    "    baseline_test_residual_t1.append(res_test_scores[0]); baseline_test_residual_t2.append(res_test_scores[1]); baseline_test_residual_t3.append(res_test_scores[2])\n",
    "    # Residual SDs\n",
    "    baseline_train_residual_sd_t1.append(res_train_sds[0]); baseline_train_residual_sd_t2.append(res_train_sds[1]); baseline_train_residual_sd_t3.append(res_train_sds[2])\n",
    "    baseline_test_residual_sd_t1.append(res_test_sds[0]); baseline_test_residual_sd_t2.append(res_test_sds[1]); baseline_test_residual_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "    # Mask Scores\n",
    "    baseline_train_mask_t1.append(msk_train_scores[0]); baseline_train_mask_t2.append(msk_train_scores[1]); baseline_train_mask_t3.append(msk_train_scores[2])\n",
    "    baseline_test_mask_t1.append(msk_test_scores[0]); baseline_test_mask_t2.append(msk_test_scores[1]); baseline_test_mask_t3.append(msk_test_scores[2])\n",
    "    # Mask SDs\n",
    "    baseline_train_mask_sd_t1.append(msk_train_sds[0]); baseline_train_mask_sd_t2.append(msk_train_sds[1]); baseline_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "    baseline_test_mask_sd_t1.append(msk_test_sds[0]); baseline_test_mask_sd_t2.append(msk_test_sds[1]); baseline_test_mask_sd_t3.append(msk_test_sds[2]) # Corrected logic using test_sds\n",
    "\n",
    "    # --- 4. Scheduler & Logging ---\n",
    "    scheduler_baseline.step(mean_epoch_loss)\n",
    "\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_baseline.param_groups[0]['lr']:.2e}) ---\")\n",
    "    print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "    print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "    # --- Per-Epoch Visualizations ---\n",
    "    print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "    # A. Plot Loss History\n",
    "    plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "    # B. Plot Sample Prediction\n",
    "    f_display_frames(current_train_data, model_baseline, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "    # C. Plot Residual History\n",
    "    plot_train_test_dice_history(\n",
    "        baseline_train_residual_t1, baseline_train_residual_t2, baseline_train_residual_t3,\n",
    "        baseline_test_residual_t1, baseline_test_residual_t2, baseline_test_residual_t3,\n",
    "        baseline_train_residual_sd_t1, baseline_train_residual_sd_t2, baseline_train_residual_sd_t3,\n",
    "        baseline_test_residual_sd_t1, baseline_test_residual_sd_t2, baseline_test_residual_sd_t3,\n",
    "        plot_title='ConvLSTM Baseline Residual Dice History (Median ± SD)'\n",
    "    )\n",
    "\n",
    "    # D. Plot Mask History\n",
    "    plot_train_test_dice_history(\n",
    "        baseline_train_mask_t1, baseline_train_mask_t2, baseline_train_mask_t3,\n",
    "        baseline_test_mask_t1, baseline_test_mask_t2, baseline_test_mask_t3,\n",
    "        baseline_train_mask_sd_t1, baseline_train_mask_sd_t2, baseline_train_mask_sd_t3,\n",
    "        baseline_test_mask_sd_t1, baseline_test_mask_sd_t2, baseline_test_mask_sd_t3,\n",
    "        plot_title='ConvLSTM Baseline Full Mask Dice History (Median ± SD)'\n",
    "    )\n",
    "\n",
    "# --- Final Message ---\n",
    "print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Conv LSTM Baseline')\n",
    "FINAL_EPOCH = args.num_epochs\n",
    "\n",
    "saved_path = save_final_experiment_data(\n",
    "    model=model_baseline, \n",
    "    final_epoch=FINAL_EPOCH, \n",
    "    base_save_dir=ckpt_save_dir, \n",
    "    k_fold_index=k,\n",
    "    \n",
    "    # --- Pass all history lists from the main training loop ---\n",
    "    all_iteration_losses=all_iteration_losses,\n",
    "    epoch_iteration_counts=epoch_iteration_counts,\n",
    "    \n",
    "    # Residual\n",
    "    train_dice_t1=baseline_train_residual_t1, train_dice_t2=baseline_train_residual_t2, train_dice_t3=baseline_train_residual_t3,\n",
    "    test_dice_t1=baseline_test_residual_t1, test_dice_t2=baseline_test_residual_t2, test_dice_t3=baseline_test_residual_t3,\n",
    "    train_sd_t1=baseline_train_residual_sd_t1, train_sd_t2=baseline_train_residual_sd_t2, train_sd_t3=baseline_train_residual_sd_t3,\n",
    "    test_sd_t1=baseline_test_residual_sd_t1, test_sd_t2=baseline_test_residual_sd_t2, test_sd_t3=baseline_test_residual_sd_t3,\n",
    "    \n",
    "    # Mask\n",
    "    train_mask_t1=baseline_train_mask_t1, train_mask_t2=baseline_train_mask_t2, train_mask_t3=baseline_train_mask_t3,\n",
    "    test_mask_t1=baseline_test_mask_t1, test_mask_t2=baseline_test_mask_t2, test_mask_t3=baseline_test_mask_t3,\n",
    "    train_mask_sd_t1=baseline_train_mask_sd_t1, train_mask_sd_t2=baseline_train_mask_sd_t2, train_mask_sd_t3=baseline_train_mask_sd_t3,\n",
    "    test_mask_sd_t1=baseline_test_mask_sd_t1, test_mask_sd_t2=baseline_test_mask_sd_t2, test_mask_sd_t3=baseline_test_mask_sd_t3,\n",
    "\n",
    "    # Set model name prefix\n",
    "    model_name_prefix=\"ConvLSTM_baseline\"\n",
    ")\n",
    "\n",
    "if saved_path:\n",
    "    print(f\"\\n All experiment data saved successfully to: {saved_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6582876",
   "metadata": {},
   "source": [
    "## Train Simple ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update (Reconfirmed) ---\n",
    "# BASE_CHANNELS = 16 # 16 channels\n",
    "# args.img_channels = 3 # 3 channels (FAF, Mask, Residual)\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the ConvLSTM_Simple model...\")\n",
    "\n",
    "# # Instantiate the simple CNN-based model\n",
    "# # NOTE: This model uses CNN_Unet_Enc and CNN_Unet_Dec\n",
    "# model_simple = ConvLSTM_Simple(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(model_simple.E1)\n",
    "# p_lstm_params = count_parameters(model_simple.P_LSTM) # ConvLSTM Core\n",
    "# d1_params = count_parameters(model_simple.D1)\n",
    "# total_params = e1_params + p_lstm_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"CNN_Unet_Enc (E1)\", \"Feature Extractor (Ablated CNN)\", f\"{e1_params:,}\"],\n",
    "#     [\"ConvLSTMCore (P_LSTM)\", \"**Recurrent Temporal Core**\", f\"**{p_lstm_params:,}**\"],\n",
    "#     [\"CNN_Unet_Dec (D1)\", \"Frame Reconstructor (Ablated CNN)\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**ConvLSTM_Simple Model**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### ConvLSTM_Simple Parameter Summary\\n\")\n",
    "# print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fb208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Pretrained Model\n",
    "\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # Load the model\n",
    "# MODEL_FILENAME = \"ConvLSTM_simple_pretrain_epoch50_20251202_103250.pth\"\n",
    "# MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# loaded_model, loaded_epoch = load_model(\n",
    "#     model=model_simple, \n",
    "#     model_path=MODEL_PATH, \n",
    "#     device=args.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95dacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- INITIALIZATION FOR LOGGING ---\n",
    "# # Set to higher epochs to clearly observe overfitting\n",
    "# args.num_epochs = 50\n",
    "\n",
    "# # --- TRAIN WITH FULL DATASET ---\n",
    "# current_train_data = full_clean_data_tensor_cpu\n",
    "# # ----------------------------------\n",
    "\n",
    "# # History lists\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = [] # Stores number of iterations in each epoch\n",
    "\n",
    "# # --- RESIDUAL HISTORY LISTS (Scores and SDs) - USING NEW NAMING CONVENTION (simple_) ---\n",
    "# # The lists below are initialized with the 'simple_' prefix to store ConvLSTM_Simple results.\n",
    "# simple_train_t1, simple_train_t2, simple_train_t3 = [], [], []\n",
    "# simple_test_t1, simple_test_t2, simple_test_t3 = [], [], []\n",
    "# simple_train_sd_t1, simple_train_sd_t2, simple_train_sd_t3 = [], [], []\n",
    "# simple_test_sd_t1, simple_test_sd_t2, simple_test_sd_t3 = [], [], []\n",
    "\n",
    "# # --- MASK HISTORY LISTS (Scores and SDs) - USING NEW NAMING CONVENTION (simple_) ---\n",
    "# simple_train_mask_t1, simple_train_mask_t2, simple_train_mask_t3 = [], [], []\n",
    "# simple_test_mask_t1, simple_test_mask_t2, simple_test_mask_t3 = [], [], []\n",
    "# simple_train_mask_sd_t1, simple_train_mask_sd_t2, simple_train_mask_sd_t3 = [], [], []\n",
    "# simple_test_mask_sd_t1, simple_test_mask_sd_t2, simple_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# # --- TRAINING SETUP (Reconfirming) ---\n",
    "# # Loss functions (re-instantiated if necessary, matching previous definitions)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "# loss_fn_l1 = nn.L1Loss()\n",
    "# loss_fn_l2 = nn.MSELoss()\n",
    "# ACCUMULATION_STEPS = 8\n",
    "# soft_dice = False # Hard Dice\n",
    "\n",
    "# lr = 1E-4\n",
    "# # --- OPTIMIZER DEFINITION (Using the working model instance: model_simple) ---\n",
    "# optimizer = torch.optim.Adam(model_simple.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# # --- LEARNING RATE SCHEDULER INITIALIZATION ---\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',         # Monitor minimum loss\n",
    "#     factor=0.5,         \n",
    "#     patience=15,        \n",
    "#     verbose=True, \n",
    "#     min_lr=1e-6         \n",
    "# )\n",
    "# # ---------------------------------------------------\n",
    "\n",
    "# # -----------------\n",
    "# # FREEZE BATCH NORM (Using the working model instance: model_simple)\n",
    "# freeze_batch_norm(model_simple)\n",
    "# # ----------------\n",
    "\n",
    "# print(f\"\\nStarting ConvLSTM Simple training on device: {args.device} for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# # --- TRAINING LOOP ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "# #    # --- 1. Training Step ---\n",
    "#     # Update model instance to model_simple\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, model_simple, optimizer, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, lambda_gdl=0, lambda_faf=0.5, lambda_mask=1.0, lambda_residual=5.0, lambda_recon=0.5, use_augmentation=True\n",
    "#     )\n",
    "    \n",
    "#     # Store iteration loss and count for plot_log_loss\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "    \n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- NEW: SCHEDULER STEP ---\n",
    "#     scheduler.step(mean_epoch_loss)\n",
    "    \n",
    "#     # --- 3. Logging and Checkpoint ---\n",
    "    \n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary ---\")\n",
    "#     print(f\"Mean Loss: {mean_epoch_loss:.6f}\")\n",
    "\n",
    "#     # --- 4. Per-Epoch Visualizations (MOVED INSIDE LOOP) ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # C. Plot a sample prediction clip \n",
    "#     # Update model instance to model_simple\n",
    "#     f_display_frames(current_train_data, model_simple, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#      # EVALUATION\n",
    "#     use_median = True\n",
    "#     if use_median==True:\n",
    "#         print('Using DICE Median.')\n",
    "#     else:\n",
    "#         print('Using DICE Aggregate Mean.')\n",
    "        \n",
    "#     if soft_dice==True:\n",
    "#         print('Using Soft DICE.')\n",
    "#     else:\n",
    "#         print('Using hard DICE.')\n",
    "        \n",
    "#     # --- Evaluation Step (Using the working model instance: model_simple) ---\n",
    "    \n",
    "#     # 1. Unpack Test Results: Returns ((Res Scores, Res SDs), (Msk Scores, Msk SDs))\n",
    "#     # Update model instance to model_simple\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#     f_eval_pred_dice_test_set(test_loader, model_simple, args, soft_dice=soft_dice, use_median=use_median)\n",
    "\n",
    "#     # 2. Unpack Train Results\n",
    "#     # Update model instance to model_simple\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#     f_eval_pred_dice_train_set(current_train_data, model_simple, args, args.batch_size, soft_dice=soft_dice, use_median=use_median)\n",
    "    \n",
    "#     # --- 1. Residual Scores and SDs Accumulation (Using simple_ prefixes) ---\n",
    "\n",
    "#     # Accumulate Scores\n",
    "#     simple_train_t1.append(res_train_scores[0]); simple_train_t2.append(res_train_scores[1]); simple_train_t3.append(res_train_scores[2])\n",
    "#     simple_test_t1.append(res_test_scores[0]); simple_test_t2.append(res_test_scores[1]); simple_test_t3.append(res_test_scores[2])\n",
    "\n",
    "#     # Accumulate Standard Deviations (SDs)\n",
    "#     simple_train_sd_t1.append(res_train_sds[0]); simple_train_sd_t2.append(res_train_sds[1]); simple_train_sd_t3.append(res_train_sds[2])\n",
    "#     simple_test_sd_t1.append(res_test_sds[0]); simple_test_sd_t2.append(res_test_sds[1]); simple_test_sd_t3.append(res_test_sds[2])\n",
    "\n",
    "#     # --- 2. Mask Scores and SDs Accumulation (Using simple_ prefixes) ---\n",
    "\n",
    "#     # Accumulate Scores\n",
    "#     simple_train_mask_t1.append(msk_train_scores[0]); simple_train_mask_t2.append(msk_train_scores[1]); simple_train_mask_t3.append(msk_train_scores[2])\n",
    "#     simple_test_mask_t1.append(msk_test_scores[0]); simple_test_mask_t2.append(msk_test_scores[1]); simple_test_mask_t3.append(msk_test_scores[2])\n",
    "\n",
    "#     # Accumulate Standard Deviations (SDs)\n",
    "#     simple_train_mask_sd_t1.append(msk_train_sds[0]); simple_train_mask_sd_t2.append(msk_train_sds[1]); simple_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     simple_test_mask_sd_t1.append(msk_test_sds[0]); simple_test_mask_sd_t2.append(msk_test_sds[1]); simple_test_mask_sd_t3.append(msk_test_sds[2])\n",
    "\n",
    "\n",
    "#     # 1. Plot Residual History\n",
    "#     # Update variable names and plot title\n",
    "#     plot_train_test_dice_history(\n",
    "#         simple_train_t1, simple_train_t2, simple_train_t3,\n",
    "#         simple_test_t1, simple_test_t2, simple_test_t3,\n",
    "#         simple_train_sd_t1, simple_train_sd_t2, simple_train_sd_t3,          \n",
    "#         simple_test_sd_t1, simple_test_sd_t2, simple_test_sd_t3,            \n",
    "#         plot_title='ConvLSTM Simple Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # 2. Plot Mask History \n",
    "#     # Update variable names and plot title\n",
    "#     plot_train_test_dice_history(\n",
    "#         simple_train_mask_t1, simple_train_mask_t2, simple_train_mask_t3,\n",
    "#         simple_test_mask_t1, simple_test_mask_t2, simple_test_mask_t3,\n",
    "#         simple_train_mask_sd_t1, simple_train_mask_sd_t2, simple_train_mask_sd_t3, \n",
    "#         simple_test_mask_sd_t1, simple_test_mask_sd_t2, simple_test_mask_sd_t3,    \n",
    "#         plot_title='ConvLSTM Simple Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe10926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DIRECTORY PATH (Change this for your execution environment) ---\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//ConvLSTM_Simple_Baseline') # Updated Path Name\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "\n",
    "# # --- SAVE FINAL EXPERIMENT DATA ---\n",
    "\n",
    "# # NOTE: Using the working model instance 'model_simple' for saving.\n",
    "# saved_path = save_final_experiment_data(\n",
    "#     model=model_simple,\n",
    "#     final_epoch=FINAL_EPOCH, \n",
    "#     base_save_dir=ckpt_save_dir, \n",
    "#     k_fold_index=k,\n",
    "    \n",
    "#     # --- Pass all history lists from the main training loop (using 'simple_' prefixes) ---\n",
    "#     all_iteration_losses=all_iteration_losses,\n",
    "#     epoch_iteration_counts=epoch_iteration_counts,\n",
    "    \n",
    "#     # Residual\n",
    "#     train_dice_t1=simple_train_t1, train_dice_t2=simple_train_t2, train_dice_t3=simple_train_t3,\n",
    "#     test_dice_t1=simple_test_t1, test_dice_t2=simple_test_t2, test_dice_t3=simple_test_t3,\n",
    "#     train_sd_t1=simple_train_sd_t1, train_sd_t2=simple_train_sd_t2, train_sd_t3=simple_train_sd_t3,\n",
    "#     test_sd_t1=simple_test_sd_t1, test_sd_t2=simple_test_sd_t2, test_sd_t3=simple_test_sd_t3,\n",
    "    \n",
    "#     # Mask\n",
    "#     train_mask_t1=simple_train_mask_t1, train_mask_t2=simple_train_mask_t2, train_mask_t3=simple_train_mask_t3,\n",
    "#     test_mask_t1=simple_test_mask_t1, test_mask_t2=simple_test_mask_t2, test_mask_t3=simple_test_mask_t3,\n",
    "#     train_mask_sd_t1=simple_train_mask_sd_t1, train_mask_sd_t2=simple_train_mask_sd_t2, train_mask_sd_t3=simple_train_mask_sd_t3,\n",
    "#     test_mask_sd_t1=simple_test_mask_sd_t1, test_mask_sd_t2=simple_test_mask_sd_t2, test_mask_sd_t3=simple_test_mask_sd_t3,\n",
    "\n",
    "#     # Set model name prefix to reflect the specific ablation\n",
    "#     model_name_prefix=\"CONVLSTM_SIMPLE\" # <--- Changed prefix\n",
    "# )\n",
    "\n",
    "# if saved_path:\n",
    "#     print(f\"\\n All experiment data saved successfully to: {saved_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b15043",
   "metadata": {},
   "source": [
    "## Ablate Spatiotemporal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Update for Memory Reduction (Confirmed from previous turn) ---\n",
    "# BASE_CHANNELS = 16 # Reduced from 24 to 16\n",
    "# args.d_attn1 = 128 # Reduced from 192 to 128\n",
    "# args.d_attn2 = 256 # Reduced from 384 to 256\n",
    "\n",
    "# # Function to count trainable parameters (Provided in setup)\n",
    "# def count_parameters(model):\n",
    "#     \"\"\"Counts the total number of trainable parameters in a PyTorch model.\"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # --- Instantiation and Parameter Calculation ---\n",
    "\n",
    "# print(\"\\nInstantiating the **UPredNet (SWA Ablation)** model with the updated configuration...\")\n",
    "\n",
    "# # Instantiate the UPredNet model (UPredNet uses E1, CFB_enc, P=DynNet, CFB_dec, D1)\n",
    "# # We assume UPredNet is accessible, and base_channels defaults to 16 if not set.\n",
    "# upred_model = UPredNet(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # Calculate parameters for each main component\n",
    "# e1_params = count_parameters(upred_model.E1)\n",
    "\n",
    "# # Calculate parameters for both CFB modules separately\n",
    "# cfb_enc_params = count_parameters(upred_model.CFB_enc) \n",
    "# cfb_dec_params = count_parameters(upred_model.CFB_dec) \n",
    "# cfb_total_params = cfb_enc_params + cfb_dec_params\n",
    "\n",
    "# # The UPredNet model does NOT have an 'SWA' module. This parameter should be 0.\n",
    "# swa_params = 0 \n",
    "\n",
    "# # P is the DynNet\n",
    "# p_params = count_parameters(upred_model.P)\n",
    "# d1_params = count_parameters(upred_model.D1)\n",
    "\n",
    "# # Ensure all components are summed up for the total count\n",
    "# total_params = e1_params + cfb_total_params + swa_params + p_params + d1_params\n",
    "\n",
    "# # --- Create Table Data ---\n",
    "# param_data = [\n",
    "#     [\"Unet_Enc (E1)\", \"Feature Extractor (No Spatial Attention)\", f\"{e1_params:,}\"],\n",
    "#     [\"CFB (Total, 2x Modules)\", \"**Pre/Post-Dynamics Mixer**\", f\"**{cfb_total_params:,}**\"],\n",
    "#     [\"**SWA Module**\", \"**Ablated**\", f\"**{swa_params:,}**\"], # SWA is 0\n",
    "#     [\"DynNet (P)\", \"Temporal Feature Predictor (Evolution)\", f\"{p_params:,}\"],\n",
    "#     [\"Unet_Dec (D1)\", \"Frame Reconstructor\", f\"{d1_params:,}\"],\n",
    "#     [\"\", \"\", \"\"], # Separator\n",
    "#     [\"**TOTAL**\", \"**UPredNet (SWA Ablation)**\", f\"**{total_params:,}**\"],\n",
    "# ]\n",
    "\n",
    "# # --- Print Table ---\n",
    "# print(\"\\n### UPredNet (SWA Ablation) Component Parameter Summary\\n\")\n",
    "# try:\n",
    "#     from tabulate import tabulate\n",
    "#     print(tabulate(param_data, headers=[\"Component\", \"Role\", \"Parameters (Trainable)\"], tablefmt=\"fancy_grid\", colalign=(\"left\", \"left\", \"right\")))\n",
    "# except ImportError:\n",
    "#     print(\"Tabulate library not available. Printing raw data.\")\n",
    "#     for row in param_data:\n",
    "#         print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Pretrained Model\n",
    "\n",
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//Pretrained Models')\n",
    "# # Load the model\n",
    "# MODEL_FILENAME = \"UPredNet_pretrain_epoch50_20251121_033305.pth\"\n",
    "# MODEL_PATH = ckpt_save_dir / MODEL_FILENAME\n",
    "\n",
    "# loaded_model, loaded_epoch = load_model(\n",
    "#     model=upred_model, \n",
    "#     model_path=MODEL_PATH, \n",
    "#     device=args.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a956c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- INITIALIZATION AND HYPERPARAMETER SETUP ---\n",
    "\n",
    "# # # --- OVERFITTING TEST ACTIVATED ---\n",
    "# # # Create an overfit dataset by replicating the first sample (index 0) 100 times\n",
    "# # overfit_data = torch.cat([full_clean_data_tensor_cpu[0].unsqueeze(0)] * 400, dim=0)\n",
    "# # current_train_data = overfit_data \n",
    "# # # ----------------------------------\n",
    "\n",
    "# # --- TRAIN WITH FULL DATASET ---\n",
    "# current_train_data = full_clean_data_tensor_cpu\n",
    "# # ----------------------------------\n",
    "\n",
    "# # HYPERPARAMETERS\n",
    "# args.num_epochs = 60\n",
    "# ACCUMULATION_STEPS = 8 \n",
    "# soft_dice = False # Use Soft Dice for stability\n",
    "# lr = 1E-4 # Initial LR\n",
    "\n",
    "# use_median = True\n",
    "# if use_median==True:\n",
    "#     print('Using DICE Median.')\n",
    "# else:\n",
    "#     print('Using DICE Aggregate Mean.')\n",
    "\n",
    "# if soft_dice==True:\n",
    "#     print('Using Soft DICE.')\n",
    "# else:\n",
    "#     print('Using hard DICE.')\n",
    "\n",
    "# # --- MODEL INSTANTIATION (REPLACED CONVLSTM with UPredNet) ---\n",
    "# uprednet_model = UPredNet(args, img_channels=args.img_channels, base_channels=BASE_CHANNELS).to(args.device)\n",
    "\n",
    "# # --- OPTIMIZER, SCHEDULER RENAMING (Corrected all references) ---\n",
    "# optimizer_uprednet = torch.optim.Adam(uprednet_model.parameters(), lr=lr, betas=(0.95, 0.999), weight_decay=1E-5)\n",
    "\n",
    "# scheduler_uprednet = ReduceLROnPlateau(\n",
    "#     optimizer_uprednet, \n",
    "#     mode='min', factor=0.5, patience=15, verbose=True, min_lr=1e-6\n",
    "# )\n",
    "\n",
    "# # Loss functions (Ensure these are correctly instantiated elsewhere)\n",
    "# loss_fn_bce = nn.BCELoss(reduction='mean')\n",
    "# loss_fn_l1 = nn.L1Loss(reduction='mean') \n",
    "# loss_fn_l2 = nn.MSELoss(reduction='mean')\n",
    "# loss_fn_dice = dice_loss # This relies on your custom dice_loss function\n",
    "# loss_fn_gdl = GDLoss(alpha=1, beta=1)\n",
    "\n",
    "# # LLR_WEIGHT and BOTTLENECK_L2_WEIGHT are correctly used below\n",
    "# BOTTLENECK_L2_WEIGHT = 1e-6 \n",
    "\n",
    "# # Freeze Batch Norm layers (essential for small batches)\n",
    "# freeze_batch_norm(uprednet_model)\n",
    "\n",
    "# # --- UPREDNET HISTORY INITIALIZATION (Corrected all references) ---\n",
    "\n",
    "# # Loss/Iteration Tracking\n",
    "# all_iteration_losses = [] \n",
    "# epoch_iteration_counts = []\n",
    "\n",
    "# # Residual Scores (Mean/Median)\n",
    "# uprednet_train_residual_t1, uprednet_train_residual_t2, uprednet_train_residual_t3 = [], [], []\n",
    "# uprednet_test_residual_t1, uprednet_test_residual_t2, uprednet_test_residual_t3 = [], [], []\n",
    "# # Residual SDs\n",
    "# uprednet_train_residual_sd_t1, uprednet_train_residual_sd_t2, uprednet_train_residual_sd_t3 = [], [], []\n",
    "# uprednet_test_residual_sd_t1, uprednet_test_residual_sd_t2, uprednet_test_residual_sd_t3 = [], [], []\n",
    "\n",
    "# # Mask Scores (Mean/Median)\n",
    "# uprednet_train_mask_t1, uprednet_train_mask_t2, uprednet_train_mask_t3 = [], [], []\n",
    "# uprednet_test_mask_t1, uprednet_test_mask_t2, uprednet_test_mask_t3 = [], [], []\n",
    "# # Mask SDs\n",
    "# uprednet_train_mask_sd_t1, uprednet_train_mask_sd_t2, uprednet_train_mask_sd_t3 = [], [], []\n",
    "# uprednet_test_mask_sd_t1, uprednet_test_mask_sd_t2, uprednet_test_mask_sd_t3 = [], [], []\n",
    "\n",
    "\n",
    "# print(f\"\\n Starting **UPredNet (SWA Ablation)** Training for {args.num_epochs} epoch(s)...\")\n",
    "\n",
    "# # --- TRAINING LOOP (100 Epochs) ---\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Epoch Progress\"):\n",
    "    \n",
    "#     # --- 1. Training Step (Using UPredNet's accumulated loss function) ---\n",
    "#     epoch_losses = f_single_epoch_spatiotemporal_accumulated(\n",
    "#         current_train_data, uprednet_model, optimizer_uprednet, loss_fn_bce, loss_fn_dice, loss_fn_gdl, loss_fn_l1, loss_fn_l2, args, args.batch_size, \n",
    "#         accumulation_steps=ACCUMULATION_STEPS, \n",
    "#         lambda_gdl=0, lambda_faf=0.5, lambda_mask=1.0, lambda_residual=5.0, \n",
    "#         lambda_recon=0.2, lambda_bottleneck=BOTTLENECK_L2_WEIGHT, use_augmentation=True\n",
    "#     )\n",
    "\n",
    "#     all_iteration_losses.extend(epoch_losses.tolist())\n",
    "#     epoch_iteration_counts.append(len(epoch_losses))\n",
    "#     mean_epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "#     # --- 2. Evaluation Step (Median/SD) ---\n",
    "#     (res_test_scores, res_test_sds), (msk_test_scores, msk_test_sds) = \\\n",
    "#         f_eval_pred_dice_test_set(test_loader, uprednet_model, args, soft_dice=soft_dice, use_median=True)\n",
    "#     (res_train_scores, res_train_sds), (msk_train_scores, msk_train_sds) = \\\n",
    "#         f_eval_pred_dice_train_set(current_train_data, uprednet_model, args, args.batch_size, soft_dice=soft_dice, use_median=True)\n",
    "\n",
    "#     # --- 3. Accumulation (Corrected all history variable assignment) ---\n",
    "#     # Residual Scores\n",
    "#     uprednet_train_residual_t1.append(res_train_scores[0]); uprednet_train_residual_t2.append(res_train_scores[1]); uprednet_train_residual_t3.append(res_train_scores[2])\n",
    "#     uprednet_test_residual_t1.append(res_test_scores[0]); uprednet_test_residual_t2.append(res_test_scores[1]); uprednet_test_residual_t3.append(res_test_scores[2])\n",
    "#     # Residual SDs\n",
    "#     uprednet_train_residual_sd_t1.append(res_train_sds[0]); uprednet_train_residual_sd_t2.append(res_train_sds[1]); uprednet_train_residual_sd_t3.append(res_train_sds[2])\n",
    "#     uprednet_test_residual_sd_t1.append(res_test_sds[0]); uprednet_test_residual_sd_t2.append(res_test_sds[1]); uprednet_test_residual_sd_t3.append(res_test_sds[2])\n",
    "    \n",
    "#     # Mask Scores\n",
    "#     uprednet_train_mask_t1.append(msk_train_scores[0]); uprednet_train_mask_t2.append(msk_train_scores[1]); uprednet_train_mask_t3.append(msk_train_scores[2])\n",
    "#     uprednet_test_mask_t1.append(msk_test_scores[0]); uprednet_test_mask_t2.append(msk_test_scores[1]); uprednet_test_mask_t3.append(msk_test_scores[2])\n",
    "#     # Mask SDs\n",
    "#     uprednet_train_mask_sd_t1.append(msk_train_sds[0]); uprednet_train_mask_sd_t2.append(msk_train_sds[1]); uprednet_train_mask_sd_t3.append(msk_train_sds[2])\n",
    "#     uprednet_test_mask_sd_t1.append(msk_test_sds[0]); uprednet_test_mask_sd_t2.append(msk_test_sds[1]); uprednet_test_mask_sd_t3.append(msk_test_sds[2]) \n",
    "\n",
    "#     # --- 4. Scheduler & Logging ---\n",
    "#     scheduler_uprednet.step(mean_epoch_loss)\n",
    "\n",
    "#     print(f\"\\n--- Epoch {epoch+1}/{args.num_epochs} Summary (LR: {optimizer_uprednet.param_groups[0]['lr']:.2e}) ---\")\n",
    "#     print(f\"Mean Loss: **{mean_epoch_loss:.6f}**\")\n",
    "    \n",
    "#     print(\"\\nResidual T=3 Test Median Dice: {:.4f} (SD: {:.4f})\".format(res_test_scores[2], res_test_sds[2]))\n",
    "    \n",
    "#     # --- Per-Epoch Visualizations ---\n",
    "#     print(\"\\n--- Generating Per-Epoch Visualizations ---\")\n",
    "    \n",
    "#     # A. Plot Loss History\n",
    "#     plot_log_loss(all_iteration_losses, epoch_iteration_counts)\n",
    "\n",
    "#     # B. Plot Sample Prediction\n",
    "#     f_display_frames(current_train_data, uprednet_model, args, sample_idx=20, T_total=4)\n",
    "    \n",
    "#     # C. Plot Residual History (Corrected all history variable references and title)\n",
    "#     plot_train_test_dice_history(\n",
    "#         uprednet_train_residual_t1, uprednet_train_residual_t2, uprednet_train_residual_t3,\n",
    "#         uprednet_test_residual_t1, uprednet_test_residual_t2, uprednet_test_residual_t3,\n",
    "#         uprednet_train_residual_sd_t1, uprednet_train_residual_sd_t2, uprednet_train_residual_sd_t3,\n",
    "#         uprednet_test_residual_sd_t1, uprednet_test_residual_sd_t2, uprednet_test_residual_sd_t3,\n",
    "#         plot_title='UPredNet (SWA Ablation) Residual Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "#     # D. Plot Mask History (Corrected all history variable references and title)\n",
    "#     plot_train_test_dice_history(\n",
    "#         uprednet_train_mask_t1, uprednet_train_mask_t2, uprednet_train_mask_t3,\n",
    "#         uprednet_test_mask_t1, uprednet_test_mask_t2, uprednet_test_mask_t3,\n",
    "#         uprednet_train_mask_sd_t1, uprednet_train_mask_sd_t2, uprednet_train_mask_sd_t3,\n",
    "#         uprednet_test_mask_sd_t1, uprednet_test_mask_sd_t2, uprednet_test_mask_sd_t3,\n",
    "#         plot_title='UPredNet (SWA Ablation) Full Mask Dice History (Median ± SD)'\n",
    "#     )\n",
    "\n",
    "# # --- Final Message ---\n",
    "# print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752719d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_save_dir = Path('/Users/Pracioppo/Desktop//GA Forecasting//Saved Models//UPredNet')\n",
    "# FINAL_EPOCH = args.num_epochs\n",
    "\n",
    "# saved_path = save_final_experiment_data(\n",
    "#     model=uprednet_model,  # Model reference changed to uprednet_model\n",
    "#     final_epoch=FINAL_EPOCH,  \n",
    "#     base_save_dir=ckpt_save_dir,  \n",
    "#     k_fold_index=k,\n",
    "    \n",
    "#     # --- Pass all history lists from the main training loop (Variables changed from 'baseline' to 'uprednet') ---\n",
    "#     all_iteration_losses=all_iteration_losses,\n",
    "#     epoch_iteration_counts=epoch_iteration_counts,\n",
    "    \n",
    "#     # Residual\n",
    "#     train_dice_t1=uprednet_train_residual_t1, train_dice_t2=uprednet_train_residual_t2, train_dice_t3=uprednet_train_residual_t3,\n",
    "#     test_dice_t1=uprednet_test_residual_t1, test_dice_t2=uprednet_test_residual_t2, test_dice_t3=uprednet_test_residual_t3,\n",
    "#     train_sd_t1=uprednet_train_residual_sd_t1, train_sd_t2=uprednet_train_residual_sd_t2, train_sd_t3=uprednet_train_residual_sd_t3,\n",
    "#     test_sd_t1=uprednet_test_residual_sd_t1, test_sd_t2=uprednet_test_residual_sd_t2, test_sd_t3=uprednet_test_residual_sd_t3,\n",
    "    \n",
    "#     # Mask\n",
    "#     train_mask_t1=uprednet_train_mask_t1, train_mask_t2=uprednet_train_mask_t2, train_mask_t3=uprednet_train_mask_t3,\n",
    "#     test_mask_t1=uprednet_test_mask_t1, test_mask_t2=uprednet_test_mask_t2, test_mask_t3=uprednet_test_mask_t3,\n",
    "#     train_mask_sd_t1=uprednet_train_mask_sd_t1, train_mask_sd_t2=uprednet_train_mask_sd_t2, train_mask_sd_t3=uprednet_train_mask_sd_t3,\n",
    "#     test_mask_sd_t1=uprednet_test_mask_sd_t1, test_mask_sd_t2=uprednet_test_mask_sd_t2, test_mask_sd_t3=uprednet_test_mask_sd_t3,\n",
    "\n",
    "#     # Set model name prefix\n",
    "#     model_name_prefix=\"UPredNet\"\n",
    "# )\n",
    "\n",
    "# if saved_path:\n",
    "#     print(f\"\\n All experiment data saved successfully to: {saved_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329e143",
   "metadata": {},
   "source": [
    "## Experiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Analysis (SWAU NET)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base directory where all K-fold folders (e.g., 'Fold_0', 'Fold_1') are saved.\n",
    "BASE_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\SWAU_Net'\n",
    "\n",
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "MEAN_SCORE_KEY = 'test_t3' # Contains the median score time series\n",
    "STD_SCORE_KEY = 'test_sd_t3' # Contains the standard deviation time series\n",
    "\n",
    "# Number of last epochs to average over for stability\n",
    "LAST_EPOCH = 50\n",
    "N_EPOCHS_TO_AVERAGE = 10\n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (T=1 is 0, T=2 is 1, T=3 is 2)\n",
    "# NOTE: This index is only used if the underlying array is not sliced for T3 already,\n",
    "# but since the saved arrays are single time series lists, we won't use it for slicing.\n",
    "T_CRITICAL_INDEX = 2\n",
    "\n",
    "# --- MAIN ANALYSIS FUNCTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_kfold_results(BASE_DIR, RESIDUAL_HISTORY_FILE, MASK_HISTORY_FILE, MEAN_SCORE_KEY, STD_SCORE_KEY, N_EPOCHS_TO_AVERAGE, T_CRITICAL_INDEX, LAST_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6325500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Analysis (CONV LSTM)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base directory where all K-fold folders (e.g., 'Fold_0', 'Fold_1') are saved.\n",
    "BASE_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\Conv LSTM Baseline'\n",
    "\n",
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "MEAN_SCORE_KEY = 'test_t3' # Contains the median score time series\n",
    "STD_SCORE_KEY = 'test_sd_t3' # Contains the standard deviation time series\n",
    "\n",
    "# Number of last epochs to average over for stability\n",
    "LAST_EPOCH = 50\n",
    "N_EPOCHS_TO_AVERAGE = 10\n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (T=1 is 0, T=2 is 1, T=3 is 2)\n",
    "# NOTE: This index is only used if the underlying array is not sliced for T3 already,\n",
    "# but since the saved arrays are single time series lists, we won't use it for slicing.\n",
    "T_CRITICAL_INDEX = 2 \n",
    "\n",
    "# --- MAIN ANALYSIS FUNCTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_kfold_results(BASE_DIR, RESIDUAL_HISTORY_FILE, MASK_HISTORY_FILE, MEAN_SCORE_KEY, STD_SCORE_KEY, N_EPOCHS_TO_AVERAGE, T_CRITICAL_INDEX, LAST_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Analysis (AXIAL UNet, i.e. ablate SWA)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base directory where all K-fold folders (e.g., 'Fold_0', 'Fold_1') are saved.\n",
    "BASE_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\Axial_UNet'\n",
    "\n",
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "MEAN_SCORE_KEY = 'test_t3' # Contains the median score time series\n",
    "STD_SCORE_KEY = 'test_sd_t3' # Contains the standard deviation time series\n",
    "\n",
    "# Number of last epochs to average over for stability\n",
    "LAST_EPOCH = 50\n",
    "N_EPOCHS_TO_AVERAGE = 10\n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (T=1 is 0, T=2 is 1, T=3 is 2)\n",
    "# NOTE: This index is only used if the underlying array is not sliced for T3 already,\n",
    "# but since the saved arrays are single time series lists, we won't use it for slicing.\n",
    "T_CRITICAL_INDEX = 2 \n",
    "\n",
    "# --- MAIN ANALYSIS FUNCTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_kfold_results(BASE_DIR, RESIDUAL_HISTORY_FILE, MASK_HISTORY_FILE, MEAN_SCORE_KEY, STD_SCORE_KEY, N_EPOCHS_TO_AVERAGE, T_CRITICAL_INDEX, LAST_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be853b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Analysis (Ablate spatial attention)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base directory where all K-fold folders (e.g., 'Fold_0', 'Fold_1') are saved.\n",
    "BASE_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\CNN_Ablation'\n",
    "\n",
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "MEAN_SCORE_KEY = 'test_t3' # Contains the median score time series\n",
    "STD_SCORE_KEY = 'test_sd_t3' # Contains the standard deviation time series\n",
    "\n",
    "# Number of last epochs to average over for stability\n",
    "LAST_EPOCH = 50\n",
    "N_EPOCHS_TO_AVERAGE = 10\n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (T=1 is 0, T=2 is 1, T=3 is 2)\n",
    "# NOTE: This index is only used if the underlying array is not sliced for T3 already,\n",
    "# but since the saved arrays are single time series lists, we won't use it for slicing.\n",
    "T_CRITICAL_INDEX = 2 \n",
    "\n",
    "# --- MAIN ANALYSIS FUNCTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_kfold_results(BASE_DIR, RESIDUAL_HISTORY_FILE, MASK_HISTORY_FILE, MEAN_SCORE_KEY, STD_SCORE_KEY, N_EPOCHS_TO_AVERAGE, T_CRITICAL_INDEX, LAST_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Analysis (UPredNet, i.e. ablate spatiotemporal attention)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base directory where all K-fold folders (e.g., 'Fold_0', 'Fold_1') are saved.\n",
    "BASE_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\UPredNet'\n",
    "\n",
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "MEAN_SCORE_KEY = 'test_t3' # Contains the median score time series\n",
    "STD_SCORE_KEY = 'test_sd_t3' # Contains the standard deviation time series\n",
    "\n",
    "# Number of last epochs to average over for stability\n",
    "LAST_EPOCH = 50\n",
    "N_EPOCHS_TO_AVERAGE = 10\n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (T=1 is 0, T=2 is 1, T=3 is 2)\n",
    "# NOTE: This index is only used if the underlying array is not sliced for T3 already,\n",
    "# but since the saved arrays are single time series lists, we won't use it for slicing.\n",
    "T_CRITICAL_INDEX = 2 \n",
    "\n",
    "# --- MAIN ANALYSIS FUNCTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_kfold_results(BASE_DIR, RESIDUAL_HISTORY_FILE, MASK_HISTORY_FILE, MEAN_SCORE_KEY, STD_SCORE_KEY, N_EPOCHS_TO_AVERAGE, T_CRITICAL_INDEX, LAST_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Analysis (SWAU NET, No Pretraining)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base directory where all K-fold folders (e.g., 'Fold_0', 'Fold_1') are saved.\n",
    "BASE_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\No Pretraining'\n",
    "\n",
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "MEAN_SCORE_KEY = 'test_t3' # Contains the median score time series\n",
    "STD_SCORE_KEY = 'test_sd_t3' # Contains the standard deviation time series\n",
    "\n",
    "# Number of last epochs to average over for stability\n",
    "LAST_EPOCH = 50\n",
    "N_EPOCHS_TO_AVERAGE = 10\n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (T=1 is 0, T=2 is 1, T=3 is 2)\n",
    "# NOTE: This index is only used if the underlying array is not sliced for T3 already,\n",
    "# but since the saved arrays are single time series lists, we won't use it for slicing.\n",
    "T_CRITICAL_INDEX = 2\n",
    "\n",
    "# --- MAIN ANALYSIS FUNCTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_kfold_results(BASE_DIR, RESIDUAL_HISTORY_FILE, MASK_HISTORY_FILE, MEAN_SCORE_KEY, STD_SCORE_KEY, N_EPOCHS_TO_AVERAGE, T_CRITICAL_INDEX, LAST_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4895c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Analysis (SWAU NET, No Augmentations)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base directory where all K-fold folders (e.g., 'Fold_0', 'Fold_1') are saved.\n",
    "BASE_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\No Augmentation'\n",
    "\n",
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "MEAN_SCORE_KEY = 'test_t3' # Contains the median score time series\n",
    "STD_SCORE_KEY = 'test_sd_t3' # Contains the standard deviation time series\n",
    "\n",
    "# Number of last epochs to average over for stability\n",
    "LAST_EPOCH = 50\n",
    "N_EPOCHS_TO_AVERAGE = 10\n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (T=1 is 0, T=2 is 1, T=3 is 2)\n",
    "# NOTE: This index is only used if the underlying array is not sliced for T3 already,\n",
    "# but since the saved arrays are single time series lists, we won't use it for slicing.\n",
    "T_CRITICAL_INDEX = 2\n",
    "\n",
    "# --- MAIN ANALYSIS FUNCTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_kfold_results(BASE_DIR, RESIDUAL_HISTORY_FILE, MASK_HISTORY_FILE, MEAN_SCORE_KEY, STD_SCORE_KEY, N_EPOCHS_TO_AVERAGE, T_CRITICAL_INDEX, LAST_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad84251",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Analysis (SWAU NET, No Augmentations)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base directory where all K-fold folders (e.g., 'Fold_0', 'Fold_1') are saved.\n",
    "BASE_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\CFB Ablation'\n",
    "\n",
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "MEAN_SCORE_KEY = 'test_t3' # Contains the median score time series\n",
    "STD_SCORE_KEY = 'test_sd_t3' # Contains the standard deviation time series\n",
    "\n",
    "# Number of last epochs to average over for stability\n",
    "LAST_EPOCH = 50\n",
    "N_EPOCHS_TO_AVERAGE = 10\n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (T=1 is 0, T=2 is 1, T=3 is 2)\n",
    "# NOTE: This index is only used if the underlying array is not sliced for T3 already,\n",
    "# but since the saved arrays are single time series lists, we won't use it for slicing.\n",
    "T_CRITICAL_INDEX = 2\n",
    "\n",
    "# --- MAIN ANALYSIS FUNCTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_kfold_results(BASE_DIR, RESIDUAL_HISTORY_FILE, MASK_HISTORY_FILE, MEAN_SCORE_KEY, STD_SCORE_KEY, N_EPOCHS_TO_AVERAGE, T_CRITICAL_INDEX, LAST_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aed2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Analysis (SWAU NET, No Augmentations)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base directory where all K-fold folders (e.g., 'Fold_0', 'Fold_1') are saved.\n",
    "BASE_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\CFB Ablation'\n",
    "\n",
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "MEAN_SCORE_KEY = 'test_t3' # Contains the median score time series\n",
    "STD_SCORE_KEY = 'test_sd_t3' # Contains the standard deviation time series\n",
    "\n",
    "# Number of last epochs to average over for stability\n",
    "LAST_EPOCH = 50\n",
    "N_EPOCHS_TO_AVERAGE = 10\n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (T=1 is 0, T=2 is 1, T=3 is 2)\n",
    "# NOTE: This index is only used if the underlying array is not sliced for T3 already,\n",
    "# but since the saved arrays are single time series lists, we won't use it for slicing.\n",
    "T_CRITICAL_INDEX = 2\n",
    "\n",
    "# --- MAIN ANALYSIS FUNCTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_kfold_results(BASE_DIR, RESIDUAL_HISTORY_FILE, MASK_HISTORY_FILE, MEAN_SCORE_KEY, STD_SCORE_KEY, N_EPOCHS_TO_AVERAGE, T_CRITICAL_INDEX, LAST_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment Analysis (SWAU NET, No Augmentations)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Base directory where all K-fold folders (e.g., 'Fold_0', 'Fold_1') are saved.\n",
    "BASE_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\ConvLSTM_Simple_Baseline'\n",
    "\n",
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "MEAN_SCORE_KEY = 'test_t3' # Contains the median score time series\n",
    "STD_SCORE_KEY = 'test_sd_t3' # Contains the standard deviation time series\n",
    "\n",
    "# Number of last epochs to average over for stability\n",
    "LAST_EPOCH = 50\n",
    "N_EPOCHS_TO_AVERAGE = 10\n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (T=1 is 0, T=2 is 1, T=3 is 2)\n",
    "# NOTE: This index is only used if the underlying array is not sliced for T3 already,\n",
    "# but since the saved arrays are single time series lists, we won't use it for slicing.\n",
    "T_CRITICAL_INDEX = 2\n",
    "\n",
    "# --- MAIN ANALYSIS FUNCTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_kfold_results(BASE_DIR, RESIDUAL_HISTORY_FILE, MASK_HISTORY_FILE, MEAN_SCORE_KEY, STD_SCORE_KEY, N_EPOCHS_TO_AVERAGE, T_CRITICAL_INDEX, LAST_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece54b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected filenames for the NumPy array dictionaries containing the TEST dice history.\n",
    "RESIDUAL_HISTORY_FILE = 'residual_dice_history.npy'\n",
    "MASK_HISTORY_FILE = 'mask_dice_history.npy'\n",
    "\n",
    "# Keys used to access the relevant arrays inside the saved dictionaries\n",
    "# These keys are assumed to hold the scores for the critical T=3 prediction step.\n",
    "MEAN_SCORE_KEY = 'test_t3' \n",
    "STD_SCORE_KEY = 'test_sd_t3' \n",
    "\n",
    "# Index corresponding to the critical T=3 prediction step (index 2 for a sequence [T1, T2, T3])\n",
    "T_CRITICAL_INDEX = 2 \n",
    "\n",
    "# --- 2. MODEL PATHS (Replace these with your actual directories) ---\n",
    "\n",
    "# Model A: Full SWAU-Net (The proposed best model)\n",
    "MODEL_A_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\SWAU_Net' \n",
    "\n",
    "# Model B: UPredNet (The Causal CNN Aggregator Baseline)\n",
    "# MODEL_B_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\CFB Ablation'\n",
    "# MODEL_B_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\No Augmentation'\n",
    "# MODEL_B_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\No Pretraining'\n",
    "# MODEL_B_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\UPredNet'\n",
    "# MODEL_B_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\CNN_Ablation'\n",
    "# MODEL_B_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\Axial_UNet'\n",
    "# MODEL_B_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\Conv LSTM Baseline'\n",
    "# MODEL_B_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\DynNet_Ablation'\n",
    "MODEL_B_DIR = r'C:\\Users\\Pracioppo\\Desktop\\GA Forecasting\\Saved Models\\ConvLSTM_Simple_Baseline'\n",
    "\n",
    "# --- 3. EXECUTION OF COMPARISON ---\n",
    "\n",
    "print(\"Starting statistical analysis...\")\n",
    "compare_models(\n",
    "    model_a_dir=MODEL_A_DIR, \n",
    "    model_b_dir=MODEL_B_DIR,\n",
    "    # Pass configuration constants to the function (fixes the NameError)\n",
    "    RESIDUAL_HISTORY_FILE=RESIDUAL_HISTORY_FILE, \n",
    "    MASK_HISTORY_FILE=MASK_HISTORY_FILE,\n",
    "    MEAN_SCORE_KEY=MEAN_SCORE_KEY, \n",
    "    STD_SCORE_KEY=STD_SCORE_KEY,\n",
    "    T_CRITICAL_INDEX=T_CRITICAL_INDEX,\n",
    "    # Fixed statistical parameters based on the study design\n",
    "    N_TOTAL_SAMPLES=66, \n",
    "    K_FOLDS=5, \n",
    "    n_epochs=10, \n",
    "    last_epoch=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452bcb47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
